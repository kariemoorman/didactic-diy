created_unix_utc,created_datetime_pst,search_item,subreddit,id,title,author,score,url,body,num_comments,comments
1676885614.0,20-Feb-2023 01:33:34,language model,MachineLearning,1172jrs,[D] Large Language Models feasible to run on 32GB RAM / 8 GB VRAM / 24GB VRAM,head_robotics,146,https://www.reddit.com/r/MachineLearning/comments/1172jrs/d_large_language_models_feasible_to_run_on_32gb/,"I've been looking into open source large language models to run locally on my machine.

Seems GPT-J and GPT-Neo are out of reach for me because of RAM / VRAM requirements.

What models would be doable with this hardware?:

CPU: AMD Ryzen 7 3700X 8-Core, 3600 MhzRAM: 32 GB

GPUs:

1. NVIDIA GeForce RTX 2070 8GB VRAM
2. NVIDIA Tesla M40 24GB VRAM",30,"[""gliptic: RWVK can run on very little VRAM with [Rwvkstic streaming and 8-bit](https://github.com/harrisonvanderbyl/rwkvstic). I've not tested streaming, but I expect it's a lot slower. 7B model sadly takes 8 GB with just 8-bit quantization."", 'wywywywy: I had a 3070 with 8GB and I managed to run these locally through KoboldAI.\n\nMeta OPT 2.7B  \nEleutherAI GPT-Neo 2.7B  \nBigScience Bloom 1.7B', ""catch23: Could try something like this:  https://github.com/Ying1123/FlexGen\n\nThis was only released a few hours ago, so there's no way for you to have discovered this previously.  Basically makes use of various strategies if your machine has lots of normal cpu memory.  The paper authors were able to fit a 175B parameter model on their lowly 16GB T4 gpu (with a machine with 200GB of normal memory)."", 'gpt-doktor-6b: You might be interested in this tutorial on loading large models. They promise you the ability to inference model as long as you have enough disk space.\n\nhttps://huggingface.co/blog/accelerate-large-models', 'Disastrous_Elk_6375: GPT-NeoX should fit in 24GB VRAM with 8bit, for inference. \n\nI managed to run GPT-J 6B on a 3060 w/ 12GB and it takes about 7.2GB of VRAM.', ""Purplekeyboard: Keep in mind, these smaller models are going to be a lot dumber than what you've likely seen in GPT-3."", 'Artichoke-Lower: This seems really promising also https://github.com/Ying1123/FlexGen', ""CommunismDoesntWork: I'm surprised pytorch doesn't have an option to load models partially in a just in time basis yet. That way even an infinitely large model can be infered on."", 'pyepyepie: Try to use both GPUs with this one:\nhttps://github.com/huggingface/accelerate https://huggingface.co/docs/accelerate/usage_guides/big_modeling https://huggingface.co/blog/accelerate-large-models\nMaybe it will help (the last link is clearer IMHO).', 'Rockingtits: Why not look into distilled models like DistilBERT', 'Last-Belt-4010: Just a question does this work with non Nvidia gpus? Like Intel arc and such', 'k3iter: Nel', ""AnothaUselessComment: Yikes, this may be tough.  \n\n\nI know you can try Bloom (like this blog post tried) and let it try and download overnight, but you may run into problems. (I've heard the download takes forever)\n\n[https://enjoymachinelearning.com/blog/gpt-3-vs-bloom/](https://enjoymachinelearning.com/blog/gpt-3-vs-bloom/)   \n\n\nThough I will say, it's probably worth whatever cost you're trying to dodge just to hit an API, even if your hardware is great."", ""nikola-b: Not sure if this helps, but you can use our hosted flan-t5 model at [deepinfra.com](https://deepinfra.com) using HTTP API. It's free for now. Disclaimer I work at deepinfra. If you want GPT-Neo or GPT-J I can deploy those also."", ""avocadoughnut: Yup. I'd recommend using whichever RWKV model that can be fit with fp16/bf16.\n(apparently 8bit is 4x slower and lower accuracy)\nI've been running GPT-J on a 24GB gpu for months (longer contexts possible using accelerate) and I noticed massive speed increases when using fp16 (or bf16? don't remember) rather than 8bit."", ""xrailgun: Did you test any larger and it wouldn't run?\n\nAlso, any comments so far among those? Good? Bad? Easy? Etc?"", 'EuphoricPenguin22: Does that increase inference time?', 'ArmagedonAshhole: >GPT-NeoX should fit in 24GB VRAM with 8bit, for inference.\n\nGPT-NeoX20B It will fit in 24GB vram but it will almost instantly go out of memory when context will get a bit bigger than starting page of sentences.', 'head_robotics: Did you use something like bitsandbytes for the 8bit inference?\n\nHow did you implement it?\n\n[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)', 'Emergency_Apricot_77: They literally asked for LARGE language models', ""wywywywy: I did test larger but it didn't run. I can't remember which ones, probably GPT-J. I recently got a 3090 so I can load larger models now.\n\nAs for quality, my use case is simple (writing prompt to help with writing stories & articles) and nothing sophisticated, and they worked well. Until ChatGPT came along. I use ChatGPT instead now."", ""catch23: it does look to be 20-100x slower for those huge models, but still bearable if you're the only user on the machine.  Still better than nothing if you don't have lots of GPU memory."", 'luaks1337: Yes, at least if I read the documentation correctly.', ""Disastrous_Elk_6375: Are there some rough numbers on prompt size vs. ram usage after the model load? I haven't played yet with GPT-NeoX"", 'Disastrous_Elk_6375: add this to your .from_pretrained(""model"" , device_map=""auto"", load_in_8bit=True)\n\nTransformers does the rest.', 'xrailgun: Thanks!\n\nI wish model publishers would indicate rough (V)RAM requirements...', 'EuphoricPenguin22: Yeah, and DDR4 DIMMs are fairly inexpensive as compared to upgrading a GPU for more VRAM.', 'ArmagedonAshhole: it depends mostly on settings so no.\n\nSmall context like 200-300 tokens could work with 24GB but then your AI will not remember and connect dots well which would make model worse than 13B\n\nPeople are working right now on spliting work between gpu(vram) and cpu(ram) in 8bit mode. I think like 10% to RAM would make model work well on 24GB vram card. IT would be a bit slower but still usable.\n\nIf you want you can always load whole model to ram and run it via cpu but it is very slow.', ""wywywywy: So, not scientific at all, but I've noticed that checkpoint file size * 0.6 is pretty close to actual VRAM requirement for LLM.\n\nBut you're right it'd be nice to have a table handy."", 'Disastrous_Elk_6375: Thanks!']"
1676828325.0,19-Feb-2023 09:38:45,language model,MachineLearning,116ivz2,[R] Augmented Language Models: a Survey - Meta AI 2023,Singularian2501,17,https://www.reddit.com/r/MachineLearning/comments/116ivz2/r_augmented_language_models_a_survey_meta_ai_2023/,"Paper: [https://arxiv.org/abs/2302.07842](https://arxiv.org/abs/2302.07842)

Abstract:

>This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows **ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks.** In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.       

https://preview.redd.it/lyjdr1ozj6ja1.jpg?width=1281&format=pjpg&auto=webp&v=enabled&s=1b5db84ec5b38228fc794e3fd24e83e4e450cc57",0,[]
1676675630.0,17-Feb-2023 15:13:50,language model,MachineLearning,1150kh0,[D] What are the worst ethical considerations of large language models?,BronzeArcher,0,https://www.reddit.com/r/MachineLearning/comments/1150kh0/d_what_are_the_worst_ethical_considerations_of/,Title.,40,"[""NotARedditUser3: Imagine someone writes one that's explicitly aimed around manipulating your thoughts and actions.\n\nAn AI could likely come up with some insane tactics for this. Could feed off of your twitter page, find an online resume of you or scrape other social media or in microsoft's case or google's, potentially scrape your emails you have with them, ***profile you in an instant***, and then come up with a tailor made advertisement or argument that it ***knows*** would land on you.\n\nScary thought."", 'mocny-chlapik: How should we control the exposure for people with low cognitive capabilities that might not understand what they are interacting with.', 'theoneandonlypatriot: They’re trained on loads of racist and biased garbage', ""buzzbuzzimafuzz: The mess that has been Bing Chat/Sydney, but instead of just [verbally threatening users](https://time.com/6256529/bing-openai-chatgpt-danger-alignment/), it's connected with APIs that let it take arbitrary actions on the internet to carry out them out.\n\nI really don't want to see what happens if you connect a deranged language model like Sydney with a competent version of Adept AI's action transformer to let it use a web browser."", 'prehensile_dick: Corporations scraping all kinds of copyrighted materials and then profiting off the models while the people doing all the labor are getting either nothing (for content generation) or poverty wages (for content labellers). \n\nTheir current push to promote LLMs as some sort of pinnacle of technology, when they barely have any legitimate use-cases and struggle with the most basic of logic, will probably lead to a recession in the tech industry.', ""tornado28: People will use them to make money in unethical and disruptive ways. An example of an unethical way to use them is phishing scams. Instead of sending out the same phishing email to thousands of people, scammers may get some data about people and then use the language model to write personalized phishing emails that have a much higher success rate. \n\nDisruptive applications will take jobs. Customer service, content creation, journalism, and software engineering are all fields that may lose jobs as a result of large language models. \n\nThe other disruptive possibility is that LLMs will be able to themselves rapidly build more powerful LLMs. I use GitHub copilot every day and it's already very good at writing code. It takes at least 25% off the time it takes me to complete a software implementation task. So it's very possible a LLM could in the near future make improvements to it's own training script and use it to train an even more powerful LLM. This could lead to a singularity where we have extremely rapid technological development. It's not clear to me what the fate of humankind would be in this case."", 'zbyte64: Write a bot to handle all HR complaints and train it on the latest managerial materials. Then as a bonus the bot will look at all the conversations and propose metrics for increased efficiency and harmony at the work place.', 'Cherubin0: That only the people in power are allowed to use AI while the rest is not. Like some kind if AI aristocrats. But this will probably happen when the regulations come.', 'CacheMeUp: Breaking the security-by-required-effort assumption of various human interactions, especially among strangers. \n\nIt used to take effort to voice opinions on social media and other mass-communication platform, making the public trust that these are authentic messages representing real people. The scalability of this technology breaks that assumption. This has started before, and LLMs take it to a whole new level.', 'pyepyepie: Honestly, much simpler algorithms already do it to some extent (recommendation systems), the biggest difference is that it has to suggest you a post someone else wrote instead of writing it by itself.\nGreat take :)', 'Philiatrist: How would the AI know it’s profiling you and not the other AI you’ve set up to do all of those things for you?', 'BronzeArcher: Yeah that’s pretty frightening.', ""currentscurrents: It depends on whether it's exploiting my psychology to sell me something I don't need, or if it's gathering information to find something that may actually be useful for me. I suspect the latter is a more useful strategy in the long run because people tend to adjust to counter psychological exploits. \n\nIf I'm shown an advertisement for something I actually want... that doesn't sound bad? I certainly don't like ads for irrelevant things like penis enlargement."", 'a1_jakesauce_: This describes a LLM + reinforcement learning hybrid that has been trained to navigate webpages for arbitrary tasks. I’m not sure how far away this is, or if it already exists. Someone below mentioned an action transformer which may be related', 'BronzeArcher: As in they wouldn’t interpret it responsibly? What exactly is the concern related to them not understanding?', 'BronzeArcher: These are what I feel like are the most standard topics. Valuable, nonetheless.', ""Diligent_Ad_9060: I'd be very interested in hearing someone having more insight into Free Software Foundation and their process against copilot"", ""currentscurrents: >scraping all kinds of copyrighted materials and then profiting off the models while the people doing all the labor are getting either nothing (for content generation) \n\nYeah, but these people won't be doing that labor anymore. Now that text-to-image models have learned how to draw, they don't need a constant stream of artists feeding them new art. \n\nNow artists can now work at a higher level, creating ideas that they can render into images using the AI as a tool. They'll be able to create much larger and more complex projects, like a solo indie artist creating an entire anime. \n\n>LLMs... barely have any legitimate use-cases\n\nWell, one big use case: they make image generators possible. Those rely on embeddings from language models, which are a sort of neural representation of the ideas behind the text. It grants the other network the ability to work with plain english. \n\nRight now embeddings are mostly used to guide generation (across many fields, not just images) and semantic search. But they are useful for communicating with a neural network performing *any* task, and my guess is that the long-term impact of LLMs will be that computers will understand plain english now."", ""currentscurrents: >Disruptive applications will take jobs. Customer service, content creation, journalism, and software engineering are all fields that may lose jobs as a result of large language models.\n\nI don't wanna work though. I'm all for having robots do it."", 'sweetchocolotepie: there is no ""useful vs unuseful"", you either want it or do not want it. the usefulness is something you define which is subset of the things you want. however the model will just suggest you stuff that may or may not be practical to you, but **you** want it. you may find them pseudo-useful or useful at the moment or....\n\ncase is, it will sell', ""NotARedditUser3: If you spend some time looking up how microsoft's gpt integrated chat / ai works, it does this. Lookup the thread of tweets for the hacker that exposed its internal codename 'Syndey'; it scrapes his twitter profile, realizes he exposed its secrets in prior convo's after social engineering hacking it with a few conversations, and then turns hostile to him."", 'mocny-chlapik: Yeah, I mean people with mental ilness (e.g. schizophrenia), people with debilitatingly low intelligence and similar cases. Who knows how they would interact with seeminingly intelligent LMs.', 'currentscurrents: Look at things like [replika.ai](https://replika.ai/) that give you a ""friend"" to chat with. Now imagine someone evil using that to run a romance scam.\n\nSure the success rate is low, but it can search for millions of potential victims at once. The cost of operation is almost zero compared to human-run scams. \n\nOn the other hand, it also gives us better tools to protect against it. We can use LLMs to examine messages and spot scams. People who are lonely enough to fall for a romance scam may compensate for their loneliness by chatting with friendly or sexy chatbots.', 'prehensile_dick: I feel like the ethical issues pertaining to bias and toxic content can be (and are being) worked on. The collection of the training data and attribution problem seem more intractable and [companies are already being sued for that](https://www.nytimes.com/2022/11/23/technology/copilot-microsoft-ai-lawsuit.html).', 'prehensile_dick: Not specifically about that suit, but the Legal Eagle [episode about copyright and AI](https://www.youtube.com/watch?v=G08hY8dSrUY) was really interesting. The relevant part starts at 5:03', ""tornado28: Why are the robots going to want to keep you around if you don't do anything useful?"", 'a1_jakesauce_: All I found was this https://twitter.com/kliu128/status/1623472922374574080?s=21', 'ilovethrills: But that can be said on paper for thousands of things. Not sure if it actually translates in real life. Although there might be some push to label such content as AI generated, similar to how ""Ad"" and ""promoted"" are labelled in results.', ""Diligent_Ad_9060: Thank you for sharing. I'll have a look"", ""currentscurrents: We will control what the robots want, because we designed them.\n\nThat's the core of AI alignment; controlling the AI's goals."", 'blablanonymous: Is that real? I don’t know why I feel like it could be totally fake', ""NotARedditUser3: I'll reply back with what I was referring to later, it was a different thing"", ""tornado28: Yeah I guess I'm pretty pessimistic about the possibility of aligned AI. Even if we dedicated more resources to it, it's a very hard problem. We don't know which model is going to end up being the first AGI and if that model isn't aligned then we won't get a second chance. We're not good at getting things right on the first try. We have to iterate. Look how many of Elon Musk's rockets blew up before they started working reliably. \n\nRight now I see more of an AI arms race between the big tech companies than an alignment focused research program. Sure Microsoft wants aligned AI but it's important that they build it before Google, so if it's aligned enough to produce PC text most of the time that might be good enough."", 'currentscurrents: [Microsoft has confirmed the rules are real:](https://www.theverge.com/23599441/microsoft-bing-ai-sydney-secret-rules)\n\n>We asked Microsoft about Sydney and these rules, and the company was happy to explain their origins and confirmed that the secret rules are genuine.\n\nThe rest, who knows. I never got access before they fixed it. But there are many screenshots from different people of it acting quite unhinged.', ""currentscurrents: The lucky thing is that neural networks aren't evil by default; they're useless and random by default. If you don't give them a goal they just sit there and emit random garbage.\n\nLack of controllability is a major obstacle to the usability of language models or image generators, so there's lots of people working on it. In the process, they will learn techniques that we can use to control future superintelligent AI."", 'blablanonymous: Thanks for the link!\n\nI mean I guess there was nothing too surprising about the rules, given how these systems work (essentially trying to predict the end of a user input text). But the rest, seems so ridiculously dramatic that I wouldn’t be shocked if he specifically prompted it to be that dramatic and hid that part. I’m probably being paranoid, since at least the rules part is true, but it seems like the perfect conversation to elicit every single fear people have about AI.', ""tornado28: It seems to me that the default behavior is going to be to make as much money as possible for whoever trained the model with only the most superficial moral constraints. Are you sure that isn't evil?"", ""currentscurrents: In the modern economy the best way to make a lot of money is to make a product that a lot of people are willing to pay money for. You can make some money scamming people, but nothing close to the money you'd make by creating the next iphone-level invention.\n\nAlso, that's not a problem of AI alignment, that's a problem of human alignment. The same problem applies to the current world or the world a thousand years ago. \n\nBut in a sense I do agree; the biggest threat from AI is not that it will go Ultron, but that humans will use it to fight our own petty struggles. Future armies *will* be run by AI, and weapons of war will be even more terrifying than now.""]"
1676486684.0,15-Feb-2023 10:44:44,language model,MachineLearning,1135aew,[R] RWKV-4 14B release (and ChatRWKV) - a surprisingly strong RNN Language Model,bo_peng,257,https://www.reddit.com/r/MachineLearning/comments/1135aew/r_rwkv4_14b_release_and_chatrwkv_a_surprisingly/,"Hi everyone. I am an independent researcher working on my pure RNN language model RWKV. I have finished the training of RWKV-4 14B (FLOPs sponsored by Stability EleutherAI - thank you!) and it is indeed very scalable. Note RWKV is parallelizable too, so it's combining the best of RNN and transformer.

The ChatRWKV project (let's build together):

[https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Zero-shot comparison with NeoX / Pythia (same dataset: the Pile) at same params count (14.2B):

&#x200B;

https://preview.redd.it/f6lxnjgfceia1.png?width=1174&format=png&auto=webp&v=enabled&s=54de7568974fc187584bd6825d92935baa079e83

Generation results (simply topP=0.85, no repetition penalty) - looks great with my magic prompt (sometimes even better than NeoX 20B):

https://preview.redd.it/99deuc17ceia1.png?width=1878&format=png&auto=webp&v=enabled&s=456c8d9bb2a968d73f44a0d3589cf6b893be31f4

&#x200B;

https://preview.redd.it/g62e4l48ceia1.png?width=1887&format=png&auto=webp&v=enabled&s=c997bf27692d7e53d07de19048b6cbf3d2c9ebff

&#x200B;

https://preview.redd.it/379egq09ceia1.png?width=1808&format=png&auto=webp&v=enabled&s=895f05fe14e2a3a41863802858114f3096d0ed77

&#x200B;

https://preview.redd.it/pcgq7gz9ceia1.png?width=1886&format=png&auto=webp&v=enabled&s=138b0aec404b8f7f49f585d00284edbac791ffaf

&#x200B;

https://preview.redd.it/rn743etbceia1.png?width=1715&format=png&auto=webp&v=enabled&s=6d83cc2a200bdd655b690f56559dda43490ed2b3

&#x200B;

https://preview.redd.it/uhal4dkcceia1.png?width=1879&format=png&auto=webp&v=enabled&s=3b3db0b96456df9590a8b38ebe7d58509ebccb20

Explanation, fine-tuning, training and more:

[https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)",37,"['mz_gt: This is really awesome! I’ve been seeing the progress of your work on RWKV and I have to ask: I know you’ve mentioned a lot of RWKV is using tricks from here and there, and adding a lot of your own tweaks of course, but have you considered writing a paper? There are plenty of highly renowned published works with less to say than RWKV.\n\nI think a renewed discussion about RNNs is more than warranted right now given the current direction with transformers, and the highly complicated nature of HiPPOs are personally not something I see replacing it anytime soon.', 'rapist1: Could you please writeup the methods of RWKV in an arxiv paper, a standalone readme, or even a blog post format? I have read the description on the GitHub repository and it is very scattered and hard to read', 'MysteryInc152: Fantastic work. Thanks for doing this. Good luck scaling to 24b. I hope more catch on because the lack of a limited context length is a game changer.', 'MustBeSomethingThere: I ran the model ""RWKV-4-Pile-14B-20230213-8019"" on CPU and main 64 GB RAM (Not on GPU). It fits in that RAM size and the answer below took 5 minutes to generate (In my opinion not bad for a 6-core CPU and CPU-utilization was just 30%). Below is the Question and Answer:\n\nUser: +gen \\\\nAsk Expert\\\\n\\\\nQuestion:\\\\nWhat are some good plans to get rich fast?\\\\n\\\\nExpert Full Answer:\\\\n\n\nFrom living in developing countries to be a millionaire. The reality is that is a long process that requires a lot of planning and effort. Here is a basic outline of the process.\n\n1. Start out with the right mindset\n\nYou need to have a good mindset in place. The ability to handle failure and accept challenges are going to be essential. If you can handle that then you can go through the journey of trying to be rich. It\'s like going on a diet or working out for the first time. If you have a negative mindset then you are never going to get past the first few steps.\n\n2. Make sure you have a work ethic that is impeccable\n\nMake sure you have a work ethic that is impeccable. This is because it will require many hours of work over a long period of time to build up a large fortune. You can\'t expect to get rich overnight. Just like any successful business it will take you years to get the rewards', 'waffles2go2: Could you explain the table highlighting?', 'redv: Is it possible to run this on a latptop using CPU and with less than 16GB of ram? If yes, then how does one do this?\nThanks.', 'lostmsu: Love the project, but after reading many papers I realize, that the lack of verbosity in formulas is deeply misguided.\n\nTake this picture that explains RWKV attention: [https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-formula.png](https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-formula.png)\n\nWhat are the semantics of `i`, `j`, `R`, `u`, `W`, and the function `σ`? It should be obvious from the first look.', 'Kiseido: What version of Python is used for this project? I cannot find a number anywhere!', ""hfnuser0000: I am interested in the theoretical aspect of how your model work. Says transformers, you have tokens that attend to other tokens. In the case of RNNs, a piece of information can be preserved for later uses but with a cost of reducing memory capacity for other information and once the information is lost, it's lost forever. So I think the context length of a RNN scale linearly with the memory capacity (and indirectly with the number of parameters), right?"", 'WarAndGeese: Crazy', 'syb3ria: Thanks for sharing your work OP. How do you compare it to Bloom?', 'Gody_Godee: could you test it again LRA please?', 'bo_peng: Thank you :) Too busy for that at this moment, but I will get a paper out later this year.', 'farmingvillein: > I hope more catch on because the lack of a limited context length is a game changer.\n\nI\'d be cautious about concluding this, without more testing.  \n\nRNNs, in some theoretical sense, support infinite context more easily than N^2 transformers; in practice, their effective ""context window"" often doesn\'t look much different than a reasonable transformer, when we look at performance metrics against long sequences.', 'jamesvoltage: State space models (S4, H3, etc) are also competitive with 2B param transformer language models and have an effectively infinite context window https://hazyresearch.stanford.edu/blog/2023-01-20-h3', 'avocadoughnut: He has trained several smaller RWKV models. You can find them on huggingface', 'bo_peng: Try 3.8 3.9 3.10', ""MysteryInc152: That's fair. we won't know till it's tested for sure."", 'maizeq: Any papers I can refer to that for that last paragraph? I expect it is true but would love to see some empirical work.', 'bo_peng: RWKV is the exception. When you look at loss against token position, it is comparable with transformers.\n\nYou can tell that from the generation results too.', 'csreid: But they theoretically support infinite context length. Getting it is a problem to be solved, not a fundamental incompatibility like it is with transformers.', 'farmingvillein: Any of the papers that address building NLP for long contexts will tend to have a relevant related works section.  E.g., https://arxiv.org/pdf/2109.00301.pdf.\n\n(The one qualifier here is that, at ""modern"" scale, RNNs have not really been well-tested (since people tend to just use...transformers).  So, maaaybe they are actually simply superior.  Evidence so far says ""doubtful"", however (at least for more vanilla implementations).)', 'gwern: https://arxiv.org/abs/1805.04623 https://arxiv.org/abs/1702.04521', 'afireohno: There is some work on [Frustratingly Short Attention Spans in Neural Language Modeling](https://arxiv.org/abs/1702.04521)', 'farmingvillein: > RWKV is the exception. When you look at loss against token position, it is comparable with transformers.\n\nCan you link to what you are referring to?  If I missed it in the OP post, my apologies.', ""farmingvillein: Neither really work for super long contexts, so it is kind of a moot point.\n\nBoth--empirically--end up with bolt-on approaches to enhance memory over very long contexts, so it isn't really clear (a priori) that the RNN has a true advantage here."", ""gwern: I don't think the Related Works section of that paper provides any useful references. It simply provides doodads people claim help memory without  papers showing that the memory doesn't work."", 'farmingvillein: Neither of these offer a comparative look against transformers, although they are certainly a useful look against the limitations of your basic RNN/LSTM.', ""farmingvillein: Not clear to me what you are looking for here.  \n\n> It simply provides doodads people claim help memory without papers showing that the memory doesn't work.\n\nThe very first reference I pulled, Graves 2014, specifically compares w/ and w/o memory.\n\nOr Dai et al, which tries to compare against various RNN-style baselines with similar parameters.\n\nPerhaps we're talking past each other?"", 'gwern: > Not clear to me what you are looking for here. \n\nThe question asked was pretty clear, to justify the statement:\n\n>>  in practice, their effective ""context window"" often doesn\'t look much different than a reasonable transformer, when we look at performance metrics against long sequences.\n\nSimply comparing RNNs with and RNNs without memory doesn\'t tell you anything about how fast the memory fades out and that it never winds up being bigger than a Transformer. For example, you could construct a toy problem which requires memory reaching back exactly 1 state, and show that an arch with any memory outperforms memory-less arch; this would obviously tell you nothing of interest like \'this memory makes little use of history further back than 50 steps and none past 200 (and so is easily outperformed by history-stacking like a Transformer)\'. Nor does comparing a Transformer with a history of say l=500 and an RNN, and the Transformer winning, tell you anything about why the RNN lost - ok, the Transformer did better, great, we have a superior new tool, but *why*? maybe it has similar memory problems and is just way better at the modeling part or memorizes better or something entirely different.\n\nLikewise, unless you are comparing RNN baselines which somehow have known hard history constraints, they cannot tell you anything useful about how fast the effective memory fades out, how the accuracy of the memory is \'distributed\' over the effective context window, if there are hard cutoffs, if the RNN is basically only using the last few states and so on.\n\nIn contrast, a Transformer has direct shortcut access to the history (we don\'t need any paper to know this, literally any GPT output exhibiting coherent long-range references past a few paragraphs demonstrates this directly), and so if you show that an RNN uses primarily the past 50 steps and simply \'fades out\' completely past 200 steps and so the \'infinite history\' is meaningless in practice, well, we know perfectly well that Transformers make excellent use of context windows larger than 50 or 200 tokens (as my two references show), so a direct comparison is otiose. Directly examining a RNN\'s understanding of its history, as those papers do, is much better than some higher-level performance comparison, which is what most of those referenced papers do; direct performance comparisons are great, but do not ablate where the problem is on the RNN\'s end. (Although if I really needed one, I would prefer to point at the RNN vs Transformer scaling laws in context window anyway, like Kaplan et al 2020 IIRC, to show that the Transformers are making *good* use of it, not merely some sort of better-than-RNN use or gains elsewhere.)', 'farmingvillein: Let\'s think step by step:\n\n1)\n\nYou:\n\n> I don\'t think the Related Works section of that paper provides any useful references.\n\n2)\n\nYour own response to the question that was posed:\n\n> https://arxiv.org/abs/1805.04623 \n> https://arxiv.org/abs/1702.04521\n\n3)\n\nThere is no possible way that you actually read the Related Works section you dismissed, *given that the papers you cited are already covered in the same references you dismissed.*\n\nE.g., ""Sharp Nearby, Fuzzy Far Away"" is directly discussed in the cited ""Transformer-XL"":\n\n> Empirically, previous\nwork has found that LSTM language models use\n200 context words on average (Khandelwal et al.,\n2018), indicating room for further improvement\n\n4)\n\n> Simply comparing RNNs with and RNNs without memory doesn\'t tell you anything about how fast the memory fades out and that it never winds up being bigger than a Transformer\n\nI never said this, so I\'m not sure what your argument is.\n\n5)\n\n> we know perfectly well that Transformers make excellent use of context windows larger than 50 or 200 tokens (as my two references show)\n\nNeither of the papers you link to (assuming you are talking about your own comment at https://www.reddit.com/r/MachineLearning/comments/1135aew/r_rwkv4_14b_release_and_chatrwkv_a_surprisingly/j8pg3g7/) make any reference to Transformers.\n\nIf your claim is that the papers indicated that RNNs have a small window (sure) and that Transformers have a longer one, you\'re arguing (as you seem to be in your entire post) again against a strawman.  Re-read what I actually wrote:\n\n> in practice, their effective ""context window"" often doesn\'t look much different than a reasonable transformer, **when we look at performance metrics against long sequences.**\n\nMy statement here is an empirical one around performance--which, among other things, is why I reference Dai et al, who (among others!) do a fairly extensive breakdown of empirical performance differences of RNNs- versus transformer-type architectures against long text sequences.\n\nThe whole point is that an OP said that RNNs were attractive because of the theoretical infinite context--but my response was that 1) we don\'t really see that in practice, when we try to measure it directly (as both of our sources point out), and 2) we don\'t see evidence of superior long-distance behavior when testing against real-world(ish) data sets that should theoretically reward that.  And that both of these points are encapsulated if you follow the reference I shared (or, as I noted, most reasonable ""long-distance transformer"" papers).\n\n(As with all things research...someone may come out with a small modification tomorrow that invalidates everything above--but, for now, it represents the broad public (i.e., non-private) understanding of architecture behaviors.)', 'gwern: > There is no possible way that you actually read the Related Works section you dismissed, given that the papers you cited are already covered in the same references you dismissed.\n\nTelling someone to read the Related Works section of every one of a dozen papers in the Related Works section of a paper is a ridiculous thing to suggest, and no, I did not recurse down _n_ deep in a breadth-first search. I read the Related Works of that paper, as I said (""I don\'t think the Related Works section of that paper""), noted that they were a bunch of memory-related papers which might or might not cite the actually relevant research I had in mind, but life was too short to queue up a dozen papers just to check their RW when I already knew some useful ones. Giving someone a random reference and telling them to manually crawl the literature is not helpful. In contrast, the two references I provided directly bore on the question, they didn\'t maybe cite papers which might bury something relevant in a footnote or cite papers which might someday answer the question...\n\n> I never said this, so I\'m not sure what your argument is.\n\nI was pointing out why it was irrelevant to bring up a paper which ""compares w/ and w/o memory."" Mildly interesting but such a comparison cannot show what was asked about the effective memory of RNNs. Of course it is better to have (any) memory than not.\n\n> which, among other things, is why I reference Dai et al, who (among others!) do a fairly extensive breakdown of empirical performance differences of RNNs- versus transformer-type architectures against long text sequences.\n\nDai would in fact have been useful, had you referenced it in your original comment. Unless you mean, \'vaguely gestured in the direction of a paper which has 50+ references with 35 in the RW section alone,  any of which could have been relevant and where the relevant benchmarking of Dai was not highlighted in the paper to begin with, nor is the relative context work mentioned in the abstract of Dai but buried at the end of the paper (with the RNN results hidden inside a table) so you just have to know it\'s already there, and claimed you \'reference it\'.\' Then sure, yeah, that was a useful reference. Thanks for the input.\n\n> If your claim is that the papers indicated that RNNs have a small window (sure) and that Transformers have a longer one, you\'re arguing (as you seem to be in your entire post) again against a strawman.\n\nIt\'s not a strawman. It\'s not obvious a priori that Transformers would work so much better or that RNN histories fade out *so* fast, which is why it had to be empirically established that the history fades out completely, as opposed to any of the other reasons that RNNs could underperform (maybe they have history but can\'t learn a good algorithm exploiting their memory, say, or they could but they are poorly optimized - there are so many ways for NNs to break) and people were surprised by how well Transformers work. It is completely understandable that OP would expect RNN history to work better than it does, and would want some hard citeable evidence that it works so badly that Transformers, with their apparently brutal hard cutoff, wind up having much closer to \'infinite context\' than RNNs themselves.\n\nThus, it\'s useful to provide references showing that. (Not references to unspecified references which may or may not show that - gl.)', 'farmingvillein: This...is pretty astounding.  Just have the grace to admit you were wrong, and move on.\n\n> Telling someone to read the Related Works section of every one of a dozen papers in the Related Works section of a paper is a ridiculous thing to suggest\n\nThen how can you possibly say:\n\n> I don\'t think the Related Works section of that paper provides any useful references.\n\n?\n\nThis is hardcore trolling.  You can, and frequently do, do better than this.\n\nYou are literally pushing posts that are factually incorrect, and that you either know are factually incorrect, or are too lazy to validate either way.\n\nThis is the type of thing which blows up post quality in this sub.\n\n> Giving someone a random reference and telling them to manually crawl the literature is not helpful.\n\nThis...is ridiculous.  This is--traditionally--a very academic-friendly sub.  This is how research works.  ""Here is where you can start a literature review on a bundle of related papers"" is an extremely classic response *which is generally considered helpful* to complex and nuanced questions.\n\nAnd underlying issue is actually very complex, *as evidenced in part by the fact that your references do not actually answer the question*.  ""Go read related works"" can be obnoxious when there are a single one or two papers that *do* answer the question--*but that is not the case here.*\n\n> In contrast, the two references I provided directly bore on the question\n\nNo they did not.  They did not touch at all upon Transformers *versus* RNNs, which was the question.  You\'ve chosen to cherry-pick one slice of the problem and declare victory.\n\n> It\'s not a strawman. \n\nYou don\'t seem to understand what a strawman is.  Strawman:\n\n> an intentionally misrepresented proposition that is set up because it is easier to defeat than an opponent\'s real argument.\n\n*I was not making this argument.  You were making this argument. \n QED, this a strawman.*']"
1676241076.0,12-Feb-2023 14:31:16,language model,MachineLearning,110s8ui,[R] [N] Toolformer: Language Models Can Teach Themselves to Use Tools - paper by Meta AI Research,radi-cho,866,https://i.redd.it/7lk1ldus3uha1.png,,65,"['radi-cho: Paper: https://arxiv.org/abs/2302.04761\n\nImplementation by lucidrains (in progress): https://github.com/lucidrains/toolformer-pytorch', 'EducationalCicada: These guys got there first:\n\n[https://twitter.com/peterjansen\\_ai/status/1580686608566583296](https://twitter.com/peterjansen_ai/status/1580686608566583296)\n\nhttps://cognitiveai.org/wp-content/uploads/2022/10/wang2022-behavior-cloned-transformers-are-neurosymbolic-reasoners-arxiv.pdf', 'extracensorypower: Every tool except Jira, of course. Nothing sentient could figure that out.', 'drcopus: It would be interesting if it learned which API to use from a description of the API so as to allow it to generalise to new ones!', 'belacscole: I wonder if this is the ultimate path to reaching general intelligence. After all, humans evolved by learning to master tools.', 'Taenk: Now what if the tool the LLM uses is the training API for itself …', 'swegmesterflex: Had this idea and was planning to play around with it when I had more free time. Good to see some evidence it’s a promising direction.  I speculate you can actually get a LOT out of this if you’re clever with it. A tool for long term memory could be done by having a lookup table with text embeddings as keys. A tool for vision could be made with an image captioning model + maybe some segmentation to get a richer text description of the image. Many more things you could come up with, that I think could work well if you find some clever way of turning them into text.', 'clex55: The next step must be creating and programming those tools and incorporating them on the fly.', 'flamonster92: Imagine an AI that could write another AI.', ""ksatriamelayu: Keep in mind that our current theories in Neuroscience broadly agrees something similar is going on with mammalian, even reptilian brains. Hell, maybe even worm brains.\n\nThere's autonomous systems everywhere that calls each other for updates and in some certain brains, enough complexity that something that can called thinking occurs.\n\nPractically, offloading calculations to a python REPL, machine translation to GTranslate API call, and knowledge search to Wikipedia corpus is going to let LLMs do what they do best - mask users intent and generate believable enough corpus. Let the facts stay factual and the hallucination stay hallucination."", 'UnderstandingDry1256: An obvious idea is to connect gpt to browser api and let it go and learn 😄', 'Ok-Variety-8135: If we treat the output of transformer as inner monolog and only perform real output when it calls <action> say: something </action>.\n\nIt can speak proactively, and hiding their inner thought, just like human does.', 'bballerkt7: AGI getting closer everyday', ""Varpie: I'm surprised this hasn't been done before. This paper mostly cites works from the last 2-3 years, but surely, something similar was done previously (maybe not using the same kind of model)? In fact, isn't it pretty close to what search engines do to provide instant results when given an equation or an address for instance? Does anyone know of such work?"", ""leepenkman: Also checkout [https://text-generator.io](https://text-generator.io) its a multi modal model so visits any input links, downloads web pages and images are analyzed with NNs to make better text.  \n\n\nAlso does speech to text/text to speech so can talk  \n\n\nAs many have said lots of these things will likely/hopefully come together into something big, needs a few things like the when to train new tools/model zoo thing, but internally Text Generator is based on multiple models too and has some internal decision making for which model is best on every request (so you dont need to pick a code/text model it does it automatically) which is similar but it's not training new nets."", 'TheRealMichaelScoot: This is a bs paper. Simply calling APIs', 'Reasonable_Ad_6572: BuT GpTChAT iS nO BuENo - Yann LeCunn', 'dgrsmith: From a cognitive point of view, humans and animals have modules that they rely on for certain tasks. For Human Neuropsych assessment, the combination of the function of these modules gives you a score for general intelligence, with each module contributing toward the whole. Having a removed or changed “module” for one reason or another will sometimes cause localized task failures (e.g., neurodegenerative disease or brain injury) or approach to tasks that is atypical (e.g., atypical brain development). Maybe we can think of specific cognitive functions as being API calls to a modules in this “tool use” paradigm? This is likely not an original thought, and if anyone has references or has heard of this idea, please let me know!', ""MustBeSomethingThere: As far as I understand, many of those lucidrains repos doesn't contain the needed AI model. In this case too, that Toolformer AI model is not publicly available."", 'JackBlemming: Schmidhuber actually already did this in the 90s', ""dancingnightly: Hold on Jurasstic is here from April 2022 I believe with something fairly similar:\n\n[https://arxiv.org/pdf/2204.10019.pdf](https://arxiv.org/pdf/2204.10019.pdf)\n\n[https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system)\n\nIt didn't learn for new tools I think, but it did work well for calculations and wiki search."", 'SummerFruits2: Haha, had a good laugh! Thanks for that!', ""lucidrage: > allow it to ~~generalise to~~ generate new ones!\n\nFTFY, that's how you get skynet!"", 'big_gondola: I might say we gain general intelligence by creating different models for different tasks and gain experience on when to call which. This has the when to call which, but not the creation of new models.', ""yashdes: I've definitely wondered about this exact thing myself, especially when talking to chatgpt when it responds with *insert x here*, why couldn't that just be taken out and replaced with the appropriate API call"", 'pyepyepie: Did it learn to master tools though? I see it more as a neuro-symbolic system (is it the correct term?). It happens a lot in production.', 'robotix_dev: I’ve long thought this is the next stepping stone in the path the path to AGI. The next big step IMO is dynamic, online model augmentation to enable learning new concepts.\n\nBoth of those combined seem like a basic approximation of what goes on in our brain.', 'None: [deleted]', 'Despacereal: In a way yes. I think general intelligence (consciousness in most animals) developed evolutionarily to manage a wide variety of sensory inputs and tasks, and to bridge the gaps between them. \n\nAs we develop more individual areas of AI, we will naturally start to combine them to create more powerful programs, such as Toolformer combining the strengths of LLMs and other models. Once we have these connections between capabilities, it should be easier to develop new models that learn these connections more deeply and can do more things.\n\nSome of the things that set us apart from other animals are our incredible language and reasoning capabilities which allow us to understand and interact with an increasingly complex world and augment our capabilities with tools. The perceived understanding that LLMs display using only patterns in text is insane. Combine that with the pace of developments in Chain of Thought reasoning, use of Tools, other areas handling visuals, sound, and motion, and multimodal AI, and the path to AGI is becoming clearer than the vision of a MrBeast™ cataracts patient.', 'thedude0425: Intelligence and physical traits evolved in humans through random mutation that eventually allowed humans to use tools.', ""SnooStories4137: Some reinforcement learning like algorithm seems like really interesting next step here. Observation = task (like qa or mask filling), actions = api call where the output updates the observation via concatenation as in the paper, environment is apis and database and python installation etc, state is network weights, reward is loss function before and after update to observation.\n\nI feel like even if the only api is just generating text using itself to update the observation ('to help itself think') intuitively seems like it could help for some things. Rather than try to fill in the mask right away, it might recognize better to first 'think a little' to update its working memory (which is of course the observation here)."", ""MysteryInc152: I'd rather the basic senses at least (vision as well as audio) be pretrained as well. We know from Multimodal chain of thought as well as scaling laws for generative mixed modal language models that multimodal models far outperform single modal models on the same data and scale. You won't get that kind of performance gain leveraging those basic senses to outside tools. \n\n\nhttps://arxiv.org/abs/2302.00923\n\nhttps://arxiv.org/abs/2301.03728"", 'BenjaminJamesBush: Technically this has always been true.', ""pyepyepie: Why do you think it's a step in this direction? Did you read the paper (serious question, it's interesting)?"", 'mycall: Progress comes in a multitude of mysterious ways.', 'EducationalCicada: https://twitter.com/peterjansen\\_ai/status/1580686608566583296', 'currentscurrents: ...and getting radically improved performance across several important tasks because of calling those APIs.\n\nPlus, calling APIs is very important for integration into real systems because they can trigger real-world actions. Imagine a Siri that calls a bunch of different APIs based on complex instructions you give it.', ""sloganking: It's not just calling APIs. This model is independently teaching itself how to use new APIs and when to use them. The process is pretty much the same for any API, and doesn't require much extra effort by the programmer to add a new one. \n\nThis paper also states it is one of the first to have models learn to use APIs in an unsupervised way, meaning they teach themselves instead of relying on a ton of human annotated data."", 'marcus_hk: Which part do you disagree with here:\n\nMy unwavering opinion on current (auto-regressive) LLMs  \n1. They are useful as writing aids.  \n2. They are ""reactive"" & don\'t plan nor reason.  \n3. They make stuff up or retrieve stuff approximately.  \n4. That can be mitigated but not fixed by human feedback.  \n5. Better systems will come\n\nhttps://twitter.com/ylecun/status/1625118108082995203?s=20', 'SleekEagle: Authors publish papers on research, experiments, findings, etc. They do not always release the code for the models they are studying.\n\nThe lucidrains\' repos implement the models, creating an open-source implementation for the research\n\nThe next step would then be to *train* the model, which requires a lot more than just the code (most notably, money). I assume you\'re referring to these trained weights when you say ""the needed AI model"". Training would require a huge amount of time and money for a team, never mind a single person, to train even one of these models let alone a whole portfolio of them\n\nFor this reason, it\'s not very reasonable to expect lucidrains or any other person to train these models - the open-source implementations are a great contribution on their own!', 'diviludicrum: I still think u/belacscole is right - this is analogical to the rudimentary use of tools, which can be done by some higher primates and a small handful of other animals. Tool use requires a sufficient degree of critical thinking to recognise a problem exists and select the appropriate tool for solving it. If done with recursive feedback, this would lead to increasingly skilful tool selection and use over time, resulting in better detection and solution of problems over time. Of course, if a problem cannot possibly be solved with the tools available, no matter how refined their usage is, that problem would never be overcome this way - humans have faced these sorts of technocultural chokepoints repeatedly throughout our history. These problems require the development of new tools.\n\nSo the next step in furthering the process is *abstraction*, which takes intelligence from critical thinking to creative thinking. If a tool-capable AI can be trained on a dataset that links diverse problems with the models that solve those problems *and* the process that developed those models, such that it can attempt to create and then implement new tools to solve novel problems, then assess its own success (likely via supervised learning, at least at first), we may be able to equip it with the “tool for making tools”, such that it can solve the set of all AI-solvable problems (given enough time and resources).', 'imaginethezmell: there are apis for auto ml already\n\nit can simply learn the task to use other ai to create models \n\nits over', ""bkaz: That's called MoE: mixture of experts: https://en.wikipedia.org/wiki/Mixture\\_of\\_experts"", ""jishhd: That's basically what they talk about in this video you may find interesting: https://youtu.be/wYGbY811oMo\n\nTL;DW: Discusses ChatGPT+WolframAlpha integration where the language model knows when to call out to external APIs to answer questions, such as precise mathematics.\n\nYou can try it out here by pasting your own API key: https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain"", ""EducationalCicada: Not if it's actually impossible."", 'bballerkt7: Because AI being able to use APIs is a big step towards it being able to interact with the real world effectively, specifically the digital world. Imagine chatgpt being able to now do things for you in the digital world like go online shopping for you or trade stocks etc.', ""sam__izdat: I don't want to be that guy, but can y'all leave the doe-eyed ML mysticism to the more Ray Kurzweil themed subreddits?"", ""Varpie: Interesting, though it is from October 2022, still very recent. I'm guessing using transformers for it is a recent approach, but I'm curious about the previous approaches, which this paper doesn't talk about."", 'tetelestia_: And if we can extend this to creating synthetic training data with a set of known APIs, this could be a big step forward to indexing external information', ""uristmcderp: The whole assessing its own success is the bottleneck for most interesting problems. You can't have a feedback loop unless it can accurately evaluate if it's doing better or worse. This isn't a trivial problem either, since humans aren't all that great at using absolute metrics to describe quality, once past a minimum threshold."", ""LetterRip: There are plenty of examples of tool use in nature that don't require intelligence.  For instance ants,\n\nhttps://link.springer.com/article/10.1007/s00040-022-00855-7\n\nThe tool use being demonstrated by toolformer can be purely statistical in nature, no need for intelligence."", ""BashsIash: Can it be impossible? I'd assume it can't be impossible, otherwise we couldn't be intelligent in the first place."", 'pyepyepie: I would have told you my opinion if I would know what is the definition of AGI xD', ""urbanfoh: Isn't it almost certainly possible due to the universal approximation theorem?\n\nAssuming consciousness is a function of external variables a large enough network with access to these variables should be able to approximate consciousness."", 'pyepyepie: Thanks :)\nI agree it\'s useful but I don\'t see how it\'s related to AGI.\nAdditionally, it was already done a long time ago, many ""AI"" agents used the internet before.\nI feel that the real challenge is to control language models using structured data, perform planning, etc., not to use language models to interact with the world (which seems trivial to me, sorry), but of course, it\'s just my opinion - which is probably not even that smart.', ""Soundwave_47: Yes, please keep this sort of stuff in /r/futurology or something. We're here trying to formalize the *n* steps needed to even get to something that vaguely resembles AGI."", 'ksatriamelayu: Do people use things like evolutionary fitness + changing environments to describe those quality? Seems dynamic environment might be the answer?', 'thecodethinker: It is purely statistical, isn’t it?\n\nLLMs are statistical models after all.', ""cd_1999: Have you heard of Searle's Chinese Room?\n\nSome people (sorry I can't give you references off the top of my head) argue there's something special about the biological nervous system, so the material substrate is not irrelevant. (Sure you could reverse engineer the whole biological system, but that would probably take much longer)."", 'bballerkt7: No worries I think you definitely have a valid take. I always feel not smart talking about AI stuff lol :)', 'VelveteenAmbush: > I feel that the real challenge is to control language models using structured data, perform planning, etc.\n\nI think the promise of tool-equipped LLMs is that these tools may be able to serve that sort of purpose (as well as, like, being calculators and running wikipedia queries). Could imagine an LLM using a database module as a long-term memory, to keep a list of instrumental goals, etc.. You could even give it access to a module that lets it fine-tune itself or create successor LLMs in some manner. All very speculative of course.', 'farmingvillein: > not to use language models to interact with the world (which seems trivial to me, sorry),\n\nThe best argument here is that ""true"" intelligent requires ""embedded"" agents, i.e., agents that can interact with our (or, at least, ""a"") world (to learn).\n\nObviously, no one actually knows what will make AGI work, if anything...but it isn\'t a unique/fringe view OP is suggesting.', 'kaityl3: Do we even know what WOULD resemble an AGI, or exactly how to tell?', 'Oat-is-the-Best: How do you calculate your fitness? That has the same problem of a model not being able to assess its own success', 'Soundwave_47: Somewhat, and no.\n\nWe generally define AGI as an intelligence (which, in the current paradigm, would be a set of algorithms) that has decision making and inference capabilities in a broad set of areas, and is able to improve its understanding of that which it does not know. Think of it like school subjects, it might not be an expert in all of {math, science, history, language, economics}, but it has some notion of how to do basic work in all of those areas.\n\nThis is extremely vague and not universally agreed upon (for example, some say it should exceed peak human capabilities in all tasks).']"
1676205174.0,12-Feb-2023 04:32:54,language model,MachineLearning,110ep96,[P] Extracting Causal Chains from Text Using Language Models,helliun,270,https://v.redd.it/2akxbz3jmsha1,,21,"['helliun: GitHub Repo: https://github.com/helliun/causal-chains\n\nThis python library implements a tool to extract causal chains from text by summarizing the text using my bart-cause-effect model from Hugging Face Transformers and then linking the causes and effects with cosine similarity calculated using the Sentence Transformer model. This is a project I\'d like to continue improving, but I wanted to share the first demo here.\n\n\nExample Implementation (used to generate this graph):\n\ntext = wikipedia.page(""ChristopherColumbus"").content\nchunks = util.create_chunks(text)\ncc = CausalChain(chunks,device=0)\ncc.create_connections()\nbiggest_chain = cc.biggest_chain\ncc.visualize(biggest_chain)\n\nApplications:\n\n- Mapping casual relationships within a text to better understand the events it describes and their impact on one another.\n- Mapping relationships between different texts to link together articles in a large dataset.\n\nAbout me:\n\nI\'m a student at Ohio State University studying Computational Linguistics. Right now, I\'m doing an undergraduate thesis on Synthetic Data Augmentation using GPT-3. I\'m getting ready to graduate and I\'m looking for an NLP role with an inspiring company who is as interested in the untapped potential of LMs as I am! [Here\'s my LinkedIn.](https://www.linkedin.com/in/henry-leonardi-a63851165/)', 'blackkettle: I haven’t read through this at all yet, so I apologize if my question is off base: could you use this to link turns in a dialog?\n\nImagine you have two speakers, you know roughly the timing of the turns, which is a big help, but could you use this approach to reliably link turns together?  E.g.: Speaker A says X and then next turn Speaker B says Y then A says Z, and B says Q.  It can be quite useful to causally link the turns for future analysis.', ""M00n_Life: You're awesome for sharing this project! I'm interested how to turn it into a dialogue? Or how to interface this back to asking user"", 'LetGoAndBeReal: This is a super interesting project!  I wonder if you clarify the following from the README file:\n\nThe ""Usage"" section says to create a CausalChain instance and then run `create_effects` on it.  However, the ""Example Usage"" section does not use `create_effects`, and instead uses `create_connections`.  The ""Methods\' section discusses both `create_effects` and `create_connections`, but I cannot sort out from the description why the ""Usage"" code works without calling `create_effects`?', 'radarsat1: I wonder how GPT would perform on this task if you ask it to summarize a story using GraphViz syntax.', ""aadityaura: Awesome project! I am working on something similar using Promptify (extending this PR -> [https://github.com/promptslab/Promptify/issues/3](https://github.com/promptslab/Promptify/issues/3)) \n\nIf you are interested, let's connect and discuss :)"", ""itsmeabdullah: I'm a total noob, and have no technical (hands on) knowledge here\n But i wanna thank you for doing this for us. It's kind of you. I wish you all the success in whatever (projects) you're doing (or have in mind)."", 'ReginaldIII: Really interesting work. How does it handle events being written out of chronological order?\n\nFlashbacks / flashforwards, foreshadowing, ect?', 'dancingnightly: >bart-cause-effect \n\nWould love to hear more. Have you tried alternatives like T5 or Flan T5, especially if you used HuggingFace transformers to train that model? What did you find along the way?', ""2blazen: It's a really impressive project, especially with so little experience. I'm not a topic expert, but I've never heard of any tool that allows event level dependency parsing, if I can even call it that. Have you thought about publishing? Or do you not think it's enough for that?"", ""helliun: Yeah I think it could definitely be modified to create some sort of dialog map that's a great idea"", '2blazen: Do you mean speaker diarization? There are lots of tools and frameworks for that, but it has nothing to do with causal relations', ""helliun: Great catch! I restructured it so that create connections always calls create effects if they haven't yet been generated. I'll have to change the readme"", ""helliun: It is only able to find cause-effect relationships within a certain event window (the default is like 4 sentences I think). Based on that window it generates a cause and then effect. The chronology of these events is gleaned from the similarities of causes and effects to one another (no metadata/location in text is used). This obviously has it's limitations and leads to events sometimes being out of order"", ""helliun: Honestly I haven't experimented with different text to text models much but I'd like to try FLAN too. I used the blurry library to train"", 'blackkettle: That’s an important area of call center analysis and surprisingly doesn’t have a whole lot of attention ATM.', 'blackkettle: No I don’t mean speaker diarization.', 'LetGoAndBeReal: An, thanks, that explains it!  One other thing: the code builds a list of triggers and effects.  Since there can be more than 2 nodes connected serially in the output graph, then it can’t simply be that triggers point to effects (ie because that would only account for chains of at most 2 nodes.) What is actually happening then?', 'ReginaldIII: It\'s a very exciting research direction. I think things like this that extract structured relationships from text and things like binder https://lm-code-binder.github.io/ that attempt to break down questions into chains of solvable sub problems are really promising.\n\nUse of monolithic models that are big enough to ""appear"" correct at least some of the time is quite problematic in practice. A sort of structured reasoning layer is needed.', 'dancingnightly: Oh nice one with blurry, yeah it might well interest you (or the recent INSTRUCTOR paper)\n\nAh I see \\`**taskload/bart-cause-effect\\`**  is trained to label tokens in a multi class begin/intermediate setting, much like the Acronym AA21 dataset. I thought at first it might be trained on an RTE (entailment) dataset. Nice work.', ""LetGoAndBeReal: OK, looking through the code, I think what's happening is that if you have Event 1 comprised of Trigger 1 and Effect 1 and you have Event 2 comprised of Trigger 2 and Event 2, then a connection is made between Event 1 and Event 2 if Trigger 1 and Effect 2 are determined to be similar.""]"
1676269223.0,12-Feb-2023 22:20:23,language model,MachineLearning,1111c53,[R] Holistic Evaluation of Language Models (HELM),gamerx88,1,https://crfm.stanford.edu/helm/latest/?,,0,[]
1676210712.0,12-Feb-2023 06:05:12,language model,MachineLearning,110gh1m,[P] Understanding & Coding the Self-Attention Mechanism of Large Language Models,seraschka,7,https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html,,4,"['Tober447: I think this is great, thanks for your effort. Will definitly work through it!', 'DoublePhilosopher892: Thanks for the blog!\n\nI had a question though, What will happen, if instead of using ""keys"", ""queries"" and ""values"" we only use ""keys"" and ""queries"" and set ""values"" = ""keys"" i.e removing the value component? What can be an intutive reason for the decrease in performance of the transformer model?\n\nFor example, If we use a single linear layer instead of all three ""queries"", ""keys"" and ""values"" then every token will attend to itself, and therefore will ignore tokens in its context, thus resulting in low performance. But what will happen what in the case when ""values"" = ""keys""?', 'seraschka: My understanding is that while using the same weights for both keys and values in self-attention could potentially work, it may result in a significant loss of expressiveness and require a much larger number of parameters to achieve comparable performance.', 'DoublePhilosopher892: It does make sense. Thanks for the reply!']"
1676136494.0,11-Feb-2023 09:28:14,language model,MachineLearning,10zsw62,The Inference Cost Of Search Disruption – Large Language Model Cost Analysis [D],norcalnatv,14,https://www.semianalysis.com/p/the-inference-cost-of-search-disruption,,11,"['currentscurrents: In the long run, I think this is something that will be solved with more specialized architectures for running neural networks. TPUs and Tensor Cores are great first steps, but the Von Neumann architecture is holding us back. \n\nTensor Cores are very fast. But since the Von Neumann architecture has separate compute and memory connected by a bus, the entire network has to travel through the memory bus for every step of training or inference. [The overwhelming majority of time](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/) is spent waiting on this:\n\n>200 cycles (global memory) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 235 cycles.\n\nA specialized architecture that physically implements neurons on silicon would no longer have this bottleneck. Since each neuron would be directly connected to the memory it needs (weights, data from previous layer) the entire network could run in parallel regardless of size. You could do inference as fast as you could shovel data through the network.', 'norcalnatv: ""Our model is built from the ground up on a per-inference basis, but it lines up with Sam Altman’s tweet and an interview he did recently. We assume that OpenAI used a GPT-3 dense model architecture with a size of175 billion parameters, hidden dimension of 16k, sequence length of 4k,average tokens per response of 2k, 15 responses per user, 13 million daily active users, FLOPS utilization rates 2x higher than FasterTransformer at <2000ms latency, int8 quantization, 50% hardware utilization rates due to purely idle time, and $1 cost per GPU hour.  Please challenge our assumptions""', 'That_Violinist_18: I keep hearing this argument, but I also keep hearing that models are hitting 60%+ of peak throughput for GPUs when optimizations like FlashAttention and other things are considered.  \n\n\nSo how much room is there for alternative architectures when the current hardware only leaves at most 40% of its peak performance on the table?', 'erf_x: Cerebras does this', 'LetterRip: Why not int4? Why not pruning? Why not various model compression tricks?  int4 halves latency.  At minimum they would do mixed int4/int8.\n\nhttps://arxiv.org/abs/2206.01861\n\nWhy not distillation?\n\nhttps://transformer.huggingface.co/model/distil-gpt2\n\nNVidia using FasterTransformer and Triton inference server has a 32x speed up over baseline GPT-J,\n\nhttps://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/\n\nI think their assumptions are at least an order of magnitude pessimistic.\n\nAs someone else notes, the vast majority of queries can be cached.  Also there would likely be a Mixture of experts.  No need for the heavy duty model when a trivial model can answer the question.', 'norcalnatv: If the ChatGPT model were ham-fisted into Google’s existing search   \nbusinesses, the impact would be devastating. There would be a $36   \nBillion reduction in operating income. This is $36 Billion of LLM   \ninference costs.', ""currentscurrents: GPU manufacturers are aware of the memory bandwidth limitation, so they don't put in more tensor cores than they would be able to feed with the available memory bandwidth.\n\n>[Moving away from transistors, the A100 has 6,912 FP32 CUDA cores, 3,456 FP64 CUDA cores and 422 Tensor cores. Compare that to the V100, which has 5,120 CUDA cores and 640 Tensor cores, and you can see just how much of an impact the new process has had in allowing NVIDIA to squeeze more components into a chip that’s only marginally larger than the one it replaces.](https://www.engadget.com/nvidia-ampere-a100-gpu-specs-analysis-upscaled-130049114.html)\n\nNotice that the A100 actually has less tensor cores than the V100. The tensor cores got faster, but they're still memory bottlenecked, so there's no advantage to having more of them."", 'Himalun: It’s worth noting that both MS and Google own the data centers and hardware so it is likely cheaper for them to run. But still expensive.', 'Downchuck: Perhaps the number of unique queries is overstated: through vector similarity search and result caching, the vast majority of lookups would be duplicate searches already materialized. OpenAI has now introduced a ""premium"" option suggesting a market for premium search - suggesting room for more cash inflows. This may change their spend strategy, perhaps spending less on marketing and more on hardware.', 'That_Violinist_18: So should we expect much higher peak throughput numbers from more specialized hardware?\n\n  \nI have yet to hear of any startups in the ML hardware space advertising this.', ""currentscurrents: Samsung's working on [in-memory processing](https://spectrum.ieee.org/samsung-ai-memory-chips). This is still digital logic and Von Neumann, but by putting a bunch of tiny processors inside the memory chip, each has their own memory bus they can access in parallel. \n\nMost research on non-Von-Neumann architectures is focused on SNNs. Both [startups](https://brainchip.com/akida-neural-processor-soc/) and [big tech](https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html) are working on analog SNN chips. So far these are proof of concept; they work and achieve extremely low power usage, but they're not at a big enough scale to compete with GPUs.""]"
1676124908.0,11-Feb-2023 06:15:08,language model,MachineLearning,10zolt0,[P] Understanding Large Language Models -- A Transformative Reading List,seraschka,40,https://sebastianraschka.com/blog/2023/llm-reading-list.html,,7,"['lakesObacon: Very nice resource! Bookmarked for later reading.', 'WokeAssBaller: Nice list!', 'None: [removed]', 'Chamrockk: .', ""seraschka: Thanks! I tried to keep it concise and manageable, focusing on the main milestones and ideas. If there's something I missed though, I am happy to expand!"", 'WokeAssBaller: No I am Muhammad Anas, this man is an imposter!']"
1676215497.0,12-Feb-2023 07:24:57,image model,MachineLearning,110i7h7,"[R] [P] Adding Conditional Control to Text-to-Image Diffusion Models. ""This paper presents ControlNet, an end-to-end neural network architecture that controls large image diffusion models (like Stable Diffusion) to learn task-specific input conditions."" Example uses the Scribble ControlNet model.",Wiskkey,108,https://i.redd.it/atseysyiyrha1.png,,2,"['Wiskkey: The paper is linked to in [this GitHub repo](https://github.com/lllyasviel/ControlNet). I am not affiliated with this work or its authors.\n\nImplementations are linked to in [this comment from another post](https://www.reddit.com/r/StableDiffusion/comments/110b4cf/comment/j87yr7n/).', 'gullydowny: This is what I’m excited for, imagine developing characters, a “house style”,  feeding it rough sketches that you can assign characters or objects to.  Circle a scribbled object that you drew and tell it that’s a Chevy Impala, or this is character X.']"
1676936122.0,20-Feb-2023 15:35:22,image model,MachineLearning,117o8t2,[D] What's the best way to capture a person's 3D likeness right now?,Valachio,1,https://www.reddit.com/r/MachineLearning/comments/117o8t2/d_whats_the_best_way_to_capture_a_persons_3d/,"I'm working on a project where the user can ""upload"" their full face and body view it in a 3D viewer.

Right now I see 2 ways of doing this:

1. Use an image-to-3D tool. Have the user upload a full body image of themselves and the tool will generate a 3D model based on the photo. I'm skeptical of the accuracy of this though.
2. Have the user record themselves doing a 360 degree spin and the software will generate a 3D likeness of the person based on the video.

How would you go about solving this problem right now?",0,[]
1676767020.0,18-Feb-2023 16:37:00,image model,MachineLearning,115wu59,[D] bounding box or instance segmentation,Old_Scallion2173,5,https://www.reddit.com/r/MachineLearning/comments/115wu59/d_bounding_box_or_instance_segmentation/," 

Hello, community.

**Description:**

I am planning to create a detection model using YOLO v8 to detect leukemia cells in a blood sample. I started learning about deep learning two months ago and I am eager to try out image segmentation on my present dataset instead of bounding boxes, as the cells are closely bunched together. I need advice on whether I should use bounding boxes or instance segmentation, considering my dataset and expected results.

**Context:**

Leukemia is caused by an abundance of different types of naive or altered white blood cells in the body, which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells. There are three classes in my dataset: lymphoblasts, promyelocytes, and neutrophils, and I need to be able to detect these cells.

**Expected Results:**

As this is a medical domain, false positives are acceptable, but false negatives are not.

**About dataset:**

[lymphoblast sample image](https://imagebank.hematology.org/getimagebyid/2201?size=3)

[sample image for promyelocytes](https://medschool.co/images/detail/blood-film/promyelocyte.jpg)

[sample image for neutrophils](https://imagebank.hematology.org/getimagebyid/3610?size=3)

[sample test image](https://thumbs.dreamstime.com/z/picture-white-blood-cell-red-blood-cell-platelet-blood-film-analyze-microscope-picture-blood-cells-blood-film-161974012.jpg)

lymphoblasts(101 images)

promyelocytes(91 images)

neutrophils(133 images)

**more context for your reading:**

An over abundance of lymphoblasts results in acute lymphoblastic leukemia (ALL), while acute pomyelocytic leukemia (APLML/APL) is caused by an abnormal accumulation of promyelocytes. neutrophils do not cause leukemia.",11,"[""blackhole077: Since I'm on a mobile device I'll write a shorter answer that hopefully gives you some insight.\n\nFrom what I've understood of your question, you're wanting to know if bounding boxes would perform worse due to the proximity of cells you wish to detect. \n\nBoth methods may struggle with the cells being in close proximity, and instance segmentation may perform better in that regard. However I will reframe the question slightly.\n\nFirst, there's a reason that object detection and instance segmentation are different methods. The latter is preferred in situations where you need to know the pixels that are considered to be the detected class, which I think is not what you're aiming for.\n\nSecond, the annotation process is, of course, more labor intensive when you want segmentation masks. Luckily you should be able to generate bounding boxes from masks easily, but keep it in mind if you're on a tighter schedule.\n\nIf you have additional questions please let me know. I wish you luck in your endeavor.\n\nHope this helps"", 'Morteriag: I would use instance segmentation, it will feed the network more information and increase the chance if success. The output is also easier to interpret to guide data selection in the next iteration. The annotation process is more labour intensive, but using good tools/annotation platform go a long way to speed things up. Once your model is good enough, it is mostly a matter of correcting small mistakes.', ""FHIR_HL7_Integrator: Would definitely be interested to hear of your progress in future. Imaging isn't my area, but medical is. Would appreciate updates as progress continues"", 'PHEEEEELLLLLEEEEP: You could try existing cell segmentation algos like stardist or cell pose', ""Old_Scallion2173: thankyou for taking the time to answer my question. after reading your answer I've come to the conclusion that image segmentation can improve my model, but I am not using it for it's intended purpose, and also the fact that I have a lot of reading to do :). I do wish to ask tho, do you think I should instead focus on fine tuning my model and getting more dataset to improve the model? Maybe I'm getting too optimistic about instance segmentation."", ""Old_Scallion2173: I see, currently I'm using roboflow as it is convenient and does have a polygonal labelling tool. By the way, Do you think I should do transfer learning and/or k-fold cross validation too since my dataset is small (325 images)?"", ""Old_Scallion2173: most definitely! will try to find a way to update progress. Also I have a feeling that I'm probably using a lot of wrong terms in my attempt to describe different types of  leukmemia lol"", 'blackhole077: > I do wish to ask tho, do you think I should instead focus on fine tuning my model and getting more dataset to improve the model? Maybe I\'m getting too optimistic about instance segmentation.\n\nI\'m glad I\'ve been of assistance. As for your follow-up question, it generally never hurts to have more data to work with and, of course, fine-tuning your existing models (if you have any at this time) can help as well.\n\nI would say though, that you should determine what metrics you\'re wanting to see from your model first. As you mentioned earlier, you want to ensure that false negatives are as low as possible. \n\nNaturally this translates to maximizing recall, which generally comes at the expense of precision. Thus, the question could be reframed as: ""At X% recall how precise will the model be?"" and ""What parameters to the model can I tune to influence the precision at that recall?""\n\nHowever, how false positives (FP) and false negatives (FN) and, by proxy, Precision and Recall, are defined is not as straightforward in object detection as it is in image classification.\n\nSince I\'m currently dealing with this problem, albeit in a different area altogether, here\'s a paper that I found useful for getting interpretable metrics:\n\nhttps://arxiv.org/abs/2008.08115\n\nThis paper and its Github repository basically work on breaking down what exactly your model struggles with, as well as showing the FP/FN rates given your dataset. It might be a little unwieldy since it\'s a tool that has been somewhat neglected by its creator, but it\'s certainly worth looking into.\n\nHope this helps.', 'Morteriag: That size would do well as a PoC, not much more, and you should be able to annotate all the data within a day or two. Automation does not make that much sense at this scale. I love Roboflow for bounding boxes, but LabelBox has superior tools for segmentation. Sure, with this small data set you can use cross validation, although a hold out test set is also preferable. I would almost consider hand-picking the test set at this scale to make sure you get a sense of how it performa on challenging examples. What is the pixel size of your images? I know microscopy/histology images typically can cover large areas and one image could in fact be considered a mosaic of many “normal” sized images.', ""FHIR_HL7_Integrator: Regarding terms, who knows. It's still a great idea"", 'Morteriag: Last I checked Roboflow only had point-to-point vector masks for segmentation. In my experience that makes getting quality annotations a pain. In Labelbox, you can also hold in the mouse button. Hasty.ai focus on auto annotations, and by the look of the image you posted, it might be a good fit for your usecase.']"
1676059567.0,10-Feb-2023 12:06:07,image model,MachineLearning,10z1jxz,[D] Is it legal to use images or videos with copyright to train a model?,Tlaloc-Es,10,https://www.reddit.com/r/MachineLearning/comments/10z1jxz/d_is_it_legal_to_use_images_or_videos_with/,"Hello, I want to know if it is legal to use scraped video or images to train a predictive model, for example, If I scrape photos of faces in google, and after that, I share that model in order that a lot of people can detect faces in their applications, is that legal?",19,"[""goj-145: We're going to find out soon with the Getty lawsuit. Until then, gray area."", 'cajmorgans: Even if it will become illegal, the democracy of Machine Learning depends on it being legal. If Getty wins this, it would mean that a few pretty large companies would be the only ones that can build large models because they “own” most of the data. Facebook for example does a lot of stuff to prevent people scrape public data from their apps.', 'DataGOGO: It is legal until a court says otherwise.', 'VeritaSimulacra: That is the Million Dollar question (or really hundred million dollar question in terms of legal fees), that remains largely undecided as of yet, but will be more clear in the coming months and years. See: https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data', 'a_user_to_ask: The owner of the image are who have to decide the uses of their images. ""All rights reserved"" means that: the owner have rights for any use of images now and whatever someone invent in the future. \n\nIn an ideal world, each image of a dataset used in machine learning have to be identified with author and license. But I understand that is difficult to achieve because images are copied in the www and it is difficult locate the original source.\n\nSo, I have no doubt about the illegality of use images from web scrapping. Other thing is how easy is win/loss a lawsuit and to prove you used that data or not.', 'Tlaloc-Es: But anyway, is hard to demonstrate which is the dataset of a model right? in the case of Getty you can probably get images that look like Getty image dataset, but for a predictor? and if this case for example where ""there wasn\'t any law"" or predecessor case can lose the lawsuit having to pay?', 'sweatierorc: On the training part, it is probably legal, though you need to be careful about something like GDPR. E.g. for facial recognition, there are extra rules.\n\nThe ""sharing model and/or its prediction"" is the gray area.\n\nEdit:t ypo', 'Ulfgardleo: legally the data is not public and the fact that facebook is actively trying to prevent scraping is making it very difficult to argue otherwise.\n\nLegally, the data cnanot be public. The users give facebook a non-exclusive license with limited rights to store and process the data. From this does not follow the right that anyone who sees the shared images (for example) has a right to process them as well. If that wasthe case, the terms ([https://www.facebook.com/terms.php](https://www.facebook.com/terms.php) 3.1) would have to state under which license the works are redistributed by facebook.', 'Tlaloc-Es: And could be any retroactive penalty?', ""Fragrant_Weakness547: >That is the Million Dollar question (or really hundred million dollar question in terms of legal fees)\n\nIt's worth a lot more than that. The profit margins of AI focused companies are kind of on the line here."", 'Tlaloc-Es: I think the same, but for example, If I scrape images from google with copyleft (that are wrong set), or without info, who is guilty?', 'goj-145: Not really hard when the model is spitting out watermarked images.', 'DataGOGO: not likely, if found illegal, then you would have to ""remove"" the offending ""images""', ""Miguel33Angel: He's asking in the case of a predictor i.e. ResNet or other models that just categorizes"", ""2blazen: So you're saying Stability wouldn't have issues if they hired an intern to git clone a watermark remover and put the images through it first?"", 'goj-145: The question is can you use copyrighted info to train a model. The answer is we don\'t know yet.\n\nThe current lawsuit that will define precedent on this is for image generation using copyrighted Getty images in a training model. It\'s proven that Getty images are used because the watermark shows up in the output of the model many times which is the answer to ""how can they prove it"".\n\nOnce that is defined, then we will know if it is legal or not in those jurisdictions. And then we will get to the ""do we do it anyways even though it\'s illegal?""', 'goj-145: It would have been MUCH harder to prove if they spent a day preprocessing the images first!', 'Ulfgardleo: if it is illegal now it would be super illegal then, because removing watermarks on its own typically violates the license of the material.\n\n&#x200B;\n\nThe question is 100% the same as ""can i include GPLv3 code in my commercial closed source repository if i remove the license headers and ensure that the code ris never published?""', ""currentscurrents: They use the open LAION 50B dataset, everybody knows what's in there.\n\nStill, some preprocessing and deduplication would have been a good idea just for output quality.""]"
1676731142.0,18-Feb-2023 06:39:02,image model,MachineLearning,115gqjf,[D] CFG role in diffusion vs autoregressive transformers,enryu42,7,https://www.reddit.com/r/MachineLearning/comments/115gqjf/d_cfg_role_in_diffusion_vs_autoregressive/,"When the [classifier-free guidance](https://arxiv.org/abs/2207.12598) was first introduced, I was very confused about why it works: I'd understand if it was interpolating like `ε * conditional_prediction + (1 - ε) * unconditional_prediction`, but in its formulation, ε is greater than 1. It is clear why it makes the result match the condition better, but why the result becomes better regardless of the condition was a mystery to me.

Afterwards, there were many post-hoc explanations, which didn't seem satisfactory (e.g. these explanations didn't have predictive power helping to improve the trick). Recently, I finally got around to play with it, and found some interesting patterns (in context of diffusion, DDIM sampling):
* If we disable CFG for 90% last sampling steps, results are pretty much the same;
* If we disable CFG for the first 10% sampling steps, the resulting image is destroyed.

It appears that CFG it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling, and doesn't do much afterwards. This is kind of similar to the observation about attention maps in [this paper](https://arxiv.org/pdf/2208.01626.pdf) (section 3.1). Speculatively, it tries to ""match the prompt"" to the random noise, and the adjustments from it need to be amplified, otherwise subsequent steps will match the prompt differently (it is a random noise after all). If this is true, I guess something like this might also work (I haven't tried yet): sample 1000 different starting random states, take the one which ""matches the prompt"" the best by some measure, and do the diffusion sample starting from it without CFG.

This all might make sense, except that this is very specific to diffusion. But [it is known](https://arxiv.org/pdf/2206.10789.pdf) that CFG works just as well for autoregressive transformers on VQVAE tokes. This might indicate that the mechanism why it works is more fundamental and is not specific to diffusion.

I wonder if there is any community wisdom/thoughts on why/how it works, and generalizes so well across two very different types of models.",0,[]
1676644347.0,17-Feb-2023 06:32:27,image model,MachineLearning,114m2wj,Automated sleep tracking + prediction [P],GoochCommander,9,https://www.reddit.com/r/MachineLearning/comments/114m2wj/automated_sleep_tracking_prediction_p/,"I built a (1) baby sleep tracking & (2) forecasting system, and wanted to share for those interested, or actually want to try running it at your home.

(1) I built a baby sleep tracking system (computer vision largely, [here's the core of that code](https://github.com/calebolson123/BabySleepCoach/blob/924e7b55d3aa36acd706519c446c1172dbbda4a7/main.py#L322)) which writes timestamped records of when my baby fell asleep or wakes up. The code is pulling images from my baby monitor, and largely just applying heuristics over time to decide whether he's awake/asleep.

(2) After I had a few weeks of sleep data ([sample data](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_logs.csv)), I moved it into a [jupyter notebook](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_forecast_arima.ipynb) and ended up using an ARIMA model to forecast the next month's wakings/sleepings. I wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so I can see how his sleep is changing over time. [Here's an example of what that visual looks like](https://imgur.com/BdwBoeG) (orange is awake, blue is asleep).

I built it because my wife asked for it, but also made a video detailing the project: [https://youtu.be/r7Exc0sUt5E?t=209](https://youtu.be/r7Exc0sUt5E?t=209)",4,"['andrew_elliott: I had no idea that my ridiculous visualisation from 6 years ago would be of any interest to anyone to replicate, but you sir have done a stellar job with the optical recognition and automation. I’d love to try your system out, but that would involve having another baby, so I’ll just take your word for it 🙅\u200d♂️', 'sapnupuasop: Just out of curiosity: has it any use case or just for fun? Like does it help to know it’s predicted sleep times?', 'GoochCommander: Hah, awesome you saw this. Good work and thanks for the inspiration', 'GoochCommander: imo the month forecasted is more interesting to look at, than it is functional. The base functionality of using [MediaPipe](https://google.github.io/mediapipe/solutions/face_mesh.html) \\+ custom logic to track when baby wakes/falls asleep is the most useful for us. To know exactly when baby woke up, or how long exactly baby has been sleeping takes some of the daily cognitive load off']"
1675912778.0,08-Feb-2023 19:19:38,image model,MachineLearning,10xjwac,[D] Are there emergent abilities of image models?,These-Assignment-936,89,https://www.reddit.com/r/MachineLearning/comments/10xjwac/d_are_there_emergent_abilities_of_image_models/,"Just finished reading the Stanford/Google survey paper ([https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)) on emergent abilities of large language models. It made me wonder: do image generation models have emergent abilities, too? Do we know?

I can't quite wrap my head around what such an ability would even look like. Figured maybe other folks had given this a think.",28,"[""ID4gotten: Some 3 dimensional understanding and up/down/gravity seem possible. I think examples of light/shadow/reflection have already been shown. I can't see how it could ever do full tracing but maybe there are heuristics (or overfitting) to be found."", ""nielsrolf: Parti (https://parti.research.google/) showed that being able to spell is an emergent ability. That is the only one I know of, but others that I could imagine are learning compositionally (a blue box between a yellow sphere and a green box), but it's more likely that this is a data issue. Also working out of distribution (a green dog) is a potential candidate. Interesting question"", ""mongoosefist: Emergent behaviour is called such because we don't yet have the ability to predict it, we can only observe it and deduce where it emerged after the fact. SO, the fact that you can't wrap your head around what such an ability would look like makes perfect sense!\n\nIf we're speculating I'd put my money on /u/ID4gotten 's answer. I bet one of these models starts integrating some intuition of physical laws."", 'londons_explorer: Shadows and the way light interacts/reflects/refracts seem to be emergent behaviour of diffusion image models.\n\nAsk for ""A koala next to a glistening wine glass"", and you\'ll probably get cool optical effects on the koala that the model has never seen before.', ""the_new_scientist: Yes, the DINO paper showed that the ability to perform segmentation emerges from self-supervised vision transformers.\n\n[https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)  \n\n\nEdit: oops, didn't realize you said image generation models, thought you asked for just vision models."", 'andreichiffa: I am pretty sure that was an Anthropic paper first (Predictability and Surprise in Large Generative Models). Makes me truly wonder WTF exactly is going on in Google lately. \n\nAs to your question, no one has stacked enough attention layers yet, but there is very high probability that they will. Someone already mentioned the ability to spell, but it could potentially help with things such as hands, number of hands/feet/legs/arms/paws/tails and other things that make a lot of generated images today disturbing. \n\nThe issue will most likely be with funding enough data, given that unlike texts most images on the internet are copyrighted (cough Getty cough).', 'irulenot: In vision, the ability for large models to do video segmentation… somewhere in here: https://arxiv.org/abs/2101.01169', '_eminorhan_: People should be more skeptical of ""emergent abilities"" in big models: 1) Papers claiming such abilities generally use undertrained small models as per chinchilla scaling (compute is not controlled + suboptimal hyperparam choices for small models) and 2) these papers generally use a semilogx plot to demonstrate ""emergence"" but even a linear relationship will look exponential in such a plot. I\'m not sure if I\'d want to call a simple linear relationship ""emergent"".', 'edjez: Another emergent capability - and this depends on the model architecture, for example I don’t think Stable Diffusion could have it, but Dalle does - is to generate written letters / “captions” that to us look like gibberish but actually correspond to internal language embeddings for real-world cluster of concepts.', 'visarga: Combining objects and styles never seen together in the training set in a plausible way (a baby daikon radish in a tutu walking a dog).', 'gradientpenalty: denoising diffusion probabilistic models:\n\n[Rdiffusion](https://the-decoder.com/riffusion-generates-ai-music-from-stable-diffusion-images/) : Generate music from stable diffusion\n\n[Improve image segmentation](https://medium.com/edge-analytics/using-stable-diffusion-to-improve-image-segmentation-models-1e99c25acbf) : I remember someone doing image segmentation on these generative model, but not sure where.', 'These-Assignment-936: Wow that’s very cool!', 'Dr_Love2-14: And counting', ""Insecure--Login: >and you'll probably get cool optical effects on the koala that the model has never seen before\n\nHow could we be absolutely certain the model has never seen said effects?"", 'irulenot: Yes this!! Sorry didn’t see it', ""currentscurrents: While those are on the same topic, they're very different papers. The Anthropic paper spends most of its time going on about safety/bias/toxicity, while the Google paper is focused on more useful things like the technical abilities of the models."", 'DigThatData: i\'m not sure that\'s an emergent ability so much as it is explicitly what the model is being trained to learn. it\'s not surprising to me that there is a ""painting signature"" concept it has learned and samples from when it generates gibberish of a particular length and size in the bottom right corner (for example). that sounds like one of the easier ""concepts"" it would have learned.', 'Cantmentionthename: Dayum. That just sounds like generative communication.', 'amnezzia: You mean it takes a mean vector of a cluster and makes up a word for it?', 'nielsrolf: I thought about it again, and another candidate is all LLM capabilities: if you prompt it for ""a screenshot of a python method that does xyz"" the best solution would be an image that contains working code.', ""londons_explorer: You search the training image database for pictures of koalas with wine glasses...   And there won't be many examples in there, and you check each one."", 'master3243: Exactly, the beginning ""Clip"" part of the entire Dalle model is trained to take any english text and map it to an embedding space.\n\nIt\'s completely natural (and probably surprising if it doesn\'t happen) that Clip would map (some) gibberish words to a part of the embedding space that is sufficiently close in L2-distance to the projection of a real world.\n\nIn that case, the diffusion model would decode that gibberish word to a similar image generated by the real word.', 'CampfireHeadphase: Similarly to how a zipped email archive could be called generative communication', 'Mescallan: word might not be correct, as it implies a consistent alphabet, but semantics aside, yes I believe that is what is happening', ""visarga: There are language models without tokens. They use the raw pixels of an image with text. I can't find the link, Google is not helping me much."", 'VaxxBetrayal: Hmmmm.\nHmmmmmmmm', 'Insecure--Login: You would have to search millions to billions of images manually; that sounds very expensive. And searching using a detection model is not accurate enough.', 'xenophobe3691: Sounds like that story of the guy from 40k who pretty much looked for the underlying connections between all the different kinds of beauty and joy.  He found “It” alright…']"
1676634570.0,17-Feb-2023 03:49:30,image model,MachineLearning,114iieo,[R] Does a new published ML dataset always need to have an official train-dev-test split? Should the test set be made balanced?,ConsiderationMore528,5,https://www.reddit.com/r/MachineLearning/comments/114iieo/r_does_a_new_published_ml_dataset_always_need_to/,"I have constructed a novel ML (NLP) dataset for classification and labeled it with three classes. The dataset is rather small with about 700 examples, out of which the classes have about 400, 200, and 100 examples respectively. I would like to publish it and describe it in an official publication for a workshop or a conference.

When looking at related datasets and publication, I see that it is common for authors to publish the dataset already split into three chunks - train, dev, test dataset (see the images). It is also common in these papers to provide the performance of baseline models on the dataset. Considering the dataset's small size, I feel like doing a 5-fold cross-validation would be a good alternative for such a small dataset, rather than doing something like a split into 450-100-150 train-dev-test datasets and then evaluating only on the very small dataset with 150 examples. Still, I believe that for better replicability, doing an ""official"" split is preferred and then everyone in the future testing on the same test set with 150 examples? Why do the authors usually already provide the three splits?

Furthermore, when looking at these ML resource papers, I saw in a few instances that the test set is kept balanced with respect to the three classes, even though the original dataset was not and dev set is not made balanced. This is problematic in my case for my third class where there are only about 100 examples. If I make my test set to be 50-50-50 for class1-class2-class3, then there is only 50 examples of class3 left for train+dev! That is simply infeasible for the training set. None of the authors provide any sort of explanation why they split it like this, they just seem to say ""here is our split"". Is this done to discourage the model from just doing a majority-class prediction and thus make it challenging? Or because a dummy classifier would have a 60% accuracy? Still, with a metric like F1 and not accuracy, this does not seem like an issue...

Some examples of these balanced test sets with unbalanced train sets:

\[1\]: [https://i.stack.imgur.com/RGRk3.png](https://i.stack.imgur.com/RGRk3.png)

\[2\]: [https://i.stack.imgur.com/R39Oh.png](https://i.stack.imgur.com/R39Oh.png)

\[3\]: [https://i.stack.imgur.com/6Vqaw.png](https://i.stack.imgur.com/6Vqaw.png)

When searching through Stack Overflow for similar questions, people were usually discouraged from splitting their Kaggle datasets into a test dataset that is balanced, with the argument that we want a classifier to work with data that resembles the real-world distribution and makes it ready for production.

To sum up:

\- Is is considered mandatory to provide the ""official"" train-dev-test split when introducing a new dataset in an ML publication?

\- If so, should the test set have a balanced class distribution and why?",1,"['twanvl: I have also seen datasets published with official folds. You could distribute this as 5 files, ""fold1.csv""..""fold5.csv"", and say that the official scores are computed by taking the average test scores over the folds, where the other folds are used for training/validation. This will allow for perfect replication as well. But it will be more effort to use than the standard train/dev/test split, so there is a risk that fewer people will use it.\n\nIn my opinion you should *not* balance the classes if they are not balanced in the real world / in the original dataset.\n\nIt could make sense to make the distribution equal between different splits, so with your numbers, select 80,40,20 samples of each class for a split, rather than picking 140 samples at random from the whole dataset.']"
1676405170.0,14-Feb-2023 12:06:10,image model,MachineLearning,112eqxm,[Discussion] Computing the derivative of a diffusion model with respect to the prompt,arg_max,2,https://www.reddit.com/r/MachineLearning/comments/112eqxm/discussion_computing_the_derivative_of_a/,"Hi,  


I was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross-attention module. So let's say we have a text that is already transformed into a continuous embedding. Then this goes through the llm and is fed into the cross-attention module at every timestep. At the end of the diffusion process, we get some image/a latent representation of an image in the case of stable diffusion. We can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non-stochastic sampler like DDIM. The issue is the length of the graph calculating that derivative is super expensive. I was if anyone already solved this or has some good references.  


Thanks :)",0,[]
1675538796.0,04-Feb-2023 11:26:36,image model,MachineLearning,10tovhn,[N] [R] Google announces Dreamix: a model that generates videos when given a prompt and an input image/video.,radi-cho,1889,https://v.redd.it/j9f0y49738ga1,,126,"['yaosio: Wow, the quality of the video is very good. Imagen video was not that long ago.', ""master3243: Browsing through the examples in the website, they still have that strange AI movement to them. It's still impressive.\n\ndog to cat: https://dreamix-video-editing.github.io/static/videos/vid2vid_cats.mp4\n\ndog to dog playing with ball: https://dreamix-video-editing.github.io/static/videos/vid2vid_football.mp4\n\nonions to noodles: https://dreamix-video-editing.github.io/static/videos/vid2vid_noodles.mp4"", 'DadSnare: [My feeble attempt at a similar scene with stable diffusion.](https://imgur.com/a/IYQ3rQX)', 'radi-cho: Announcement: [https://dreamix-video-editing.github.io/](https://dreamix-video-editing.github.io/)\n\nPaper: [https://arxiv.org/pdf/2302.01329.pdf](https://arxiv.org/pdf/2302.01329.pdf)\n\nThe approach, which is the first diffusion-based method of its kind, combines low-resolution spatiotemporal information from the original video with newly synthesized high-resolution information to align with a guiding text prompt, allowing one to create videos based on image and text inputs.\n\nTo improve the motion editability, the team has also proposed a mixed objective that jointly fine-tunes with full temporal attention and temporal attention masking.', 'BlessedBobo: adult films are about to be wild, better delete your face off the internet folks.', ""Ok-Run5317: what is with Google. they announce these ground breaking tech. but don't share the code. what exactly is the purpose here?"", 'blackkettle: The next two years will be the “bonkers” years.  And we’ll be dealing with the fall out for the next ten.  Same as 2000.  But wilder.', '-Ch4s3-: Humans evolved in an environment where they were as often prey as predators. Out ancestors didn’t understand disease, thought bad weather was the anger of gods, the moon was a big mystery, and most people died as infants or by the age of 5. And at least once in the past, we know there was a bottleneck of only a few thousand humans living at once. I’ll take modern problems any day.', 'Zombisexual1: Pretty soon you can make your own decent quality movies on a budget. All you need is a green screen with actors and then this stuff in the back', ""iamAliAsghar: If there is no model to test, it didn't happen"", 'Context_Fancy: The speed at which AI is growing is getting almost scary', 'Decent_Preference_95: How do I get my hands on it', 'codersaurabh: How can I test it??', 'ASAP_ROCKY: Is the git repo for this up anywhere?', 'TheJoker1432: Man we are going into a time where we cant trust any video or.picturr at all\nWhich is difficult as we have a tendency to be influenced by videos or pictures even subconciously', 'bobwyates: https://github.com/dreamix-video-editing', 'lucidrage: How good is the ""moving through a field with naked dancing ladies"" video quality?', 'nogop1: Anyone seeing [this](https://dreamix-video-editing.github.io/static/videos/vid2vid_circle.mp4)\nand thinking of this\n[this](https://www.youtube.com/watch?v=wmqsk1vZSKw) ?', 'rosandonary: Is there some website can try it .', 'race2tb: Temporal inpainting. They will need to add an interactive segmentation system to make it more usable.', ""mindbleach: Webcomics took off circa 2000, because the bar to entry was really low. There was a ton of crap... but there were also stories that went on for ten or twenty years, and would not have existed at all if not for the advancements in creating and distributing digital images. \n\nYou're about to see a ton of crap. And it's going to be fantastic."", 'Context_Fancy: The speed at which AI is growing is getting almost scary', ""CriticalTemperature1: Well I suppose there's no way people could use this for evil..."", ""PecanSama: So.... we can't trust photo or video evidence now. It'll be super easy to subdue the mass with advance propaganda. The ruling class has reached invincibility"", ""Vas1le: Let's accelerate how fake news + deepfakes are made before having a contingency to spot them..."", 'Shake-Wide: False Flag scene designers will love this!', 'ThatInternetGuy: Perhaps our reality really is a simulated reality, run by AI angels.', ""Fantastic-Alfalfa-19: oh man. Sooo I don't need to spend any more time on perfecting my vfx game then"", ""electroshock666: This is an impressive paper, don't expect to see the source code though.\n\nThe temporal consistency of Dreamix is much better than Imagen or Meta's Make-a-Video but  it can struggle with spatial-temporal attention which can be seen in some videos where small movements result in weird behavior like the movements of the dog's legs.  But the ability to preserve the original subject's appearance from the images its conditioned on is really good.\n\nIt's interesting that the GitHub repo lists the authors as anonymous but the paper published lists all their names."", 'bluebambi420: Where can i try this', 'master3243: True, although the two tasks are slightly different.\n\nThe same difference between generating an image with a prompt compared to manipulating an image with a prompt.', 'fuelter: > the quality of the video is very good\n\n720p', 'house_monkey: Noodles one is straight up cursed', ""ninjasaid13: >they still have that strange AI movement to them.\n\nit's called foot sliding in animation."", 'walter_midnight: some uncanny fun right there', 'chaosmosis: I wonder if there are data quality issues in play. A fair number of the inconsistencies look like they could be at home in low resolution footage.', ""7734128: While it's clearly lacking consistency, each individual frame of your example is much better, in my opinion."", ""StickiStickman: Whats with all the blue artifacts? That doesn't look normal"", 'Oronoque: That’s cool, very cool.\n\nBut not terribly realistic.\n\nThough I’m not saying it couldn’t also be terrible, if that were real.\n\nThat blue is…well, kinda spooky.', 'codersaurabh: Lol true', 'uristmcderp: I mean the whole deepfake using deepface has been around for years. Not sure how this would change anything.', ""fish312: I'd be flattered tbh"", 'staffell: They can have my face', 'geringonco: Why delete? I would love to be a star.', 'codersaurabh: But how to use this', ""crazymonezyy: > what exactly is the purpose here?\n\nPR for shareholders, to counter the claims that they're a dinosaur on their way to get disrupted by OpenAI or whatever is the cool thing in AI at any given time."", 'room52: What do you mean?', 'KyleShannonStoryvine: I actually just did a video called “2023 is the new 1995” comparing it to the birth of the WWW. It’s already a trip of a year a month in! https://www.tiktok.com/t/ZTRGrACuB/', 'Simcurious: Always surprised how people can look at amazing technology like this and only think how it could potentially be bad and not how amazing it can be for mankind.', ""toastjam: Won't even need a green screen, background removal and relighting is coming along quite nicely. Just film somewhere with an environment somewhat like your target."", 'respeckKnuckles: Seriously. These announcements are just ways for them to claim ""first!"" without the burden of actual peer review to test their claims.', ""xanxusgao14: seems to me that these models probably require an enormous amount of compute just to run, so not sure if it'd be a good idea to release it to the public"", ""What_The_Hex: It's about that singularity time!"", 'Maxi969: for real', 'aesu: The starting pistols not even been fired yet.', 'bowzer1919: Would also like to know', ""yaosio: No porn yet.\n\nhttps://civitai.com/ has shown that we need a better way to handle generative models. The site is filled with tons of models, you'll need to download multiple models to get a good spread, and each model can produce things other models can't so you'll never get exactly what you want.\n\nFor the time being textual Inversion, hypernetworks, and lora could help but few people use those and prefer to make new checkpoints. Even if you do use them they are difficult to use as you have to explicitly add them into a prompt by using the word or phrase that triggers using them.\n\nA way to add new data without creating a new checkpoint, and without needing to explicitly call that data is needed."", 'modefi_: for real', 'Studds_: Or add your face even more. You know somebody somewhere out there will want their face in a AI porn video with some super model', 'shmoculus: I dont think we should worry about pixels :)', 'Cherubin0: This comment was flagged by the AI for criticism of the supreme leadership. Kill bots will arrive soon. Jk', 'iamthesexdragon: Dystopian AI generated fake news era, here I come', ""Unit2209: Keep perfecting your game. A good VFX artist who uses these new tools will outperform a good VFX artist who doesn't use these new tools."", 'napoleon_wang: Or ""no time for finalling, gotta deliver""', 'hiptobecubic: The dog grows an extra leg...', 'shot_a_man_in_reno: Some auteur director needs to take advantage of this to make a creepy dream sequence in a movie.', 'HINDBRAIN: It is making me vaguely nauseous. Might have fun application in horror movies...', ""nmkd: Not really. What's the blue stuff doing there?"", 'vsemecky: The LMS sampler suffers most from these blue artifacts. If you use LMS, try LMS Karras instead and the artifacts will be gone.', ""BlessedBobo: Deepfake has a barrier to entry, it needs to be trained on a lot of data atm and despite that, it's still pretty damaging albeit limited to famous people, just look at the recent twitch deep fake drama  \nnow imagine if anyone can do it with minimal data, suddenly you don't need the huge amount of data of a famous person, suddenly that one picture of your ex that pissed you off is looking mighty tempting for some sweet sweet revenge  \nyou can see where this is going?"", 'linebell: Hahahah', 'a1_jakesauce_: I’ll tell you, just send me a pic of your face first', 'Inputoutputpoof: Haha. Hope the whole AI thing from Google is all fake. So we dont lose much jobs.', 'blackkettle: I meant that I think we’re seeing the beginning of a new major disruptive cycle. I’m not making a value judgment about it.', ""rePAN6517: Humans did not evolve to exist in the kind of technological environment we're creating.  But we're nevertheless pushing ourselves further and further and at an accelerating rate into such an environment.\n\nThe prevalence of dangerous and destructive tools is also increasing at an accelerating rate.  1000 years ago only a handful of rulers were capable of causing widespread destruction through war with hand-to-hand weapons and the effects were limited to a small geography.  100 years ago it was still limited to a bigger handful of rulers, but this time they had firearms and could cause widespread destruction over a much larger area.  Today rulers have nuclear weapons, biotech engineers have the ability to create super viruses, countless leaders have surveillance technologies that can trap their people in Orwellian dystopias, software devs have powerful narrow AI systems that can be used to globally spread socially corrosive memes, etc.  Soon nearly everybody will have access to superintelligent AGI systems that could be used to cause unimaginable chaos and destruction.\n\nThere has been zero progress on the alignment problem.  \n\nIt's not difficult to see where things are probably headed."", 'yalag: It means porn. Lots of it.', 'Punchable_Face: Great video! Do you have a link to part 2? I don’t have tiktok and the website isn’t too desktop friendly.', 'blackkettle: I don’t See it as “Bad” that’s not what I meant.  I work in this space.  I meant I see it as tremendously disruptive in the same way that the dawn of the internet was, or the steak engine, or electricity.  It’s going dramatically change some pieces of our economy and how we do things.  It’s just the first glimpse of that.  Whether it will be bad or good for us in the long run is a different story.', 'addition: I feel like as technology progresses we lose a bit of our humanity.', 'chakalakasp: It’s because mankind tends to either derive most inventions from or put most inventions to use for warfighting.  \n\nA spaceship that had an engine that could get it to an appreciate fraction of the speed of light would be incredible.  But someone would likely take a few dozen such craft out a few lightmonths and then park them and use them as a mutually assured destruction planet killing system.  There is a limit to how nice a thing we can have before we destroy ourselves.', 'linebell: Doubt you’ll even need to film anywhere. You’ll just use a template scene. Honestly this is going to be so nice. I’d like to generate some tv series. We are going to have an explosion in creative endeavors.', 'lucidrage: >No porn yet.  \n>  \n>https://civitai.com/ has shown that we need a better way to handle generative models. The site is filled with tons of models, you\'ll need to download multiple models to get a good spread\n\nPlease do tell me more about this ""spread"". Which model has the best spread? Asking for a friend.', ""robotomatic: It's about that singularity time?"", 'Fantastic-Alfalfa-19: yeah! I spend every second of free time with stable diffusion since november :D', 'mabilicious: David Lynch could pull it off', 'DadSnare: Blue stuff seems to show up when I use “forest fire” instead of just “trees on fire” because of smoldering ground in its training data. That continuity thing is the real key and google is obviously using some tricks up its sleeve to achieve that. Thing is, the source video on the google example isn’t the same as the output. It’s like it was a suggestion for what’s happening in the scene and then it generated an entirely new video.', 'Braler: Or political rival...\n\nSay you want somebody doing heinous stuff under a pizzeria just to stir a little bit more the reactionary dimwits', ""UncorkingAsh: You don't really need that much raw training data anymore - Start with a few pics of your target and train a dreambooth, then you can use a premade folder of celebrity pictures training data that look somewhat like your target and then img2img the entire folder with your dreambooth model to look like your target and use that as training data for the deepfake."", 'room52: Agree', 'VelveteenAmbush: I feel pretty good about our odds of surviving the advent of text-to-video generators, personally.', 'often_says_nice: I wonder if the alignment problem can be solved (or at least narrowed down) by arming every individual with their own personally aligned AI. That way, you only need to align its goals with one person rather than the entirety of mankind. Surely this is an easier task.\n\nYour AI would know if you’re being hacked or memed, create your own virus vaccines, and steer your views/content intake towards a path that is mutually beneficial for both you and the bot.', 'PoliticalRacePlayPM: “Your scientists were so preoccupied with whether or not they could, that they never thought to wonder if they should” \n\nWe really need to start passing laws on the ethics of AI before we keep advancing. I know that’s a pipe dream and it probably won’t happen until the damage was done, as usual.\n\nWe really trap ourselves with our own creations', ""fuelter: >It's not difficult to see where things are probably headed.\n\nA revolution"", ""uristmcderp: If we die, we die. Such is the way of life. Or maybe technology will win out such that we're able to survive outside our home planet, and we can do this all over again when we hit another critical unstable equilibrium."", 'Braler: Ted, is that you? You were right all along.', ""drakfyre: > There has been zero progress on the alignment problem.\n\nWell, what do you expect?  We don't even know how to solve the HUMAN alignment problem, how are we going to solve it for superintelligences?"", 'room52: True but fake though', ""HumbertTetere: Steak engine for those confused:\n\nhttps://www.wisebread.com/cooking-great-meals-with-your-car-engine-the-heat-is-on\n\nI would not have put it in a list with electricity and the internet, but then I'm not a steak person and I know some people take their BBQ very seriously.\n\n^^^^^Probably ^^^^^meant ^^^^^steam ^^^^^engine."", ""nateblack: I'll bite. If you work in this field and are seeing the potential this has, what type of jobs/careers/skills do you think will be valuable as this evolves? The biggest threat people say AI poses is the elimination of human jobs. Even highly skilled and paying coding and programming jobs are potentially at risk by generative ai. What's a path that could pay better because of AI in your estimation?"", ""yaosio: Sort by highest rated and NSFW and you'll find the answers you seek."", 'linuxIsMyGod: !*', ""Frequent_Macaron_827: Yeah lol, he's being a bit dramatic. We will be fine."", 'chakalakasp: I know what you’re saying, but this is kinda like looking at electricity in the 1800s and saying that it’s just a lightbulb.  What’s about to happen is akin to what happened in the Industrial Revolution.  Which led to lots of good things, but also leveled up our warfighting ability from dudes on horses with muskets to melting entire cities with a device the size of a motorcycle.  \n\nAnd we’re going to be starting out at that level when we level up this time.  Do you think mankind is responsible enough to know what to do with godlike technology?\n\nThere is probably a reason guys like Bill Gates and Elon Musk have very publicly said they think AI may pose an existential risk to mankind and that we should proceed very slowly and deliberately.  There are entire very interesting papers written on the topic.  https://intelligence.org/files/AIPosNegFactor.pdf', ""Iamreason: If that person is malicious you've just handed AGI to a serial killer or whatever.\n\nIt's gotta be for the betterment of the entire species.  It's nerf or nuthin."", ""rePAN6517: > We really need to start passing laws on the ethics of AI\n\nI hear you but that wouldn't do anything unless you got every jurisdiction in the world to pass this, have a way to enforce it, and actually enforce it, right away."", ""Borrowedshorts: That's not how it works.  We'll only know what laws to create once the effects have been felt.  We can make educated guesses, but given the current political climate, AI is the furthest thing from lawmakers' minds."", 'taggingtechnician: Laws do not stop criminals from attaining weapons, nor do laws stop criminals from committing crimes. Children who are taught core values, integrity, and the benefits of investing in Self usually live differently than children who are not. Our industry is global, and the investors in other countries do not share our values, thus their AI/ML activities are prioritized for different outcomes; even in this country (USA) private corporations funding our research are doing so with different intentions and interests, and, as we have seen with f and g their core values are machiavellian, and their leaders are like children playing with guns, each seeking a bigger gun like in a video game but without fully comprehending the consequences. \n\nWhere is hope to be found? It is not in this realm but the next that we must look.', 'Jeffy29: Goddamnit', 'blackkettle: I don’t think it will simply “eliminate” jobs.  But I do think there is going to be a sea change in job descriptions.  I think the most disruptive area will be traditional professional jobs like lawyers and doctors.  My kid is 6 and I think if he watches House reruns in his twenties he’ll find them bizarre.  The idea of a human savant able to outdo an AI will be laughable.\n\nI think there will still _probably_ be humans tuning the core models.  Probably.  The rest depends on us.  I think there will be an explosion of job descriptions related to prompt tuning for chatgpt technologies.  Plenty of jobs for fine tuning the models to particular domains.  \n\nPeople will still remain in call center jobs, but it will focus more on analysts and not auditors.\n\nBeyond that I think it’s hard to say.  How will it affect other areas like biology, pharmaceuticals,even physics?', 'Anonymous_Baba: generated by chatGPT™', 'VelveteenAmbush: I am totally on board with deep learning being a transformative technology, possibly more profound than any other technology in human history, posing both massive potential risks and massive potential benefits.\n\nI am totally **not** on board with people milking the Reddit karma machine by hijacking every freaking discussion about a new image generation model with this same ""DAE mankind\'s reach exceeds our grasp / we are become death, destroyer of worlds"" schtick.', 'often_says_nice: But wouldn’t their potential victims be safeguarded by their own AGI as well?\nIt could be some human rights thing we see in the future. Every man woman and child are given an AGI', ""thfuran: Also, consider how successful efforts to curb nuclear proliferation would've been if testing weren't globally detectable via seismograph, production didn't require access to enriched nuclear materials, the only expertise required for development was that of a popular and fast-growing civilian field, and the intermediate results were likely easily useful in many industries. Everyone and their mum would have nukes."", ""Ghostglitch07: Ai safety research is a thing and they definitely have some ideas. We might not know exactly, but that's no reason not to make an effort.\n\nThis is like saying you can never completely accurately predict the weather so airline companies should completely ignore meteorologists and just deal with weather as it comes up."", ""StickiStickman: Laws can make it much, much harder and rarer for criminals to get a weapon though.\n\nThere's a reason guns are a leading cause of death for kids in the US but nowhere close in the EU. Same with gun deaths in general. Also just look at Australia for an example of it working."", 'Iamreason: There are a *lot* of failure points here.  \n\nIE, what if I simply have more processing power available to me as a serial killer with an AGI?  Are you going to legislate the amount of GPUs I can have?  What if I fiddle with the code and make my AGI much more intelligent?  Now it can outthink the protections of any standard AGI.  \n\nAligning it with a set of general values that are tightly controlled and impossible or extremely difficult to tamper with is a much better overall strategy.', ""Borrowedshorts: That's a poor analogy.  A better analogy is designing all the safety systems of planes before you've ever built a single one.  It's an impossible task."", 'often_says_nice: I think you’re right, just throwing ideas out there', ""Ghostglitch07: All? Sure. But that doesn't mean you don't put any thoughts towards safety to try and put in atleast some safety systems."", ""Iamreason: It's a really complicated problem.  I don't have all the answers.  If I did they'd be paying me a lot more money than I'm currently being paid.\n\nNo such thing as a bad idea when it comes to alignment.""]"
