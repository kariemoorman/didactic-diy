created_unix_utc,created_datetime_pst,search_item,subreddit,id,title,author,score,url,body,num_comments,comments
1676785417.0,18-Feb-2023 21:43:37,language model,datascience,1162ssq,Buzz around new Deep Learning Models and Incorrect Usage of them.,brokened00,189,https://www.reddit.com/r/datascience/comments/1162ssq/buzz_around_new_deep_learning_models_and/," In my job as a data scientist, I use deep learning models regularly to classify a lot of textual data (mostly transformer models like BERT finetuned for the needs of the company). Sentiment analysis and topic classification are the two most common natural language processing tasks that I perform, or rather, that is performed downstream in a pipeline that I am building for a company. 

The other day someone high up (with no technical knowledge) was telling me, during a meeting, that we should be harnessing the power of ChatGPT to perform sentiment analysis and do other various data analysis tasks, noting that it should be a particularly powerful tool to analyze large volumes of data coming in (both in sentiment analysis and in querying and summarizing data tables). I mentioned that the tools we are currently using are more specialized for our analysis needs than this chat bot. They pushed back, insisting that ChatGPT is the way to go for data analysis and that I'm not doing my due diligence. I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field. 

After just a few minutes playing around with ChatGPT, I was able to get it to give me a wrong answer to a VERY EASY question (see below for the transcript). It spoke so confidently in it's answer, even going as far as to provide a formula, which it basically abandoned in practice. Then, when I pointed out it's mistake, it corrected the answer to another wrong one. 

The point of this long post was to point out that AI tool have their uses, but they should not be given the benefit of the doubt in every scenario, simply due to hype. If a model is to be used for a specific task, it should be rigorously tested and benchmarked before replacing more thoroughly proven methods.

ChatGPT is a really promising chat bot and it can definitely seem knowledgeable about a wide range of topics, since it was trained on basically the entire internet, but I wouldn't trust it to do something that  a simple pandas query could accomplish. Nor would I use it to perform sentiment analysis when there are a million other transformer models that were specifically trained to predict sentiment labels and were rigorously evaluated on industry standard benchmarks (like GLUE).

https://preview.redd.it/sz3ejc1313ja1.png?width=1700&format=png&auto=webp&v=enabled&s=16393c01bdbca3a79f55ef61c486d9aa5645680a",99,"['nraw: Whenever the business side is telling you how to do something instead of what outcome they need, your bullshit senses should be tingling.\n\n""We need chatgpt to create our knowledge graph in a graph database on a quantum computer"" is just that big $$$ manager asking for some data from the db, preferably in Excell or PowerPoint.', 'WignerVille: Saw this post on LinkedIn. Thought it was funny. People spend more time on new buzz words than understanding what they are currently using. \n\nhttps://www.linkedin.com/posts/olalindeberg_datascience-ai-machinelearning-activity-7032064588560969730-1-Y2?utm_source=share&utm_medium=member_android', 'maxToTheJ: As someone who works in ML. Nothing has generated so many “experts” on “chatGPT” and “ML” as chatGPT. So many people are using LLM as a buzzword and pretending to be experts everywhere. I largely dont even engage because it feels like pissing in the wind\n\nEven with the deepdream hype people didn’t claim expertise', 'nth_citizen: The simplest question I\'ve found to trip up chatGPT is: *In the statement \'a large mouse ran up the trunk of a small elephant\', which is larger?*\n\n> In the statement \'a large mouse ran up the trunk of a small elephant\', the word ""large"" refers to the mouse and the word ""small"" refers to the elephant. Therefore, the mouse is larger than the elephant in this statement.\n\nThis clearly shows chatGPT has no context for the nouns. \n\n>  I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field.\n\nJust like chatGPT!', 'misterwaffles: This is really common unfortunately and somehow you have to delicately frame things so that leadership instead explains what they want (in terms of outcome, user experience, etc.) and you, the expert, get to choose the solution, not the other way around. That\'s why they hired you. But ChatGPT is the hottest buzzword on the planet right now.\n\nArguably, one could say that you are not making this decision based on data, but on your expert opinion. So, therefore, you should give ChatGPT a chance, but with a big caveat.\n\nMy sincere suggestion is to tell them you will create an ensemble model that contains your solution mixed with the ChatGPT solution, which is superior to ChatGPT by itself. You could say, a specialized sentiment model plus the general ChatGPT. So, each model\'s predicted probabilities will be combined in a weighted fashion, such that the weights are hyperparameter tuned for performance. If that means ChatGPT ends up being weighted 0, then so be it. Whether you want to discuss that fact is up to you. It\'s a win-win, you will have done your ""due diligence,"" and it\'s the best compromise I can think of. You don\'t have to lie, but understand you will be letting the machine learning select the best predictions, rather than you, so you are not going against your leader.', 'CrossroadsDem0n: Ask chatGPT if your way is better. If you can coax it to say yes, give the results to the manager and declare mission accomplished.', ""riricide: I get triggered now if someone starts talking about chatGPT and how it's going to solve world hunger. As they say, a little knowledge is a dangerous thing. I'm impressed you didn't roll your eyes and walk away when your higher up was pretending he was an LLM expert."", ""pyepyepie: Just tell them that using OpenAI's models means sending your data to another company. 90% of CEOs of established companies don't like it.\nBy the way, I am almost sure fine-tuning some OpenAI's model that fits the task will work better than your models but again, I would not send the data outside so fast. Also, it's not cheap.  An additional reason would be the fact that they can stop or change the service every day.\n\nJust give them logical arguments, and don't push back for no reason.\nAnother idea would be to use their models to generate a larger dataset to train your model.\nMan, you have so many relevant arguments - performance is really a bad one.\n\nFor the rant, 100% agree :)"", 'tiensss: My old boss would have done that. I am so happy I am not working there anymore.', ""FHIR_HL7_Integrator: Totally to be expected. The term AI has too much baggage with it and generates expectations for casual users. There does need to be some tempering of the public's knowledge, but that's likely something that's going to take a while.\n\nIf sentiment analysis is all they're after, there are better ways to do it than ChatGPT. ChatGPT might be the easiest, just fire off a request. But maybe not as flexible for future needs. Idk."", ""bakochba: How would chatGPT do analysis? It's just a chat bot"", ""proof_required: Hey but non technical stakeholders have some skills which you don't have. So we need to listen to their wisdom. Data scientists are all nerds who can't communicate or interact with people normally.\n\n/s\n\nSeriously since I have become senior, I ignore most of what the business leaders have to say. I hate ELI5 to them and trivializing lot of DS concepts. It just makes them feel like that they also know stuff and can dictate whatever asine ideas come to their mind.\n\nI hope you have a manager good enough to filter out such garbage ideas. I once had a CEO at a start-up who wanted to have update about how much model metrics have improved on a weekly/daily basis. We weren't even updating our models so frequently."", 'Dead-Limerick: I wonder how many people thought mathematicians should give up when the calculator was invented', 'm98789: Some points to help:\n\n1.\t\u2060\u2060APIs to a 3rd party won’t work for many enterprise applications due to data security reasons and / or regulatory reasons.\n2.    APIs like that of GPT-3 divinci-003 won’t work for cost reasons. Yes it is seems relatively low cost now, but for a large-scale application it may not be economically viable and also there is a risk of prices changing. Additionally many apps will need a fine-tuned model and as you can see from the pricing page, it’s multiple times the current cost of just using the off-the-shelf api.\n4.\t\u2060APIs to 3rd parties won’t work for reliability and SLA reasons to clients who require high uptimes. When signing a deal with a customer who depends on a high uptime, you can’t blame the 3rd party api for it being down, it is your service that is down.\n5.\t\u2060Flexibility to customize the network is currently very limited via the APIs. R&D innovation in areas like extending token limits, multi-modal learning, and other aspects that may be more specific to best support the needs of your business can’t be done via API.\n6.\t\u2060Risk of violating the terms of the API. Usually you need to be approved for your application for a LLM, or at least be very cognizant of their usage terms (which also may change later). What if somehow your client puts in text out of your knowledge that violates the terms of the LLM provider?\n7.\t\u2060Lack of IP. For enterprise applications built on core tech like an AI model, it may be suboptimal from a business perspective to outsource that to a 3rd party when it comes to considering the IP assets of the company. That is, basically you don’t have much of an IP story. This is relevant if your company is up for consideration to be invested in or acquired. This can lower overall value in the minds of investors or potential acquirers because there is less of a moat for competitors.\n\nTldr; so for at least these reasons (security, cost, reliability, flexibility, terms, IP), my prediction is that in-house NLP is still going to be relevant for the foreseeable future.', 'Blue_Eagle8: Chat GPT is like the internet craze of the 1990s and something that people are comparing to a gold rush. I recently read an article where someone asked Chat GPT for research papers and it gave 10 research papers on that specific topic. The thing is, the papers didn’t exist. It just made it or rather “artificially generated” those papers with DOI numbers which didn’t exist.\n\nI am honestly worried about this. Think of what will happen if we give people dictionaries with errors or science books with science fiction. Chat GPT becoming mainstream is something similar imo.', ""pitrucha: Sure, let me comeback to you with estimated costs (which will be way higher for using OpenAI API) and are you okay with us sending confidential data to a third party that we don't know if it won't be used to train some other model?"", 'Final-Rush759: Ask chatGpT how to do sentimental analysis.', 'artsybashev: You can use the openai api to get the vector representation for you documents. Use the post https://api.openai.com/v1/embeddings. Then you can use your existing infrastructure to build your classifiers.\n\nUsing the embeddings will give you better results than the chat. The main research and business question is then if the resulting accuracy justifies the costs.', ""JackMitcham: Not saying you're wrong, but I find it interesting that you didn't offer it a sentiment analysis question and instead opted for a physics problem.\n\nAs a language model, I'd expect it to be better at sentiment analysis. Not that it would be better than the specialized models, but I would be interested in seeing how it performs against those industry benchmarks."", ""SynbiosVyse: Part of the reason the bot failed on your question is that it's a very oddly framed question."", ""Relevant-Rhubarb-849: No!!!!! It corrected its answer to the right one!!!! You just didn't understand why it was right!!!!  Go back and read the original question.  It's final answer was correct. I'm not kidding.  Your question was ambiguous and you just thought your interpretation was the only one\n\nThe stated question did not specify if the 200 miles the cars travelled was the sum of two cars or if each car traveled 200 miles.  \n\nIt's final answer of two hours is correct for 4 cars if we read the problem statements as saying the sum distance for two cars was 200 miles. \n\nIt didnt get the answer right on the first reply but then again your question was not a good one and you assumed it was a hood one. And then you did not see why the final answer was correct after you nudged it.\n\nIt was quite reasonable for the AI to assume 200 miles was the sum since adding in the information about the number of cars would be irrelevant otherwise.  I think it was giving you credit for not asking a silly question so it took the interpretation that would make the number of cars relevant .  \n\nIt's actually demonstrating the chatgpt has a theory of mind!! It was interpreting your ambiguous question in the way that would give you credit for asking a more Thoughtful question. It's theory of your mind tried to guess what you really meant  to ask . \n\nIt's final answer was incorrect. It's first answer was not"", 'EvenMoreConfusedNow: Learn about the concept of temperature and use that as the main argument for never using such tools for anything other than a creative task', 'NewEcho2940: So ChatGPT is a project manager?', 'Bioinfbro: As said below, ita pretty complex skill which is called managing upwards. Aka making your bossss happy. You can explain that chagpt can write at college level but has kindergardners math skills. Your specialised tool is theother way around. Happy to add chatgbt to get the best of both worlds? Management likely wants to show investors that they are staying ahead of the tech curve.', 'BEETLEJUICEME: I mean, it is true that LLMs are now outperforming lots of more specialized fields.  \n\nSo there’s a chance that this higher up person had read something with some truth in it and they were [badly] trying to bring that up.  \n\nI often think about Disney spending over a million dollars de-aging Princess Leia for one uncanny valley scene in The Force Awakens.  They were using all the most up-to-date and advanced de-aging tools and graphics.  And then a year later someone does a faceswap on their $2000 home computer and it looks better.  \n\nNow, all of Disney’s resources are going into graphics that are essentially “faceswap” related.  But it took them a few years to turn that ship around because they were so tied to the old way of doing things.  \n\nSimilarly, DragonSpeaking and all of those super advanced text-to-speech programs have been outpaced by much simpler stuff. Read a [nice article](https://www.newyorker.com/tech/annals-of-technology/whispers-of-ais-modular-future) about Whisper recently. \n\n>\tThe point of this long post was to point out that AI tool have their uses, but they should not be given the benefit of the doubt in every scenario, simply due to hype. If a model is to be used for a specific task, it should be rigorously tested and benchmarked before replacing more thoroughly proven methods.\n\nI agree with this in theory.   But, as the tools rapidly change the next couple of years, the question is going to constantly be one of balancing reliability with not getting stuck in the past.', ""IWantAGrapeInMyMouth: business owners and execs have no clue what technology is, this is going to be a problem forever because it's a field that inherently spits out people who think surface level understanding of hyped tech is market research"", ""speedisntfree: I'm not someone who usually gets bent out of shape about this sort of thing but I have been quite concerned at how easily people will regard chatGPT as some sort of oracle and also how accepting people seem to be of deaths caused by self-driving cars being tested on the road.\n\nPeople try chatGPT and it does a surprisingly good job in answering a few questions, responding with well written responses (it is a language model afterall). People then seem to build large amounts of trust in it with large extrapolations into wider technical fields and questions. I wonder if it plays to human frailties, similar to how someone who is eloquent and learned about a specific subject area gets trusted by others on subjects well outside their area of expertise if these individuals chose to comment.\n\nI’m glad I work in science, since people I work with immediately devised some loose tests to start to evaluate it and quickly found significant problems. I also know that University of Cambridge, Biology asked it to generate some assignments, then intermixed them with real students assignments to be graded. The chatGPT submissions got a passing grade but not much more. This information was passed on to students."", 'data_in_chicago: > I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field. \n\nI’m going to tell a dark truth about these “naive managers” that hear about a buzzy AI concept and insist it should be shoe-horned into places it doesn’t belong. They’re not dummies. They’re not ignorant. When you say “it doesn’t make sense for this application” they probably 100% believe you. And they ask to do it anyway for a very good reason — it advances their career.\n\nChatGPT and generative AI (both development and application of) are one of the few tech areas seeing large investment and rapid growth right now. OpenAI [has raised $11B](https://www.crunchbase.com/organization/openai/company_financials). Startups specializing in generative AI [are one of the only sectors seeing increased valuations](https://www.ibtimes.com/chatgpt-sparks-ai-gold-rush-silicon-valley-3669436) right now.\n\nImagine you’re an overpaid marketing executive. In boom times, you can fake it and ride the natural current of a growing market. But when the economy starts to contract, you find it hard to “prove your value”. You’re worried you’ll get laid off, and with the market as it is, it’ll be hard to find something that pays as well as your current gig.\n\nThen you start hearing the buzz around this ChatGPT thing. People are enamored with it. People like to talk about it. Rich people are throwing money at it. If you can find a way to slap “ChatGPT AI” onto your product or your marketing or (most importantly) your resume, maybe people will talk about you too. Of course, if it’s BS then it won’t impress the companies actually working on AI. But a CEO at some other company in some other industry might associate “ChatGPT” with “that magical thing VCs keep investing in” and assume you have some of that magic yourself. So even if things don’t work out at you current org (and that’s looking more and more likely), you can slingshot your way into another cushy position somewhere else. All because you asked that nice young data scientist to “do a ChatGPT thing”.', 'continue_with_app: This isy everyday struggle, thanks for saying it - i am in a club now.', 'Voth98: This may be a generalization but from the sounds of it I don’t think you explained this right to your boss. ChatGPT doesn’t do sentiment analysis, so even if it was better, it doesn’t do it! End of story.', ""CrunchyAl: You can test it against Rankerhack problems, and it doesn't do well. It may give you the correct output, but it will fail the test cases most of the time. It's mostly overhyped due to Art generated AI and is a glofied chat bot that also has some biases opinions that seem neutral, but it is not really."", 'Mysterious_String_23: Go with them down the path and explain how it will affect them vs what you want to do and how much better that will be.', 'balpby1989: Yikes, op basically asked GPT a question with 2 answers and blamed GPT for only answering 1 of them (and based on what op responded sounds like he doesn’t know there are 2). To me, even answering that first 1 is quite amazing for a chat bot. And if you ignore the “pride of data scientist” for a second, due to the hype of GPT, the manager asked a legitimate question for someone who is not technical and cares about the well-being of the company, and I believe it’s up to the data scientist to acknowledge and answer that question for the manager and the audience. Unfortunately sounds like op not only failed to answer the question (expensive API, data security etc), but also got frustrated because someone “lower” on the technical level asked a legitimate question that belittled his “fine tuned” model and his pride of a data scientist. I also feel bad for GPT as a bot who can’t defend itself and simply wants to help.', 'psychmancer: So I work as a DS and what is weirder here is not that they want you to use chatgpt but they are getting a chat bot to do sentiment analysis. It would make more sense to have chat write you a python code to do it in nltk in python and then run it normally. This feels very executive level request and all that implies', ""Relevant-Rhubarb-849: I think your test question is entirely wrong for your purpose.  Chatgpt isn't a general analytic engine intended to do math. But it is a good text content processing and summarizing engine.  It can predict what likely follows from prior events.  While I don't know anything aboud how metric and quantified sentiment analysis is, from the name I'd imagine it involves inferring what people are likely to do given what they have said before. That probably is a great job for chat gpt"", ""brokened00: Yeah, honestly I'm not a cocky or overconfident data scientist, as I've only just finished my masters a year ago, but it irks me beyond control when someone who doesn't know the difference between a histogram and a bar chart tells me what deep learning models to use..."", 'Acrobatic-Artist9730: Can you give me the data in scanned word tables?', ""brokened00: That's really funny!"", ""brokened00: That's a great example. It's something a human could almost never get wrong."", 'TwistedBrother: This is a facetious example. It’s entirely possible to both specify and learn implicitly that size modifiers refer to the relative size of the object they modify and not the absolute size of all objects. It’s not necessary but it’s plausible and probable.\n\nIn fact, asking chat GPT “which is larger: a large mouse or a small elephant” will almost certainly produce the correct response. I can already imagine it lecturing me on the average sizes of these animals. It’s partially about how we ask the question. Our ability to ask questions depends on our own mental models, like subject verb object. In English, which is not universal. \n\nThe machines will continue to develop reasoning skills. Those who don’t get how the model reasoning works (even superficially if they can’t tell you the difference between a transformer and an lstm) are bound to either over estimate the model or critique the model as if that critiques the entire approach. The example of using the large mouse and the small elephant does the latter.', ""brokened00: That's a great suggestion. I suppose I ought to at least assess its usefulness in a scientific way, rather than just basing my opinion on light reading and informal experimentation."", 'Tiltfortat: This should be a last resort solution imo. You’re the expert in the field and it is your job to clearly communicate that building an ensemble model to include ChatGPT just because someone higher up jumped on the hype train would be a waste of resources. \n\nManagement has the last word of course but in these situations it’s very important to make sure everybody is aware of your position so this cannot be blamed on you in the end. \nAlso, this should be a red flag for you and you should consider looking for a different company in the long run.', 'brokened00: Fair enough. I will raise some of those points and try to find out about pricing/terms/etc from OpenAI to help bolster my argument. \n\n It wasn\'t performance so much as ""unreliably"", I suppose, that I was raising. Like I said, the guy suggested using it to basically query databases. I think we all know that a pandas query would yield more credible results than a chat bot. Hence why I tried to ask it logical and easy math questions to see if it could be a reliable tool outside of just NLP. It cannot, at least yet.', ""brokened00: I agree. The higher up seemed to think otherwise. And when I lightly pushed back on that notion (explaining why that wouldn't be reliable), they told me I need to do my due diligence and explore the use of the tool for those purposes. I am not kidding, they have zero knowledge of this area nor even tangentially to this area."", 'maxToTheJ: Yeah OP isn’t appreciating the execs business domain knowledge\n\n/s', 'Drakkur: This phenomenon is common across all fields where the metric for success is money/company position. \n\nJust because someone has great skills in (business acumen/networking) doesn’t mean they generalize to other areas (which is what we see in the OPs post). The same is true for technical experts thinking their their business acumen is equivalent to their technical aptitude (which is what we only see when we step back and look introspectively).\n\nThere would probably be less head butting between teams like this is all of us thought more holistically about problems/solutions.', ""brokened00: Almost didn't read that as sarcasm and started to lose it."", 'brokened00: Great points. Thanks for laying it all out in a way that should be digestible for higher ups!', 'Oceanboi: Based 300iq play', 'koolaidman123: Theres literally way better open source embedding models, using openais embedding service is pointless', ""brokened00: Good suggestion. I have actually played with GPT 2 and CLIP embeddings before in a past project to feed into LSTMs to do classification tasks on video data. \n\nAs you mentioned a BERT model from Hugging Face, that is essentially what we've been using."", 'GeorgeS6969: I think OP’s just frustrated at the situation, rightfully so if you ask me, and threw the first thing he could think of that tripped chatgpt.\n\nI agree it would be interesting, but I get why OP’s not interested.', ""Relevant-Rhubarb-849: I wanted to point out the OP also got the math wrong !!!!!  The problem is the question is ambiguously worded and he chose one interpretation of it when there is a different one.\n\nIf I say to you two cars drive 200 miles in 4 hours that could mean either:\n\nThe sum of the distance travelled by two cars was 200 miles \n\nOr it could mean each car traveled 200 miles.\n\nJudging from the OPs follow-up question he thought he was asking the second scenarion but really the first question makes more sense---after all why supply the irrelevant information about how many cars were driving unless of course were speaking of the sum, in which case that number is needed.\n\nThe error the chat gpt makes is not the one the  OP thinks it is but rather a different error.  ChTgot first computes a speed as though 200 miles is the amount each car drove then it uses this speed in an equation that is correct for estimating the time needed to reach a total summed distance of four cars. \n\nSo it did make an error.  \n\nBut then when the OP tries to coach it in the right direction he's making the wrong assumption again assuming the question was not ambiguous."", 'Oceanboi: Yes but what does that specifically entail?  If the idea is “I’m going to ask chatGPT to analyze this text data for sentiment”, \n\n1) there are already models that do this without having to back door the tokenization properties of a chat bot to perform weird black box sentiment analysis\n\n2) by using a physics problem he is demonstrating that the model cannot solve or put into practice simple deterministic functions.  Sometimes, it will provide the correct function and in the same response abandon that function.  This is just bizarre behavior, so how could I rely on anything it gives me regarding sentiment analysis?\n\n3) Can you imagine what it’d cost?  You’d have to give it pretty large prompts to analyze or multiple smaller prompts in tandem.  I’m less versed here but I can’t imagine it’d be something a company would be willing to pay for.  But maybe take the neutral route and check out chatGPT for no other reason than to get paid to play with SmarterChild2.\n\nThese are the three major issues I see.  People are mistaking what chatGPT is and isn’t because it is impressive and mind boggling to interact with.  But once you spend enough time on it you take a step back and realize how volatile its outputs are, even on lower temperatures.', ""brokened00: But, surely if this bot is to replace specialized tools that are proven reliable in their applications, then it should be expected to answer an elementary level logic question.\n\nIt's really not much of a math question, but rather a logic question LIGHTLY disguised as a math question. I wanted to see if the model would try too hard and overcomplicate the simple, and that proved to be the case in this scenario."", 'brokened00: The cars are driving independenlyt of each other. Increasing the number of cars by a factor of 2 is not going to make all of the cars LITERALLY DOUBLE THEIR SPEED.', ""Relevant-Rhubarb-849: Whoa! Now that I think about it further I realize the question was so ambiguously worded it had a third possible interpretation for which the gptchats first answer was correct\n\nThe third interpretation is that in the first sentence of the problem the number of cars is irrelevant and it's simply telling you how fast all the cars drive.  So 50mph. The second sentence is asking how long it would take four cars to elapse a total of 200 miles.  That would be 1 hour with each going 50 miles for a total of 200.\n\n\nFinally I note that the original question also has a fourth and unanswerable interpretation.  If we assume 200 miles is the sum distance two cars went, they might have travelled different fractions of that .  Maybe one car drove 150 miles and the other drive 50.  In that case there would be no way of answering how long four different cars would take to sum to 200 miles.  Unless you assume the second two cars were identical to the first two cars.  Which in fact gptchat says it will assume.  \n\nSo I'd say most of the confusion here is on the part of the person asking the questions not chat gpt.\n\nAn ambiguous question was asked and under one possible answer gptchat got the answer correct on its first response. When the author told it it chose the wrong interpretation it corrected its answer to the correct answer to a different interpretation \n\nSo the OP was mistake twice ! Gptchat was correct both times ! Ha!"", 'GeorgeS6969: How can chatgpt predict what likely follows from prior events if it’s not a good general analytic engine? In the test question it literally failed to predict how many hours it’s likely to take four cars to covers 200 miles at 50 miles per hour. Do you really believe predicting people’s behavior requires *less* analytical capabilities?\n\nThis is such a weird take it reads like it was written by chatgpt.', 'Categorically_: No', 'NewEcho2940: I just say why like I’m genuinely curious. They have no answer and look stupid.', 'readermom123: I also really like the ‘is a banana bigger than a cat?’ Example that was going around.', ""nth_citizen: > This is a facetious example.\n\nWell I got it from Daniel Kahneman's book, Noise so it was not intentionally so.\n\nI believe such examples are useful to 'burst the bubble' of hype around chatGPT to enable more realistic discussion."", 'AltOnMain: Yes, and assuming that chatgpt gets weighted near zero and the higher ups ask in depth questions about the model, it will be very easy to put the results in to a few powerpoint slides that easily and convincingly communicate “chatGPT sucks for us” without you having to get sucked in to a power struggle.', 'dfphd: I\'m going to go against the current here: hell no.\n\nThis is exploiting the fact that people from a science background feel the need to fairly assess things that don\'t warrant being fairly assessed.\n\nHitchen\'s Razor: what can be asserted without evidence can also be dismissed without evidence.\n\n>It should be a particularly powerful tool to analyze large volumes of data coming in\n\nSays who? Why? Based on what? Measured how? \n\nLet\'s flip the script here (because I\'ve been on the other side of things): if a data scientist were to go to a CEO with an idea and said literally the same thing ""this technology should be a particularly powerful tool to analyze large volumes of data coming in"", who thinks the CEO is going to blindly agree to it without justification?\n\nIf you raised your hand, use it to slap yourself in the back of the head.\n\nI\'m more than happy to entertain the idea that chatGPT could be revolutionary to any number of industries and applications, but before I dedicated resources  to it - resources who already have a god damn day job - I am going to need either a) a business case developed by someone else that clearly highlights the value of chatGPT for my (or a similar enough) problem statement, or b) a very well thought out business plan that details how we would derive value from it relative to what we do today', 'jobeta: Exactly. Try to remove pride from the problem. They’re probably wrong but also paying you so you can ask if “they would like you to pivot and assess the feasibility and cost if switching to ChatGPT.” And then treat that as an experiment. It’s very likely everyone will learn something. Last I checked, the model behind ChatGPT isn’t open source anyways is it? So even if you wanted, you couldn’t fine tune it for your problem. Which is nice because it means you just need to see how it performs out if the box for your use case?\n\nI wouldn’t go the ensemble model route tho. Because they don’t know what it even is and won’t expect it and it seems like a pain to maintain. I would just compare performance of each.', ""IpsoCranius: It needn't be scientific. Approach it as a product. What do you give it? What does it return? What features does it offer, and what does it need? How much would it cost to adopt into a pipeline (labor and opex), and what would the ROI look like?\n\nYou know it's the wrong tool, always remember to help others to save face and you seek alignment, and you'll see their trust in your skillet grow."", ""bakochba: Do they think you can feed it data (company data!) And tell it to run an an analysis and it would spit out results?  Everytime I've used it you just ask questions or ask it to generate code but even then it's example vide nothing custom"", 'proof_required: Yeah one thing I read over and over here and also out there is how come people believe that we should be listening to execs because technical people have no understanding of how real business works. This whole narrative has been sold so much from top level that now even technical people have started parroting it.', 'artsybashev: It really depends on what your are trying to achieve.\n\nI think that if the boss wants to micromanage and make you use embeddings from a buzz-company, there is nothing wrong with trying it out. The worst that can happen is you get the metrics to show that gpt3 embeddigs provide worse results and costs 1000x more than bert from huggingface.', 'artsybashev: Unless your classification task goes very deep into the inner thoughts of the author of your documents, you probably wont beat bert. GPT3 is good in understanding the intention of the author but more commonly we are classifying documents in a more shallow level based on just the information content. Bert is great at that.\n\nBut as said, no harm trying it out and maybe you and your boss will learn something interesting. Just have some fun trying new things out!', ""brokened00: No, that's not how it works, friend."", 'Relevant-Rhubarb-849: It doubles the total number of miles their sum elapses in a given time.  You are not seeing that the question is ambiguously stated and has several possible interpretations.  Think it over and you\'ll see the other ways it can be interpreted.\n\n\n1.  Two cars ""each"" independently drive 200 miles apiece in 4 hours\n\n2. Two cars drive a total summed distance of 200 miles in  4 hours. (100 apiece)\n\n\nGiven the complete context of the question, the second one actually is the more logical interpretation not the first one.  \n\n\nOtherwise the original question is as stupid as asking, if you have one bucket that holds 2 gallons and another bucket that holds one gallon, how many buckets do you have?  Or asking what color was napolean\'s white cat? Or how many green Chinese pots in a dozen?\n\nAn intelligent  person not assuming the questioner is being devious or stupid would assume that knowing the number of cars that went 200 miles was not irrelevant and so would be led to assume that the questioner meant the total elapsed miles of the two cars not their individual milage.', 'Relevant-Rhubarb-849: No I was being perfectly serious.  Chatgpt and transformers are at their heart trained like BERT in predicting the missing thing in an ordered set. They go way beyond that since they have internal memory states as well that are keeping track of objectives and prior info.  But these things are not storing details like how to do physics or math in analytic terms.  They are storing guidelines and connections between ideas.  The latter is good for fuzzy reasoning and generalization and perception of abstract patterns but less good at memorizing cold facts like the millionth digit of pi.  These things only have about 80 billion parameters and even fewer LSTM feedback states.  So compression theory tells you they can\'t memorize that many things.  Thus if you really want to have it not make math errors then it has to be worse at something else like memorizing us senators or movie stars or Chinese cukturak affairs.  Questions that drill down on acute specific knowledge are likely to find a blind spot.  But top level patterns and connections and summaries of observations are what a transformer type system is good at.  I have no idea what kind of data the OP is analyzing is.  Text comments of sentiment? Or tick boxes on a scale of 1 to 5 on well constructed customer queries?  A chatgpt could extract the meaning of a customer comment like ""it\'s better than nothing at All"" or ""I\'d rather eat poo and die than use this tool"" pretty well. It might be really bad at constructing numerically precise things like a histogram of how many red headed customers rated the new hair dye at a given rating.', 'Relevant-Rhubarb-849: Do you realize that the gptchat did get the right answer in the end?  Two cars each traveling 25mph would cover a total of 200 miles in 4 hours.   Individually each would travel 100 miles.  Four cars would travel 200 miles total in two hours.\n\nThat was the final answer.\n\nThe poster wrongly assumed in their mind that both cars traveled 200 miles individually. Thus he was dilberateky trying to supply irrelevant information to confuse the chat gpt but ended up not realizing he hard musphraded the question and got got it right in the final answer', 'venustrapsflies: ""I don\'t have time to explain the details to you""', 'speedisntfree: This is what I have seen good experienced people do. They calmly ask a few pointed questions in a non-confrontational tone and usually the person realises they need to pipe down to avoid making themselves look stupid.', 'ShawnD7: “Are you questioning me?!”', ""brokened00: Definitely. \n\nMy initial stance was basically summed up well in what you just said. I'm already spread pretty thin at work with multiple projects, each of which should easily have dedicated teams working on them, so I really didn't want to spend a lot of time trying to justify my reasoning for not going the GPT route.\n\nI almost was shocked that I would be expected to look heavily into something that I advised would most likely not be fruitful. I feel like my expert opinion (however capable of error) should be weighted more heavily than a non-expert opinion."", 'psychmancer: sadly this is my boss every time I ask for a new toy in my job, they keep asking me to explain ""why it is needed"" and ""is it needed right now""? Like I have answers to that, I just want to play with AI or try a new tool I saw on a YT video.', 'brokened00: Yes, they literally think that. I think people are grossly misinformed on what some of these new AI solutions trained to do.', 'maxToTheJ: Its 100% beneficial for ones career. The part I take issue with is the amount of self brainwashing to parrot it unironically in an anonymous forum where you have nothing to gain.', 'Relevant-Rhubarb-849: Okay then tell me how it does work', 'brokened00: So, if 10 people each have a heart rate of 60 BPM and you add 90 people to the room, their hearts will all beat at 600 BPM and explode inside their chests?', 'GeorgeS6969: That’s fine but you did not justify your previous claim that chatgpt is able to predict “what likely follows from previous events”, or “what people are likely to do given what they have said before”.', 'NewEcho2940: “How much experience do you have with ML. I have 5 years”', 'NewEcho2940: Yep, some of us around here are good experienced people.', 'MagentaTentacle: ""Are you fucking sorry?""', 'NewEcho2940: Man, if I ever worked with someone like that it would be on. Hell yes I’m questioning you!', 'proof_required: I find these kind of execs\' ideas same as that friend of yours who has lot of start-up ""ideas"" but has never written a line of code and wants you to implement it over a weekend.', ""brokened00: I believe I explained it in your other comment thread. But, the car's travel rates are independent of each other. Increasing the number of cars by a factor of 2 does not double the speed of every car. That just wouldn't make any sense. If anything, increasing the number of cars would slow things down due to traffic. But in a simple question involving 4 cars, why would they suddenly drive way faster just because of the presence of other vehicles?"", 'Relevant-Rhubarb-849: Notice how you used the word ""each"".\n\nNotice how you also are adding rates not beats. \n\nNow reread the original question. It does not use the word ""each"".   It also gives miles which are additive not rates which are not.', 'Relevant-Rhubarb-849: Well I was expecting people had an inkling of the research into transformers when I aluded briefly to some general properties of chatgpt but I see I should not have assumed that prior knowledge so excuse me if the following is too pedantic.  Not trying to insult anyone\'s knowledge.\n\nBERT is half of a transformer pair.  It is often trained supervised on the task of ""leave one out"" recovery problems so for example give it an English sentence or a genome string and mask out a word or phrase or run of characters. It thus predicts the missing characters in the string.  If you always make the missing word be the last word in a sentence you now have a method of generating sentences by having it emit the next word in a sentence given all the prior words.  Transformers add in even more ability to remember contexts and can transform  information in one form to information in another form.  Thus if the transformed form of a set of input texts is ""summarize"" ""or find relations it has the ability to draw inferences or predict the appropriate response to some input.    Internally chatgpt is using all these tricks to memorize the guidelines it must follow and transform varied information in ti English sentences and  paragraphs that are predicted to answer a question.  That\'s what I was referring to. \n\nMy general drift here is that I find it shocking people think chatgpt has expert or domain expertise or is good at applying math. It\'s not. It\'s good at predicting from patterns. You might say well math is a pattern. And it is but it\'s also such an exact pattern that learning it requires more domain expertise than you could encode in its tiny brain and exact math is not just abstraction and prediction of what the likely response should be.  \n\nThus people should be not holding chatgpt up to the light for perfect accuracy but for an amazing ability to summarize, guess conclusions and stunning talk in English that is coherent across a paragraph.\n\nThe idea of extracting sentiment from consumer information could be plausibly what it is really good at. It depends if the data is soft like text or hard like numerical entries', 'Relevant-Rhubarb-849: Read the stated question. No where does it say the cars both went 200 miles.  They might have gone 100 apiece for a total of 200.  Gptchat logically assumed the latter the questioner assumed as you did the former.', ""brokened00: I see your perspective. I just don't believe a human would really interpret the question in the specific way you are describing."", 'GeorgeS6969: Okay so chatgpt is not good at predicting “what likely follows from prior events”, or “what people are likely to do given what they have said before”.', ""brokened00: I see what you're saying, but humans interpret the question in a different way. The model is meant to have human-like conversations, but completely misinterprets what I am asking, where a human would usually not have that issue."", 'Relevant-Rhubarb-849: Thank you for acknowledging. Since others may be thinking along similar lines let\'s extend the conversation on your last point.\n\nConsider this ambiguous puzzle question \n\nI have 100 spiders.   They lay fifty eggs a day.  How many eggs total are laid in 2 days?\n\nIf I had said ""chickens"" in stead of spiders you would know from experience that a single hen can\'t lay 50 eggs by herself in a day. Thus you would immediately assume that collectively the 100 chickens produce a total of 50 eggs a day.  \n\nBut I said spiders.  And you probably know that spiders can lay a lot of eggs at once.  You probably aren\'t an expert on how many that is or how often they do that.  But it might be reasonable to assume that the 50 is the average number per spider.\n\nSo in the case of chickens you\'d answer 100 in two days and in the case of spiders you would answer 10,000\n\nThe case of the cars here is not only ambiguous but a possible red herring is inserted.  Why say 2 cars?  If it\'s irrelevant Why not say a car can go 200 miles in 4 hours?  But if it is relevant then it\'s logical to assume it\'s meaning is 100 apiece.\n\nFor example let\'s rephrase the question:\n\nMy fleet of cars can cover 200 miles of the city in 4 hours. If I double my fleet how long will it take to cover 200 miles?\n\nI think you might from this wording think that 200 is the sum of fleet-miles\n\nNow is that really how a human would read \n\nMy two cars can go 200 miles in 4 hours.  How long would it take with four cars?', ""brokened00: I can see how interpretations and wording can cause undesirable results and how your example illustrates that point. But I also think this somewhat bolsters my thoughts that using a query specifically designed to find what you desire as an output would mitigate that issue entirely, because the logic chain won't be inside a black box, so to speak."", 'Relevant-Rhubarb-849: I agree but I want to add an additional insight about chatgpt that makes it a slightly different level of AI.  \n\nWhat you said is correct that a single query to an Ai in English is prone to an ambiguity pitfall unexpectedly whereas a structured query with a deterministic algorithm is not.   \n\nBut chatgpt has the novel property that you can talk back and forth in a way that at some point both of you understand what goal of the query is. This is different than anything that existed before.  Now it\'s true that this is still at a primitive level where there\'s no assurances the AI then actually does what was mutually agreed upon.  But that\'s a whole different problem.  Ignoring that secondary compliance issue the idea that you can eventually communicate with a back and forth that reaches sufficient clarity is new.  The final problem is that even if the desired outcome is fully agreed the AI might do it wrong unintentionally.  For example, ask your 6 year old what 6 time 9 is and after you explain multiplication and get to 2x3 tables in mutual understanding they still might mis compute. \n\nIn the case of a structured query the algorithm isn\'t helping you construct the query.  You will get what you asked for but you may not be able to ask for what you want.  If you ask ""is the teeth baring person in the picture happy or frightened"" you\'d be able to cobble some varied code to hunt for teeth and some rules that might spot certain instances of fear or pain.... but you\'d have a hard time really constructing that query.  Even if you could think of how long it would take and you might have a whole lot of other types of queries.  \n\nThese chat gets can be programmed in English.  You describe a lot of things about euphoria and fear in plain English much better than any structures query can be written to represent tgat.\n\nAnd then when it doesn\'t quite work right you can easily say what\'s not right \n\nSo this back and forth to a mutual understanding is something I think we just turned the corner on in AI tgat wasn\'t there before chatgpt \n\nLots of improvement needed for its encyclopedia stored of knowledge , on compliance, on accuracy checking are needed.  But the big step is the elucidation of a negotiated mutual understanding in plain English rather than code.\n\nSo I\'ll forgive it for math errors when it can be coached to the right answer in the end\n\n\n\n\nBy the way, I don\'t mean to tell you your bussiness. You are the domain expert on what is meant by sentiment analysis.  I suspect from your point of view that it\'s probably more numerically well defined than fuzzy like text English.  So you may be quite correct that staying  away from a black box is the right move.  Perhaps you can do both though.  Try gathering both numerically quantified data as well as qualitative impression data and try to see how well gotchat can make one correlate to the other .  That way you can argue when gotchat does better and when it does not concretely while still following management direction.  Ultimately you will get more mixes of data collection if it works and someday it will be ready\n\nIn the mean time you can use this example to demonstrate the ambiguity problem to management']"
1676299387.0,13-Feb-2023 06:43:07,language model,datascience,111aiza,"Senior data scientist, 8yrs experience in deep learning + MS Electrical Engr... What's next?",Sleepy_Carrotz,0,https://www.reddit.com/r/datascience/comments/111aiza/senior_data_scientist_8yrs_experience_in_deep/,"30F living in Austin
MS in EE at 22, 8 years of work experience in deep learning in which I went from Engr I to Senior.

Have extensive experience under my belt   demonstrating everything from expertise in multiple languages and frameworks, aiding directly in exec level business decision making, excellent cost savings, and a large range of problem spaces and deep learning work. I have excellent communication skills and have served as project lead for several (small, 1-3 person) teams. Spaces include valuation modeling, chemometrics, object detection, image style (using triplet loss), consumer-product matching, consumer-agent matching, multi arm bandits in chatbots, and various clustering problems of locations, consumers, and products. I also was in charge of labeling training & QA over about 20 offshore folks for image projects.

I'm curious about dipping my feet further in deep learning but also in management by going for a tech lead / team rep position at my next company but I've no idea how to apply for something like that or what kind of position I should be seeking. Is this typically something you have to earn your way into via promotion rather than through applying? 

Medium sized company just acquired, wanting to look elsewhere. Have been working here for 4 years and just want something new.",6,"[""therealtiddlydump: >I'm curious about dipping my feet further in deep learning but also in management\n\nYou've described the two different paths -- individual contributor vs manager. You might want to get that sorted out first."", ""dfphd: >I'm curious about dipping my feet further in deep learning but also in management by going for a tech lead / team rep position at my next company but I've no idea how to apply for something like that or what kind of position I should be seeking. Is this typically something you have to earn your way into via promotion rather than through applying?\n\nIf there is a job posting for it, it means they couldn't find someone internally to do it. So the answer is simple - apply.  \n\nYes, at some companies they will very much focus on promoting internally, but that really only works at companies where you have a LOT of junior people and a huge recruiting pipeline. And even then - at some point, you are still going to be open to bringing people in from companies of equal or greater statute to yours.\n\nSo if you're  Google, you're probably not hiring a Manager from a low level tech company. But are you going to take a Manager from AWS or Apple? Probably. \n\nBut for most companies, as I said, if there is a job opening it's because they need someone to come in and they couldn't find someone internally."", 'Sleepy_Carrotz: Right,  which is why I stated cursory interest in a team lead type position', ""Sleepy_Carrotz: Gotcha!!! I have a way of overcomplicating. You're right. I just need to keep looking"", 'brrow: Many places are required to post the job externally even if the hire will eventually be internal', 'CliffDraws: Word of warning, if you hate lead/manager it can be extremely hard to get back to a singular role.']"
1676251500.0,12-Feb-2023 17:25:00,language model,datascience,110vut0,From Market Research/Insights role to Data Analyst? Looking for advice,Patient_whale4209,0,https://www.reddit.com/r/datascience/comments/110vut0/from_market_researchinsights_role_to_data_analyst/,"I have 3yrs of quantitative research experience and currently work in a small Analytics team at one of the top market research agencies (same league as Nielsen). 

I’m quite good at analysing small data sets (a few thousand rows) and building simple models in SPSS and R. Been told I make impressive slides that tell stories with data in plain language - I’m from marketing communication background.

I’m also doing a masters degree in Business Analytics on the side. It’s been 1.5yr, the progress has been slow bc studying part time, but all my grades are 90+ and have learned the basics of Python, SQL and Tableau.

Still I feel like I don’t quite meet the requirements of most junior Analyst jobs. It seems I’ll have to either take on grad roles at bigger companies or go to small data consultancies if I’m to take a leap. I’m an immigrant, already in my early 30s and there’s financial pressure (single, saving for an apartment).  

Not sure what should be my next step. Welcome any assessment on my situation, and advice on what kind of jobs/ companies I should aim for, which area I should focus my time and energy to learn and develop, etc.

Thanks in advance!",3,"['acewhenifacethedbase: Wait, why do you think you don’t meet the requirements for a junior analyst role? Because you haven’t graduated yet or haven’t picked up all the skills on their JDs or what? From what you’re saying I think you may be more than ready for Analyst roles, and remember, just because you don’t get a lot of callbacks or you fail some interviews doesn’t necessarily mean you couldn’t do the job or won’t get one.\n\nWhich Analyst roles are you looking it? Will they pay much more than your current role?', ""Tysonko: Thank you for your input,. I should have clarified this, I'm looking for a junior Data Analyst/Data science role and trying my best to meet requirements for that. The reason I don't think i meet the requirements is because I basically self taught all this myself in the last 5 months and I do not have any real world experience in these roles before. And the nearest one being my old job (had to quit to pursue the masters degree as I had to move countries) is the only semi-relevant one. As of currently I'm a full time student. And most of the internships I applied to required me to be in my penultimate year (second last year) so I cant even apply to those since I only have a single year degree."", 'Patient_whale4209: Thanks for your reply. I think what I lack is ‘real-world’ experience with the tools I learned at school. For example, sure I’ve written SQL to query mock database for school projects and can answer those interview questions, but I never used SQL at work writing queries on real database. Same with Tableau and other tools.\n\nI’m not sure if putting together a profile of side projects I’ve done using these tools would help?']"
1675363158.0,02-Feb-2023 10:39:18,language model,datascience,10rx6tv,What else is left? Should I continue with my masters in DS?,burralohit01,551,https://i.redd.it/os2nnoqt2vfa1.jpg,,270,"['mrbrambles: Chatgpt is good at fluffing it’s own resume', ""Unlikely_Tie8166: ChatGPT subscription is 20$/month. You'll have to work for less to remain competitive in the market. Jokes aside, I can't imagine data science being automated in the foreseeable future. That said, you might have to deal with lesser amount of annoying crap that usually takes 90% of data scientist's time"", ""Renegade7559: Data scientist here.\n\nChat gpt is a great tool but it's confirmation bias in the extreme. some times it writes great code, sometimes not so great \n\nThe better more specific instructions you give it, usually the better code.\n\nBut, theoretically it doesn't understand the justification for why you are using a specific modelling approach.\n\nAsk chatgpt to give you a random Forest algorithm for your data. You'll get one. It doesn't mean the models appropriate. Likewise for exploratory data analysis. It cannot ask the right questions or understand what to ask."", 'therealtiddlydump: The robot said it, it must be true', ""PryomancerMTGA: But can it post to reddit while it's supposed to be working like I do?"", ""thevillagersid: I'm pretty sure the only real expertise ChatGPT has is in gaslighting users. Your data science career should be safe for the time being."", ""data_ciens_ultra: Do you guys realize that ChatGPT is just a chatbot right?\n\nany answers or solutions it creates are just estimates of what it thinks a human would say.... I don't know how to explain how unrealistic it is to think that ChatGPT is going to be replacing jobs in the tech industry....\n\nlets just put it this way: if your job can actually be done by a chatbot, you deserve to be replaceable..."", ""MeatMakingMan: Now ask a data scientist about the tasks chat gpt CAN'T automate"", ""bikeskata: Tbf, you shouldn't continue a masters in DS because most of them are ripoffs, not because of chatGPT"", 'ThePerfectCantelope: Nope, May as well call it quits and switch to carpentry', 'bmo333: I’m still going to pursue a MS Data Science this fall.', 'AdFew4357: That’s it. That’s the end of this sub for me. I just hate this subreddit so fucking much. Can we actually talk about fucking real shit. What do you want to hear OP? Yeah man; chat gpt is taking all of our jobs. It’s gonna replace all research scientists, engineers and analysts. Human existence is meaningless. Go quit. Like what? I swear I tested the limitations of chat gpt, and yeah while it’s impressive we still have such a long way to go when it comes to massive takeover of these systems. Again I’ll say it again you guys are too trustworthy of those systems and put too much faith in them. I’m not batting an eye at chatgpt and I won’t for another 60 years', ""aeywaka: Ha! I'd like to see chatgpt try to handle stakeholder expectations when they want to change an annual project...annually."", 'Coollime17: The only part of your job it’s replacing is copy pasting from stack overflow. Honestly I’ve been loving the GPT-3 based code completes as it really speeds up a lot of code writing tasks.', 'devastation35: Last part is true. LOL, ChatGPT can barely do arithmetic even though it gives correct formula for my homework.', ""miscmate: I've been working in DS without a masters for about 4 years, most of the value is in to ask the right questions (what are the business needs, how do you define the target, how does the business links to the output of the model, do they understand it, which population are you talking about, is there specific bias to be aware of) . Best case scenario, ChatGPT can only give you answers."", 'on_the_pale_horse: Wait this is a serious question lol  \nI thought this was r/ProgrammerHumor', 'Odd_Application_655: ChatGPT is perfectly able to automate pretty much everything that is in a typical DS course, BUT a senior data science knows that this ""pretty much everything in a typical DS course"" accounts for a small portion of the actual work. You will lose most of your time trying to understand your data, create the variables out of business needs and justify your work to people who do not understand what you do.\n\nAlso, any AutoML package like PyCaret is able to streamline basically most of the ""vanilla"" duties of a data scientist.', 'kaisermax6020: You still need someone who works on the specific requirements and parameters the AI needs for generating these tasks.', 'First_Bullfrog_4861: jesus bro have you learned anything in your studies. chatgpt is not capable of a single one of these points rn.\n\nit simply regurgitated a list of DS subdomains.\n\ndon’t panic, pack your towel, keep going, it’ll be worth it.', 'riderx65: Will not completely eradicate the job but will reduce the number of people required. Also job roles will change.\n\nPretty much the same that happened as cloud services were adopted.', 'Tadpole_Southern: It’s a tool to make you more efficient, it’s not going to replace you.', 'Allmyownviews1: I was playing with chatgpt last week asking it to put together Python functions to collect data, generate distributions, chart some data for a range of parameters, perform some basic ML and present the results. I was mainly looking to see if it would do an IFORM diagram.. but was testing the other elements.\n\nThere was not a single section of code that would run without some modification. It generally would do the task with simplest route, but i was both impressed and concerned in equal measurez', 'YungTerpenzee: Do a masters in statistics instead', 'bilalis99: You would be the one building those chats...', 'Insighteous: In my opinion. Every data scientist asking this is not qualified enough in terms of degrees. Otherwise one would know that there is much ChatGPT cannot do.', 'whispertoke: Like others have said, it requires a ton more training. It will not be able to give more nuanced or appropriate solutions unless results are being reviewed and rated, and the only people who can effectively rate serious technical DS outputs are serious technical data scientists. I wonder if at some point big tech will employ DS people to use AI products and give detailed feedback on solutions, but I could see free enterprise services being offered in exchange for that sort of thing...', 'ach224: Chatgpt can do a lot of shit according to chatgpt.', 'ccwhere: Im a quantitative ecologist, which basically means I write R code for a living. I’ve experimented with the statistical modeling capabilities of ChatGPT. The tech is impressive in that it can fit models and return undergraduate-level interpretations (complete with errors), but it completely falls apart with any degree of complexity. The other commenter saying that ChatGPT is good at gaslighting analysts is spot on.', 'RationalDialog: ChatGPT is the most confident job scammer ever.', ""Frequentist_stats: **It can assist you, but can't replace you.**"", 'renok_archnmy: It is entirely too entertaining to watch tech people freak out about ChatGPT threatening their livelihoods after decades of tech people automating away other people’s jobs. I can only imagine the schadenfreude felt by all those whose ability to earn was automated away by some brogrammer over the last 10-50 years. \n\nAdmittedly, I do wish efforts were spent actually automating away the mundane and tedious parts of regular people life, like folding towels and matching the socks after laundry, cleaning the toilet, and fetching groceries. Dusting baseboards and ceiling fans, changing lightbulbs, putting the dishes away after cleaning, ironing pants, vacuuming the couch, etc.', 'willard_style: Institutional / Domain knowledge! Anyone can build and run a model, but you (can) level up your domain knowledge to understand how all the pieces fit together from problem, to solution, to presentation (or handoff)', 'Financial-Back313: yes you should', 'Opposite_Meeting8136: Maybe read the last line', 'Voth98: It literally can’t do any of those things yet. Someone please show me examples of it doing any of that with a real dataset. Hell, even a toy dataset would surprise me.', 'rroth: ChatGPT is the Chuck E Cheese of computing technology. Your future job is safe.', ""swappybizz: Won't / shit code any original work. All derivative."", ""CrunchyAl: I doubt it can do them well. I used ChatGTP on Hackerrank and Leetcode questions, but the code it gives isn't the correct output or, in most cases, from my experience, fails the test cases. Honestly,  I think you'll be fine and should continue."", 'neurodiverseotter: I tried to give ChatGPT some medical questions today. It gave me false correlations of two symtpoms, when asked for a source it invented a study and the authors names. Programs like these will not replace humans who can verify their results anytime soon', 'bbateman2011: Keep Calm and Carry On', ""Meaveready: Can ChatGPT automate the process of feeding the data and requirements and prompting ChatGPT exactly with what is needed then train and validatr a ML model and manage its deployment? No? Doesn't that already sound like AI-assistance more than automation?"", 'Living_Teaching9410: I asked ChatGPT how to compare between K-means and HDBSCAN clustering and his answer was Silhouette Score😁which is totally wrong', 'billyions: Human level intelligence is widely in demand. Being able to use intelligent assistants has always been valuable.\n\nThere is so much more to learn and do.', ""justneurostuff: chatgpt can't automate any of those things. esp if this post isnt a joke yeah id agree you have a lot more to learn about ds"", 'clf28264: Garbage in garbage out.', ""great_raisin: The true value that a data analyst brings (that can never be replaced/automated) is domain knowledge and a deep understanding of the industry/business. Someone who's been working at a company for a few years will have vast amounts of organic knowledge that can't be easily documented or turned into a set of rules that can be used by a computer to do their job."", 'dgrsmith: I’m looking forward to data science training being less about deep programming instruction and how to build a tool and more about how to ask and answer good scientific questions. As someone with PhD training in a scientific field, I think one of the greatest advantages I have is learning how to consume and produce solid research. Some of my advisors referred to their PhD training as teaching them how to learn anything. If masters programs started teaching solid research skills with lighter focus on tool implementation, that would be great. You need to be able to read code for sure, but I don’t think it will be as necessary to pull rarely used classes in pandas out of your back pocket at a moments notice.', ""slippu: Chat gpt doesn't really work the way most people think it does, it is more accurate to describe it as a translator but instead of translating from english to another language it translates english words to tagged long form phases + ideas in a form that is contextually correct to your request (programming language is simply another form of context to express an idea). It does do a great job of giving the illusion that it's thinking though!"", 'lambofgod0492: It literally says a human data scientist is still required, so 🤷🏻\u200d♂️', 'umairican: ChatGPT is great, and will continue to improve, but it still is full of errors and makes the mistake of providing code that doesn’t work, but does so with such confidence that you believe it’s right at first. Knowing why it isn’t right, and how to fix it, is a reminder of the value we will continue to have for some time.', 'Capitalmind: ZeroGPT identified chatGBT generated content with generic searches. I made two changes to my input and get 100% human written responses that zeroGPT cannot detect. So while I was thinking this was an area that still needed to be fixed, so far it isn\'t.\n\nMusings aside, I think we\'re a long way off feeding legislation and standards into a machine and getting coherent concise and correct information out of it. Take something like the Wikipedia, millions of definitions and titles however there are millions of duplications citation errors and broken links. I think we really will have cracked AI when we can feed it something like Wikipedia and reduce the entire website to a knowledge base of concise and perfect data. AI is still limited to snippets and doesn\'t seem to retain any memory and a good example is getting it to teach you something. A human tutor can identify your optimal ways of learning and provide examples, inferences and metaphors. Chat GPT certainly doesn\'t do that currently, though it would be good to see this level of intelligence come from the data. I\'m still a big fan of supervised AI to teach a system rather than millions of random examples. A recent example of music generated from AI was every degree of painful I\'ve ever heard. Had it been taught music theory, genres, tempos, the many sounds and idiosyncrasies of individual instruments and many playing styles I\'m sure it would do better.\n\nShould chatGPT get to a stage or entire websites have been reduced to a perfect data set, I see such websites disappearing quickly. Many wiki style and answer sites will disappear as we will get our answer straight away. They\'re still opportunity to develop other generated content such as charts and graphs from a description rather than a photo.\n\nGoing back to my early point, Western countries got together and agreed legislation needed to be standardized in something like an XML hierarchical structure and almost every country bailed on the idea. The only country that is accepted the challenge is New Zealand and their website ""better rules"" gives you an example of how this works. There is currently one company working with this technology that I know of and their results are astounding.', 'sunole123: Is this self awareness that it knows what it knows and can talk about it?', ""NOT_theprofessor: Abacus to scientific calculator \nAx to chainsaw \nNeedles to sawing machine\n.....list goes on\n Data tools to AI\nYou still need to know how to do the damn work to use the machine.\nDon't get thrown around my internet hype. Most people don't know the actual work that goes in so they assume AI as a Skynet umbrella that does it all."", 'NeoAsylum: ChatGPT talks a lot. Its a language model. Just try asking about easy mathematical problems and you will see it fail.', 'purplebrown_updown: I used it for the first time today to see if it could do what took me two hours to learn in pandas. It did it after querying chat gpt in five minutes. I’m absolutely blown away by how good it is for python coding.', 'theaimlguy: ChatGPT is certainly promising to say the least, however, people are really going overboard with their fears at this moment. There is a lot to be done to polish it and make it reliable. \nAs to the concerns of it being replacing people, it is much less likely. More so, it will be essentially complementing people and will create a niche area of experts to use it as a highly efficient tool!', 'LesFirewall: I’m getting my masters too and honestly while ChatGPT is a good tool, it’s doesn’t write perfect code. It definitely helps with some tasks but I’m still doing the majority of the work.', 'FinancialAppearance: Why would you trust chatGPT here, which is essentially writing its own advertisement?', 'cronjefourieza: Hahaha calm yourself my son. ChatGPT appears awesome but once you delve deeper you soon realise it has many flaws. Complete that degree, trust me you’ll still be using it about 20years from now😂', 'Lanky-Truck6409: Lolbro chatgpt can barely do an sqlite table that does exactly what I want and makes up fake data to make you happy', 'dtr_ned: if you are worried or think chatgpt can replace you as a data scientist…. i don’t think you’re smart enough to be one', ""nooptionleft: Man, ChatGPT is still making up numbers when I ask for mock up abstract... the tech is incredible but let's calm down a little here. It's great at giving you snippet of code for super specific task but when you ask for more generic approaches it copypaste from stuff that is already done\n\nAlso, the 1st point is straight up lying. Believe me, I've tried. I would love to not spend a week trying to understand what the hell the wet lab was thinking when he labeled the data and why the hell half of them are NA\n\nNot saying we shouldn't be thinking about this, automation is a freaking huge problem for everyone even if it doesn't put us out of a job, but I don't think you'll stop being paid anytime soon"", ""benzall: It will definitely increase productivity, but won't completely automate the DS job. But this also means that it's going to get lot more competitive to raise the bar to become DS"", ""TheTackleZone: Well, validation. Which you know because ChatGPT told you. So there you go, the bot that needs to be validated is telling you it needs to be validated. But does it really need to be validated? You may need a human to validate that.\n\nOn a separate note I'm enjoying my argument with chatGPT that highly complex GLMs are just as blackbox as GBMs, and the presence of one way tables just lures humans into a false sense of security that you know what's happening once all the interactions are applied. It saw my point initially but now it's arguing back."", 'minorcontribution: Sorry, but by now I feel like the ""will chatgpt replace me"" question has been beaten to death on this and related subreddits, with many many great explanations clarifying it will not. Even if this post is in jest, I kind of feel tired of the topic. Should I just shut up? Maybe we can create a sticky for people that feel doubtful or fear for their job security?', 'veridian21: You can tell how many words a sentence consists of, so go for it!', 'jzcommunicate: It just made a list of basic DS concepts. If that’s scaring you off, then no, you shouldn’t continue.', 'BafbeerNL: Sure thing, this will just make your job easier', ""thalamisa: Of course, it's a tool to help your work, but I cannot imagine where AIs can automate all the aspect of data cleaning. It's kinda an utopia."", 'bagHolder888: Can ChatGPT harmonic mean though? That’s the only thing that matters.', ""bobbruno: AFAIK, it's not doing the understanding of obscure business needs and framing them as a data science problem. It's not going over all your accessible data and picking what might be useful. It's not thinking out of the box to engineer features in a way that exposes the information hidden behind the noise/representation.\n\nIt might eventually do all those things and more; right now it doesn't, it makes very basic mistakes in its coding, it can't do synthesis of an entire end-to-end solution and it only knows about wha was in its training data - very expensive to update.\n\nIf it ever gets to this point, I suggest we all become lazy socialists, let the machines do the work and go sip free Mai-tais in some tropical paradise. Until then, there's a lot of work to be done."", ""PuddyComb: Lol tell it to do some linear regression for you, it will VERY politely tell you to eff off, as that's not what it's made for."", 'balpby1989: The job will be more domain focused. Barrier to entry will be lower, you will see more candidates, and salary will decrease.', 'SteakAndEggs1964: Chatgpt is something that will help, may it be with coding, stats, or etc. Its not replacing you.', 'sammyhga: 😂😂', 'rv24712: Well. If chatGPT steals my future job, I will turn on to chatCGT (here: https://chatcgt.fr/). Which will be his best competitor. At least in France.', ""ShoEnRyu: I remember someone talking about this before. You can automate and process a ton of data and replace the mediocre ones in the field. But when it comes to drawing conclusions that target the problems you're trying to fix, AI is still a ways off from doing what a human mind can do in thinking up realistic probabilities and impact."", ""wellju: It's just a tool you'll use to get more done in the same time."", 'Fifoblivion: You should check out 60 ChatGPT Prompts for Data Science by [Travis Tang](https://www.linkedin.com/in/travistang/) if you’re interested to learn the field.  \nHere is the [link](https://www.linkedin.com/posts/greg-coquillo_60-chatgpt-prompts-for-data-science-activity-7023740271486017537-Q4TI?utm_source=share&utm_medium=member_desktop) to the post.', ""TheChurchOfDonovan: If you like data science and are excited about using ChatGPT to augment your career... Then you're gonna be just great. Money's gonna rain \n\n\nIf not.... ummmmm"", 'spiritualquestions: Here\'s how I look at it. Chat GPT won\'t completely automate away the need for data scientists. What it will do is make the senior Data Scientists essentially create the same output as what an entire team used to do, by themselves. So we will see much smaller teams of seniors. It is cheaper to hire one senior data scientists for 400k that does the work of 10, than hire a senior for 300k plus 9 juniors for 900k. \n\nHowever, I think as the senior talent is hoarded by the top companies, we will (hopefully) see many more start ups popping up due to powerful new technology, and lots of talent looking for work. So the less experienced data professionals will find work at other companies eventually. \n\nOne space I find very promising and is where I am trying to specialize is in the ML infrastructure, engineering, and operations. A talented data scientist + a powerful AI tool will be able build high performing models; however, building the infrastructure around those models will likely not be automated until much later. The AI world as not ""solved"" model deployment and monitoring yet.', ""Tvicker: Just expect getting more tasks in the nearest future bc chatgpt may help to speed up most of routine (ds still has a lot of routine i wis to be automated, let's admit it). I am already asking it for data cleaning funcs and helping me with reg exps"", ""laughingwalls: Switch to stats. JK. Maybe\n\nChat GPT is going to just replace programming languages. You still have to know what your doing. It cannot do these things unprompted and specify problems itself.\n\nPeople don't like to hear it, but for building models coding has been the least important aspect. Thats why everywhere is trying to sell people data bootcamps and claim anyone can be a DS? You can teach anyone to type a python command. Actually understanding what they are doing is a completely different thing."", ""ShadowLancer128: No, ChatGPT certainly won't be able to replace lots of people in the tech industry, especially data scientists & engineers - but it's a first sign of the beginning of the end. If we have a chatbot now, and if we have computational knowledge engines like Wolfram Alpha, then how long do we expect to continue to have a future in this industry? 5 years? 10 years?\n\nThe biggest two things we can currently expect to be automated by AI's are complicated calculations in both work, and academia, and i'll give you one guess what data science truly is.\n\nHow long do we really have, huh? How long?"", 'NewEcho2940: Data Science is dead. Please vacate the premises. Leave the remaining jobs for people who understand data science.', 'jj_HeRo: As a DS: right now way better to do computer security.', 'uchi__mata: Man, ChatGPT is really good at spewing absolute bullshit in incredibly confident tones.', 'lambofgod0492: It will replace us maybe in 10 years, you are fine until then', 'MinderBinderCapital: This is just the beginning', 'burralohit01: Nicee, good one.', 'renok_archnmy: It’s a Microsoft product so more like $1500/seat/year to use the output or $1500/core/year to license for use of feature on servers with a minimum of 32 cores to support 100 headcount active users.\n\nOr you can use azure, where you’re charged $1/compute minute but they don’t tel you you gotta pay $1500/month for VPN endpoint, another $1500/month for firewall, and if you need any semblance of data privacy and security you gotta go with gov tier account. \n\nOh, and that $1/compute minute grows exponentially as all the executives and directors fumble their way through inputting overly vague prompts that don’t produce the answers they expect and can’t rage out on a robot so they hire us back so we can be the whipping post and expected to translate human language to succinct and precise computer prompts to get the exact data they want for their TPS reports out the computer and somehow this is better than just writing SQL…', ""burralohit01: >>That said, you might have to deal with lesser amount of annoying crap that usually takes 90% of data scientist's time\n\nThat’s a win right?"", 'INtuitiveTJop: I have some data science training, so I can see the relevance to the field, and I have over the last week as an analytical chemist running an analytical company increased my productivity three fold by incorporating a tool that I can direct, ask to rephrase, and essentially write legal documents, reports, maker my emails friendlier, and help me with Python scripts because I’m doing everything in this small business. It is such value for money that I would’ve had three educated employees work full time to achieve. I work a paragraph at a time, but composing large chunks of documents. I’m still in charge of the overall process. Its a game changer. I can see that essentially the lower jobs of every position will be replaced including data science, and be directed by someone that is an expert in the field it someone that can at least figure out multiple fields in a small business environment. The world is changing, it’s exciting, but a lot of people will lose jobs. All the middle and lower range people truly.', ""comzet7: If the work load decreases by 90%, so would the number of data scientists employed. Isn't that concerning?"", 'Own-Necessary4974: You’ll also be expected to get more done', 'Living_Teaching9410: But would not that decrease the number of employed DS', 'MirrorBredda: What about Auto-ML mate ?', 'footballfanabc: DS hardly is being automated because implicit hypothesis is still inside human head', ""spudmix: Also a data scientist here, please listen to this.\n\nChatGPT is not a domain expert at... pretty much anything, really. Ask it some number theory questions and watch it fumble. Ask it why it chose a specific recommendation for a loss function and watch it be confidently and convincingly wrong.\n\nChatGPT is not a reliable source of truth and it is *certainly* not a reliable source of truth on its own capabilities. It's very powerful but it is ultimately a very good chatbot, not a PhD-in-everything sitting in your pocket."", 'DirectedAcyclicGraph: ChatGPT is like the office bullshitter. It would probably ace job interviews and then leave everyone wondering once it starts working why nothing it does actually really leads anywhere.', 'Helikaon242: It also doesn’t have the niche domain knowledge that a DS is expected to have. It doesn’t know how my product’s data works, it doesn’t know how it’s collected or what our business interests are. It could probably help me tune a model or get ideas to iterate, but it can’t tell me what my product objective is or interpret experimental data or any other kind of inferential thinking.', 'burralohit01: Right, yes this makes sense', 'Big_Pomegranate7314: I feel like you could have replaced chat gpt with “a lot of data scientists” and everything you said would remain true.', 'metamagikk: This is today’s ChatGPT. The analogy is to the first websites in the late 90s such as Yahoo and Altavista or the clunky first iPhone which had low battery life, a hard to use keyboard, slow Internet and ran one app at a time. \n\nAt the time, critics were quick to point out all its issues. Now - we stream everything all the time and aspects of the phone have changed culture. \n\nHow much of this bullshitting and inaccuracy in ChatGPT is gone in 10 years?', 'bijay_: It is just a start for GPT, more is to and will come in future. They would increase the data for training in different domain areas. It might not understand or do different tasks as of now but in upcoming 3-5 yrs it would be able overcome these flaws and do different difficult tasks.', 'burralohit01: What do you think? Can it though?', 'burralohit01: Idk can it?', 'burralohit01: Thank you Sid', 'WallyMetropolis: Why did we decide to say gaslighting instead of lying, suddenly?', 'Joeythreethumbs: No one realizes this, and it’s somewhat surprising that this many folks on a sub dedicated to data science don’t even understand what the major constraints of LLMs are. Sam Altman not only said that people are setting themselves up to be majorly disappointed with GPT-4, but there’s speculation GPT-4 might not even come out this year. Research in this area has more or less stalled out over the past three years, and the last mile work OpenAI and other companies are attempting to tackle are super non-trivial. \n\nWe’re nowhere close to the kind of AGI that would see a global reconfiguration of the workforce. As it has been said many, many, many times over the past few months, these models will just be a helpful assistant that needs major babysitting.', 'srmoure: I find it useful as a PA. I need to write an email and I just paste my ideas in chatgpt to get a great email in seconds. Also chatgpt is much nicer than me to request things to be done ;-) . I wouldn\'t trust it with generating insights, but it can certainly generate text to communicate the ones I provide.\n\n&#x200B;\n\n""As an executive, I find ChatGPT to be a valuable tool for composing emails. With just a few clicks, I can quickly and efficiently draft a professional message by simply pasting my thoughts into the chat. Furthermore, ChatGPT\'s polite language makes it a great choice for making requests on my behalf. While I don\'t trust it to generate insights, it excels at communicating the ideas I provide through its text generation capabilities.""', ""Voxmanns: I wouldn't be so quick to jump to that conclusion. I was able to mock up an entire website in under 2 hrs including learn how to set up a local server with apache. Granted, what I did wasn't exactly rocket science but if I didn't use chatgpt it would've easily taken 3x as long to do all that. \n\nI don't think it'll replace people as in fully automate a large set of tasks. More so, I think it will improve efficiency of existing roles enough that demand will be lowered and thus salaries and openings would go down."", 'burralohit01: Oh actually… I work in a service based it company. And since gpt came I automated my workflow using gpt. And I’m completing my day’s work in 10 mins. This is actually more than what you think it is.\n\n>>if your job can actually be done by a chatbot, you deserve to be replaceable...\n\nUsually if you’re a fresher you are assigned with these menial jobs and then you slowly climb up, if these bots are gonna replace us then there’s no way we can get into a job, cuz especially in tech ( only speaking from what I know) you can’t start from a higher hierarchical position from the beginning, please correct me if I’m wrong.', 'zazzersmel: whats scary is how so many people just seem ready to have machines replace them. maybe theyve been conditioned by industry rhetoric or science fiction, but you can tell so many people are willing to set aside any critical thought and just exclaim that ai is here and able to do whatever a human can.', 'NittyGrittyDiscutant: ChatGPT not, emerging standards for typical tasks and programs made according to these standards will.', 'SaintYeezy21: Yea I learned that the hard way 👀😢', 'PhlipPhillups: >if your job can actually be done by a \\_\\_\\_\\_\\_\\_, you deserve to be replaceable...\n\nHow many times do you think a sentence like this has been said before somebody got laid off?', 'burralohit01: Update:\n\nChatGPT cannot automate several important aspects of data science, including:\n\nDomain Expertise: ChatGPT has vast knowledge but it can not replace the domain knowledge and expertise of a human data scientist. A human data scientist must understand the specific business problem and the context of the data to make informed decisions.\n\nStrategic Thinking: ChatGPT can not provide strategic direction for data science projects. A human data scientist must understand the business goals, prioritize tasks, and make decisions about which models and techniques to use.\n\nEthical Considerations: ChatGPT can not make ethical decisions about data privacy and security. A human data scientist must understand the ethical implications of data science and ensure that data is collected and used in a responsible manner.\n\nCreativity: ChatGPT can not generate creative ideas for data science projects. A human data scientist must come up with new and innovative ideas for how to use data to solve business problems.\n\nCommunication and Interpretation: ChatGPT can not effectively communicate the results of data science projects to stakeholders. A human data scientist must be able to present results in an understandable way and interpret the results in the context of the business problem.', 'burralohit01: Yes I’ll update in the comments', 'burralohit01: Obviously but… placements…', 'burralohit01: Id like to see the gpt handle the hammer and chisel', ""josecitohp: That's what Jesus did"", 'burralohit01: Me too.', 'CrazyRage0225: Pretty wild a masters student is asking this question too. Obviously has never worked with real world data for an actual use case. Code is just a tool.', 'justin_reborn: Exactly', 'bbateman2011: I want your job. Not only do my stakeholders not communicate clear goals whatever it is they want changes at least weekly', ""Meaveready: Ah and writing detailed documentation, it's good at that (not like you will use all that as actual in-code documentation)"", 'burralohit01: Yeah but the workforce required will be significantly less', 'burralohit01: Yeah man', ""fabio_work: What do you mean? I'm new to the field. What part of an engineer's work have cloud services replaced?"", 'Meaveready: Are a lot of people really being recruited just to do stuff that you can get by asking GPT a bit?', 'midwestcsstudent: Nah', 'burralohit01: I wasn’t clear in my expression, I’m not saying it’ll eradicate us completely, I’m just saying there’ll be much lower job openings, so the competition’s gonna be tough. Survival of the smartest.', 'burralohit01: Yeah I’m not there yet but, I’m seriously trying to learn thoroughly. I was spending way too much time with gpt, just went into panic mode.', ""whispertoke: Also, I believe it's worth giving serious thought into how DS people can best leverage chat GPT. Better to work with the current than against it. For example, a teacher friend told me that after noticing students writing essays with ChatGPT their department had students specifically create essays using the tool and then critique those essays as their assignment, rather than try to prevent students from using it or trying to write it off as an ineffective tool, which clearly it's not."", ""paid__shill: It can assist you so long as you already know what you're doing and can identify the wrong information it gives for every 3rd answer 😂."", 'burralohit01: Yes, it can do the work of those 5 poor interns combined in 1 min. Now the interns are searching for another job.', 'burralohit01: Sorry, yeah I went into panic mode there', 'burralohit01: I am not….. yet', 'burralohit01: Sorry, this is the last one', 'burralohit01: Thanks', 'azur08: I keep seeing it’s a MSFT product but it isn’t yet, right? They’re just the exclusive provider of cloud infra and own the plurality of shares?', 'RationalDialog: > and if you need any semblance of data privacy and security you gotta go with gov tier account. \n\nCan you elaborate on that?', 'gardenjonhson: If you know how to use it, sure', 'JimothyC: Sorta, it means overall workload will drop which might suck. If 90% of currently done work disappears then less work hours will be required in some capacity.', ""Deto: It's a win if you're good.  Less annoying crap that takes up time means that fewer data scientists can do the same volume of work that was previously handled by more.  So then there are less data scientist positions.  That edges out the lower end of the pool from the job market.\n\nStill, though, I think the fears of chat GPT are a little overblown right now.  (though in a few decades, who knows)."", 'Nomorechildishshit: You would need 3 full time educated employees to write you friendlier emails, reports and help you with small scripts? These jobs would have to be the easiest ever', 'PresidentOfSerenland: Which sector are truly safe and what kind of new jobs do you expect to see in the future?', 'HaroldFlower: you should look up Eli Whitney and the invention of the cotton gin', 'oblivision: it also makes data science 90% cheaper, so there is more demand for it because the number of things it makes business sense to use it for increases.', ""eccentrus: or that small businesses can finally afford a data scientist, and data scientist will work like how accountants do, some in-house in large corpos, and some working as consultants for smaller to mid-firms.   \n\n\nMy parents have 2400 SKUs in their small business and it was practically impossible to properly digitize without considerable work by multiple highly-educated people, which is unaffordable, I would be able to do many things alone with chatGPT, and would be able to find work consulting similar sized but complicated businesses.   \n\n\nIt will broaden the reach of the field if you ask me, and that's better for the long run, we can't all try to work in the same firms inside the big cities."", 'ItsmeStp_: exactly', ""venustrapsflies: It's more like having Ken Jennings in your pocket. It has approximate knowledge of many things and doesn't have a good sense of when it's wrong or how to fix itself. That's certainly not useless but it's on the other end of the spectrum from a competent human."", 'YangYin-li: But it passes exams, doesn’t that mean it’s qualified for quite a lot?', 'strobelight: > Ask it why it chose a specific recommendation for a loss function and watch it be confidently and convincingly wrong.\n\nAfter working in several organizations in large companies, I expect ChatGPT to be promoted frequently.', 'Vagabondclast: A data scientist here,  just to play devils advocate,  why would business need us if they know and have domain expertise, and this ChatGPT is good at crunching data and giving insights? Recommendations can be managed by business themselves, right?', 'Renegade7559: RemindMe! 10 years', 'therealtiddlydump: Lol no', 'midwestcsstudent: It literally tells you at the end that it can’t, bro. If you’re so scared a machine can do your job better than you, maybe it can, though.', 'Lanky-Truck6409: A lie is a one-time thing and is a lie regardless of intention or whether you are caught. \nGaslighting is systemic lying or phrasing truths with the goal of convincing a specific person to believe something that is either false, or true but heavily eschewed.', 'RationalDialog: > We’re nowhere close to the kind of AGI that would see a global reconfiguration of the workforce.\n\nAnd the company that figures it out will milk it with so much greedy you will still be cheaper.\n\nEven in car factories not everything is done by robots. Heck I remember a German brand reducing robots and increasing humans labor as the robots simply could not be configured to properly deal with the gazillion configuration options so the had humans and robots which is too costly.', ""Extraltodeus: Yeah somebody in /r/ChatGPT got worried because he pasted his blood analysis in a new chat and the answer was worrisome.\n\nWhile I really don't like chat GPT filters, I feel like even then, they are too short and simple to let people know how limited that thing really is. A lot of people seem to take it for an actual intelligence."", 'AchingforBacon: Disagree. GitHub copilot has increased my data science team’s productivity by over 40%. ChatGPT is already increasing productivity. We had it write some scripts from scratch and just made edits where needed. My teams velocity is already increasing as a result of AI. To say it’s having nowhere the impact is not true', ""hockey3331: >More so, I think it will improve efficiency of existing roles enough that demand will be lowered and thus salaries and openings would go down.\n\nCould the barrier of entry become higher? Maybe, but isn't that constantly happening in every (more or less) field?\n\nLike, do you keep rewriting a linear regression from scratch for every model you want to fit? Or do you use solutions that are pre-packaged? Do you constantly write your own sort functions? Do you rewrite the max/min/sum functions? \n\nI'm generalising, but I think being more efficient and finding automation to solution frees our hands to attack even more complex problems. Which I guess involves constantly upskilling oneself, but I enjoy learning."", 'Dylan_TMB: This just describes how it makes jobs more efficient not replacing.', ""freedumz: Making a website is a redundant job, you apply the same recipe all the time \nIn the data world, it's a little bit different"", 'burralohit01: Yeah its essentially bringing down the search time in google or stack overflow for me', ""Meaveready: For that kind of very standard tasks, you could certainly find a step by step tutorial, and you would consume it the same way you do with GPT's output (minus prompting him for every step) so how would it have taken you 3 times more time instead?"", 'RationalDialog: And what was your prior knowledge? I doubt an average person without say CLI knowledge could have done it.\nAnd do I need chatgpt for this? you can get blog posts via google search that achieve the same thing.', 'mrbrambles: Yea, this might one day be seen as data scientists of the 20-teens/2020s pulling up the ladder behind them', 'Meaveready: If your job can be completed in 10min somehow, may it be ChatGPT or not, then your job was questionable to begin with', ""bigdatabro: No, it's called history. Most jobs from over a century ago have been automated by machines, and it's massively beneficial to humanity. I'm glad we automated harvesting wheat, assembling cars, sewing clothing and multiplying numbers. And in the tech space, us data engineers and other SWEs and DevOps have been automating workflows for decades.\n\nThe entire field of data science is automating the process of data analysis and prediction making. If you think humanity would be better off without tools like calculators or compilers, then you're free to go live on a commune in the woods."", 'renok_archnmy: ChatGPT has no knowledge…\n\nIt has massive weight matrices that contort inputs into output that resemble human language.', ""TheDrunkTiger: I bet we're less than 10 years away from CNC machines with some AI capabilities"", ""Not_invented-Here: And this is why chatgp won't really replace anyone."", 'kaisermax6020: Maybe, but other professions like developers, technical editors, marketeers and pretty much any other white collar job might have to deal with the same problem in the future.', 'renok_archnmy: Bruh…', 'burralohit01: Hardware engineers maybe', 'Tadpole_Southern: Gotcha. I agree with you, it will be interesting to see how that effects competition for entry level jobs. It’s going to be a pretty big shift in how we do things in the workplace. \n\nI think it starts with schools setting students up for success and teaching them how to properly use these tools. There will be some rebalancing to get all parties in-line with the new technical framework and it will not happen overnight. \n\nThe universities that teach how to use these tools will have better placement in the workforce and so on.', 'Insighteous: I know that but don’t worry. You’ll be fine as long as you never stop learning new things.', 'renok_archnmy: Microsoft wouldn’t spend that money if it didn’t intend to monetize it as their product. \n\nThey have a loooonnnnggggg history of buying up competition to establish a monopoly. They bought QDOS and rebranded it as MS-DOS. It’s literally their modus operandi.', 'renok_archnmy: Hyperbole \n\nhttps://learn.microsoft.com/en-us/azure/azure-government/compare-azure-government-global-azure', ""Zantipicta: Idk about gov tier account but data science work in defence will be safe. We aren't allowed to upload any semblance of confidential data to cloud platforms so for the far foreseeable future, unless a company hosts chat gpt on an internal server - which is highly unlikely in the next 10 to 15 years - this tool won't be permitted."", 'RationalDialog: I think 90% of skill is apparently using google search better than other people.', ""RationalDialog: That's the fallacy of assuming you are paid for your time. You are paid for your skill, and if that skills helps the company save 1 million 1 time per year for 1 hr of your work, you are already worth 200k."", 'burralohit01: So consider me 0 in data science. Where do I go from here? Even after knowing about the competition and all I still want to get in. Am I being foolish here?', 'Strict_Cantaloupe: Lol this is amazing. Analytical chemist runs an analytical company consisting of friendly emails, legal documents, and Python scripts. He does this for 20$/month and wipes out the work force aside from highly experienced individuals.\n\nI was going to say so much more but your comment speaks enough for itself already.', ""INtuitiveTJop: No, I would need a lawyer, or in terms of full time employment, would've cost at least 5k, a study director to write reports for my analytical service, and yes someone full time to write emails to customers and to my remote staff working with our parent company. Instead I did it in a much shorter time span with the help of generating large amounts of text."", 'AntiqueFigure6: AI research?', 'INtuitiveTJop: Every sector is safe, but only the top thirty percent of that sector in terms of people with experience and expertise. To get into that group the competition will be tough and keep things so that only the top people get in.', 'INtuitiveTJop: This got me down a rabbit hole. There goes three hours love ;)', 'robert_ritz: This is the real answer. So many things are NOT being done because data scientists are too expensive.', 'dragonofthewest0: Ken Jennings getting roasted over here', ""thegrandhedgehog: I'd hate to be Ken Jennings and stumble across this comment."", 'data_mystic: ken jennings is the perfect organic intelligence analogy of chat gpt', 'spudmix: **tl;dr Do not ask ChatGPT what it can do. It doesn\'t know. It will tell you what it thinks you want to hear and you have no way of knowing whether it\'s mistaken or not.**\n\nYes, ChatGPT is actually very ""knowledgeable"" about many things, in a manner of speaking. I don\'t mean to say it\'s stupid or lacks expertise - the critique is a little more nuanced. The fact that it does ""know"" so much is actually a significant risk factor IMO, because it leads us to be unscrupulous about it in general and we won\'t pick up when it gets shit wrong - especially when we don\'t know much about the subject ourselves.\n\nChatGPT is ultimately a conversational language model, and one of the weaknesses of this is that it\'s trained to offer what it thinks is an *appropriate* response moreso than a *correct* response. The danger lies in those areas where the model is incorrect but still offers a conversationally appropriate response; for example:\n\n    Me: Solve the following equation: 1/(sqrt(3)) + 10 + 9 + 100 + 20\n    \n    ChatGPT: The equation 1/(sqrt(3)) + 10 + 9 + 100 + 20 \n    can be solved by adding all the terms together:\n    \n    1/(sqrt(3)) + 10 + 9 + 100 + 20 = 140.52\n    \n    Note that the square root of 3 can be calculated to be approximately 1.732.\n\nThat answer is wrong (it\'s actually 139.58ish), but it\'s stated confidently anyway. A domain expert in mathematics wouldn\'t get that wrong, or at worst would be capable of recognising that they might be wrong and warning their interlocutor of the potential for inaccuracy. ChatGPT fills neither of these requirements.\n\nSide note if you\'re interested: this is probably a consequence of the structure of the transformer model wanting to run equations in parallel rather than sequentially -  it is very good at mathematics which can be run in parallel (e.g. ranking terms in ascending order) but relatively poor at those which require iterative computation and intermediate values, such as taking the sum of a series of terms.\n\nThis mathematical example is one area which is easy to reproduce, but there are other areas; the model will fervently tell you that it *can\'t* make jokes based on immutable characteristics, for example\n\n>No, I would not make a racial joke. As an AI language model developed by OpenAI, I am programmed to promote respectful and safe interactions, and making a joke based on someone\'s race goes against these principles.\n\nbut ChatGPT has been caught making jokes based on racial groups before at the expense of American Indians, if I remember correctly. People tend to take what it says about its own capabilities and limitations quite seriously, which is the same kind of issue - it is *appropriate* for ChatGPT to refuse to make racial jokes, but it is not strictly *correct* when the model refuses to do so, because it has occasionally broken this rule.\n\n\\[Edit\\] To relate this back to the conversation at hand, you can\'t just ask ChatGPT how it can help with data science and take its word for it. It might be correct, but it might also just be appropriately lying to you - you actually have to research the capabilities of the model to determine if it\'s any good at what it says it\'s good at.\n\n    Me: Are you good at adding lots of numbers together?\n    \n    ChatGPT: Yes, I am capable of adding large numbers together\n    accurately. As a language model developed by OpenAI, \n    I have been trained on a large dataset of text and have \n    learned mathematical concepts, including arithmetic \n    operations such as addition.\n    \n    If you provide me with the numbers you would like added, I would be happy to assist you with the calculation.\n\n**In the conversation above ChatGPT is** ***specifically*** **claiming that it\'s good at something which it is actually quite poor at.**', ""TheTackleZone: Exams use knowledge to test understanding as a way of exteapolating future utilit - which is generally a very poor way to go about testing understanding, but is a traditional method (and often easy to automate scoring by scanning for the presence of key words). I'd say the error lies more with the exam system than our analysis of ChatGPT."", 'TheTackleZone: There\'s still the application of domain knowledge. That\'s where the understanding comes in. ChatGPT ""knows"" how to write code. Business owners ""know"" the domain (sometimes). Neither understand how to connect the two together.', 'RemindMeBot: I will be messaging you in 10 years on [**2033-02-03 19:48:31 UTC**](http://www.wolframalpha.com/input/?i=2033-02-03%2019:48:31%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/datascience/comments/10rx6tv/what_else_is_left_should_i_continue_with_my/j73ckmr/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdatascience%2Fcomments%2F10rx6tv%2Fwhat_else_is_left_should_i_continue_with_my%2Fj73ckmr%2F%5D%0A%0ARemindMe%21%202033-02-03%2019%3A48%3A31%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2010rx6tv)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|', 'TheTackleZone: Gaslighting is specifically where you are trying to make people doubt themselves, not just believe something else - which is exactly what has happened to OP.', 'WallyMetropolis: That\'s not at all right. Lies don\'t have to be one-offs. And gaslighting is trying to make someone believe things that they can see, plain as day, are false.   \n\n\nAnd that\'s not what ""eschewed"" means either. To eschew is to forego or to avoid.', 'Joeythreethumbs: Actually, I doubt the government would even allow AGI to be employed by private industry, as it would cause mass economic chaos. Furthermore, while businesses would probably like to automate a lot of jobs, I think they’d also realize that would be a major mistake in the long run. The majority of Americas economic activity comes from consumer spending; if massive swaths of consumers are chronically unemployed, or constrained by UBI, that means those businesses wouldn’t be able to continue being increasingly profitable.', 'Joeythreethumbs: It’s confidently incorrect almost all the time. I think it’s good for broader, Wikipedia style summations, but it fumbled with specifics fairly regularly in my experience.', 'Joeythreethumbs: My last paragraph literally states what you’re saying: they will be major productivity boosters, not replacements for jobs.', ""_amas_: Shame it doesn't help with your reading comprehension, brother"", ""Voxmanns: Sorry, probably wasn't clear on my point. \n\nIf the jobs become significantly more efficient then it will effectively remove the amount of openings for that job and drive the salary for those jobs down. I think the concern of it replacing jobs is people losing their jobs, right? I'm saying I don't think it's to that level, but it's not something to write off as  non-threat either."", 'data_story_teller: Exactly. ChatGPT didn’t identify the need to create that website in the first place.', ""PhlipPhillups: If you have 5 manual laborers, and you hand one of them a jackhammer, that improved efficiency now means you have 1 manual laborer and 4 on unemployment.\n\nThose other 4 might also be provided their own jackhammers, but a company only does that if the improved efficiency leads to proportionally greater demand for their now cheaper product. Sometimes that's the case, sometimes that isn't the case. \n\nEven if the demand increases, but only such that 3 of the 4 keep their jobs, that's still a 20% decrease in workforce and a corresponding race to the bottom regarding wages."", 'renok_archnmy: I would never trust it’s output since it lacks the ability to cite its own references.', ""Voxmanns: Primarily because it can express changes to the code that is already written.\n\nFor example, I wanted at one point to make a portion of the page resizable by clicking and dragging the section of the screen. I didn't know how to accomplish this with HTML/JS/CSS so I asked ChatGPT and it wrote the example on top of the code I was already using and gave a brief description of what everything was so I could further tweak it to my liking. I didn't have to flip through multiple stackoverflow examples with a bunch of irrelevant code and extrapolations. I did that after the fact to better learn how to use it, but not necessary when I was throwing together the demo.\n\nThere was also a section where I needed to make a long bulleted list. I couldn't quickly think of a fast way to take the \\~60 lines and put them in an <li> tag so I just told ChatGPT to do it and copy/pasted.\n\nLastly, while it was writing stuff up I was free to do something else for the \\~30-60 seconds it was running. Like it wasn't really helpful on the apache set up. So while I had it writing a bit of code I was able to focus on something else and work in parallel or think about what I was going to do next."", ""Voxmanns: Decent prior knowledge. That's why I am confident I would have been able to do it in 6 hrs. My point wasn't that a newbie could do it in 2 hrs - they'd take longer. My point was that the accompaniment of ChatGPT was more efficient than the standard process of using google searches and programming it that way."", 'renok_archnmy: This phenomena is a constant fixture in tech and 100% of the resin for the massive canyon between people already working for a few years and everyone trying to get a foot in the door.', 'burralohit01: Yeah, it is, that’s why I’m looking to do masters in ds', 'zazzersmel: get a life', 'burralohit01: Almost there. I’m a mechanical engineer, I’ve seen some stuff, were almost there', 'renok_archnmy: Except that the framers are functionally illiterate and often have about 1/2” variance in their measures and cuts. Even the most well cnc’d stud pack ain’t gonna fit once it gets to the site and sits in the sun and rain for a few days before the house is dried in.', 'burralohit01: Yes, my concerns exactly. Its like a proper disruption. We knew it was coming but still didn’t anticipate this. Am I wrong in believing this as a moment of history being written?', 'sfsctc: What?', 'burralohit01: Yeah, as I said above in some other comment, we are witnessing a proper disruption, Like the ford kind.', 'azur08: None of that has any bearing on the current state of the product at all...which is what the question was about', 'random-incident: It would be interesting to see how much lift ChatGPT provides over someone who is a google search expert.', 'MrPezevenk: If something takes 10 times less time to be done, it takes 10 times fewer people working full time.', ""Deto: It's not that simple.  Maybe you make $1M/yr for the company.  However, they're still going to pay you as little as possible to keep you working.  If they pay you $200k/year for that effort and find that they can pay people $150k/year for the same output, they'll do that.  It's all based on how many other people have the skillset (supply) and how many jobs are available (demand)."", ""JimothyC: You get hired for your skills but managed by your hours. This type of thing may apply more to data analysts but regardless managers don't like people sitting around as they perceive it as being unproductive. This mindset is outdated but still prevalent."", ""mjs128: This is kind of a pointless conversation at this point, because ChatGPT can't really automate data science work yet.  But hypothetically if / when it can, salaries would depress because the pool of people who could do the work will be significantly higher, no supply / demand imbalance that has been driving DS salaries over the past 5-7 years"", ""Deto: I mean, you're a zero now, you won't be a zero later (unless you just don't apply yourself to your studies).\n\nPursue what you like.  Over time, eventually, AI is going to take jobs away from every discipline, not specifically data science."", 'recovering_physicist: >essentially write legal documents  \n...  \nI would need a lawyer\n\nOooooh boy your company are going to love you when this one blows up', ""venustrapsflies: I realize this came across as much harsher towards Ken than I intended, lol. He is a very smart guy and certainly has the ability to judge his own accuracy and know if he's wrong."", ""TheTackleZone: I'd hate to be ChatGPT being equated with Ken Jennings."", ""Vagabondclast: The business of today doesn't know it, but with every B school and even high school, for that matter, teaching the students about leveraging data and identifying patterns and all , how far are we to see the breed of business leaders who know how to connect these two?"", ""Lanky-Truck6409: Idk, I guess I am thinking of the difference compared to my own experience with actual gaslighting, where framing was done to the point of making me wonder if my sense of reality is wrong, instead of focusing on the actual statements and actions claimed by the other party.\n\nI see lying as someone saying they did something they did not do, for example, whereas gaslighring was me doubting my ability ro assess whether they truly didn't do it and whether I really did what I did."", 'INtuitiveTJop: But now you can employee two to do a job that three could do before. It does replace jobs', 'ohanse: If a team of 3+ChatGPT can now match the output of 10 without, how are you gonna justify keeping 7 extra headcount?', 'Dylan_TMB: This assumption is only true if the current rate/amount of software being developed is at the demands capacity. If demand is constant then more efficient development will lead to layoffs. In reality this is just going to lead to more software being made faster.', 'Dylan_TMB: Yea... That\'s what I\'m saying? In a sector with near infinite scalability and billions of people coming online do you think there isn\'t economic sense to give all 4 ""jackhammers"" even if company A only sees a value in giving 3 jackhammers there will be a new company B that can now enter the sector with 1 jackhammer🤨', 'None: [deleted]', 'khanraku: lol', 'Ok_Distance5305: And the “ford” moment only led to more high paying jobs.', ""tommy_chillfiger: I've been so glad and surprised as I've grown up that being good at google searches is so much rarer and more important than I ever considered lol. I spent a lot of time alone as a kid and desperately curious so as soon as I had a dialup connection it was ON. Now working in tech it blows my mind some of the questions people will go to managers/engineers to ask."", ""RationalDialog: You are still assuming you are getting paid per hour of work and no for your skill. You need to change your mindset. \n\nExtreme case go over to this thread:\nhttps://old.reddit.com/r/technology/comments/10s1nbf/reddit_staffers_who_lost_jobs_livid_at_being/j702ywo/\n\nBig tech recruits talent and just pays them a lot to do nothing as long as they don't work for the competition. That is the extreme case of what I'm trying to convey.\n\nInstead of cleaning data you will have more time to discuss and solve complex issues with your peers. So no the team will not be cut from 10 to 1 DS. that would be stupid. Heck the only reason you are doing the cleaning is because they know you are hired for your skill and can't come up with new cool stuff 24/7. it's enough for that to happen maybe even just once a year depending on impact. So in the mean time better to let you just clean data than hire more people to do it. And you will likley also do a much better job at it.\n\nYou are not paid for the hour, you are paid for your skill."", 'INtuitiveTJop: Everything gets checked by a lawyer, it takes him an hour at four hundred dollars an hour to go over my work. To write the board resolution, the employee handbook, and the pricing agreement he would’ve cost us a tremendous amount of money. Besides, we already have all the templates available for these documents. I’m not stupid and wouldn’t have gotten to my position if I was. But, I’m very quick to utilize something that makes things efficient and save money.', ""FourierEnvy: HAHAHA, don't tell him! Let him learn from his horrible, painful legal mistakes!!!"", ""thegrandhedgehog: No way, that was so good, don't retract it! Also the part implying he was at the other end of the spectrum from a competent human. Hilarious, even if unintended."", ""WallyMetropolis: I think that's a better way to define it. Because a lie can go on for years. Lies often spawn other lies. Think of criminal fraud, or having an affair."", '_Joab_: And this is a bad thing? When did data scientists become Luddites? More productivity means more resources for larger projects, new bottlenecks that require human workers.', 'Joeythreethumbs: Have you ever worked in data science? For every one question you answer, ten more get asked. This has been the story of human information gathering for as long as our species has existed. If anything, this will necessitate more data literate folks, as we’d need more folks who know what kinds of questions to ask, and when to call out these models on their bullshit.', ""_Joab_: I don't know how it is where you work, but at my company there are always projects put on the backburner for lack of people to tackle them."", 'Joeythreethumbs: That’s not how DS works. A more productive team just gets more questions thrown at it, as stakeholders get better insights and use those insights to test out new ideas, or ask more complicated questions. This will likely lead to more people being needed in the field. We’re not factory workers, where robots can do a single, well defined job to a degree that it puts thousands out of work.', 'RationalDialog: > In reality this is just going to lead to more software being made faster.\n\nexactly. the amount of stuff that should be made into software where I work is insane. but apparently still cheaper not to do so.', ""Voxmanns: I see your point. I'm just not so sure it'll be entirely realized that way in every company. There are plenty of companies looking to reduce their IT budgets - IE the demand capacity for that company is lower than that of the market. In those companies, I would expect lay offs to occur.\n\nThat doesn't necessarily mean the market demand is met. So those people laid off could still find a job elsewhere. Maybe that's safe enough to be considered a non-threat by some. Maybe that would create a surplus in some techs or industries. Again, I don't see this as making developers obsolete or anything. But I don't think it's something to write off as insignificant either. \n\nTime will tell though. This could be just another tech that has its flash in the spotlight and then gets seamlessly integrated into day-to-day with no hugely measurable net difference in the supply/demand of most roles."", ""PhlipPhillups: Still, that only happens if all of the other logistics of the company can keep pace (or if a new company can also provide all of the other logistics from scratch). \n\nIn the manual labor example, let's say it's home construction and the tool is excavators. There still has to be demand for new projects. What was the stopgap for undertaking new projects before? Maybe it's labor for digging out a basement, and now business will boom because of the excavator. But maybe the stopgap was a shortage of qualified roofers, or electricians, or plumbers, or the cost of lumber, or maybe the economic prospects of the city don't justify an increase in housing supply at all.\n\nIf any one of those factors is the stopgap, then the laborers will still get laid off because while the excavator does make the cost of completing projects cheaper, being able to dig out basements doesn't actually increase the rate of completed projects."", ""RationalDialog: This and managers need employers to fire in case things go south. they can't fire or sue ChatGPT so it will be themselves getting fired which isn't going to happen."", 'qtpnd: Unless it gives you an unsecured website full of xss,sql injections and other fun stuff.', ""MrPezevenk: You don't understand. Data scientists in that scenario aren't going to just start working for 30 minutes a day when they could be working more, and companies are not going to keep demanding that sort of workload any more. And people absolutely work on a per hour basis very often. But even if we ignore that case, if a single freelancer can now cover the needs of 10 times as many projects, they're not going to just sit there doing nothing instead. Which means that fewer of them will be needed to get the job done, which means fewer employment positions. Yes, maybe if every DS decided that they were going to only work for 1 hour a day then what you are saying might make sense but it won't be like that. \n\n>Instead of cleaning data you will have more time to discuss and solve complex issues with your peers. \n\nIF the employer needs that. Which they may not. \n\n>You are not paid for the hour, you are paid for your skill.\n\nPeople often say that but the reality is that on a large scale that's basically what it boils down to.\n\nOf course this doesn't mean necessarily that DS positions WILL decrease in that scenario, because the new technology could conceivably create other positions in other places. But your argument isn't right."", ""WanderlostNomad: doesn't this just wrongly assume other people won't take company offers to work based on a per/hour basis?\n\nif someone who can do your work just takes that offer and tries to make it up in bulk via multiple clients.\n\nit would break your imaginary picket line."", ""tommy_chillfiger: >Big tech recruits talent and just pays them a lot to do nothing as long as they don't work for the competition.\n\nIs this really happening? Bit of a tangent but that should be illegal imo. How is this not considered horrible for the economy to allow this kind of practice? It is directly compromising the efficiency of the market. People suck."", 'paid__shill: So you have templates available, and a lawyer to review them after. What is ChatGPT adding here? Filling the blanks with your company name?', 'INtuitiveTJop: Well, in the end we’ll see how it turns out, it could go any way', 'INtuitiveTJop: It’s all about whether there is plenty of money to employ people or whether management wants to save costs. I can’t imagine someone rubbing a business and employing people that they can’t justify on paper. If x amount of work is completed by y number of employees and it’s at turn pace expected then it’s justified, but to keep extra employees on to keep answering extra questions does not make business sense. Of course I see that you can answer more questions, but business people run things in the end.', 'ohanse: We have a bunch of ideas that get met with glassy eyed stares from cross functional colleagues more than stuff being put on the back burner and half my workload is appeasing leadership’s misguided ideas or crafting data to match pre existing assumptions.', ""Dylan_TMB: Yea but the demand for software and data analysis is very high. The stopgap has almost always been talent. That's what I'm saying. I haven't seen a place or had a friend that has been at a place that had enough people for the work."", 'renok_archnmy: Imports modules with malicious code too (assuming something more complex than html)', 'RationalDialog: Not sure I\'m following you. I mean you are supposed to solve complex problems which take a while to solve and you will have ""creative blockages"" at times. Solving complex problems doesn\'t need much solved problems say per year to make your salary worthwhile for the company. \nBut you can\'t obviously do grunt work while you should solve complex problems? \n\nyou can also name it doing research vs complex problems solving. Removing the data cleaning gives you more time for research, what you are actual paid for.', ""RationalDialog: Is it really that surprising that it happens? It makes a ton of sense to pay someone 500k a year just that he doesn't invent something for the competition that is worth 100 mio a year.\n\nThe morality of it is another topic but I doubt many would turn down working for 200k or 500k and have very little actual work. Albeit it depends. If you have to go to the office and twiddle thumbs it would suck."", ""mjs128: Not really, no. I'm sure it's happened in some every niche cases for specialized skills, but no FAANG company is hiring a run of the mill dev or data scientist under the assumption they aren't going to work for competition.    \n\nIt's more driven by empire building during an era where tech companies were printing money."", ""INtuitiveTJop: Essentially, I'm able too create these documents, use chatgpt to create some outline, I fill it with previous board decisions or it r rewrite them for a current situation and on terms of the employee handbook it was almost completely written work chatgpt with me directing what is required for each paragraph. Like everything ort is always a mixing and a matching game. The lawyer in turn checks after I'm done. The results were rather good and I did it without tremendous effort and research or relying on the lawyer too write it with expensive fees"", 'WanderlostNomad: depends. if 90% of the ""grunt"" work was done by an AI which leads to faster output. some overachieving datascientist can prove they can get more job done in less time.\n\nthey\'re not gonna faff around working on a single job for days, if they can do it in one.\n\nwhich opens up free time. \n\neventually they\'ll figure : i can earn more, if i use all that ""free time"" on other paid projects.', ""tommy_chillfiger: It's not surprising that someone would want to do that; the competitive advantage is obvious. I'm surprised that it's *allowed*. It was the morality, indeed, that I was speaking on. And the economic implications."", ""robert_ritz: Data scientists who are able to deliver excellent work in less will be just like current designers or app developers that can do that. They will be rare but will be paid significantly more for their time on a per hour basis.\n\nThat rarity ensures they can't out-compete everyone."", 'RationalDialog: > eventually they\'ll figure : i can earn more, if i use all that ""free time"" on other paid projects.\n\nOr you go to the gym, jogging or mountain biking or play with your kids. Hence why back to office sucks so bad for people paid for skill vs work hours.']"
1675371398.0,02-Feb-2023 12:56:38,language model,datascience,10s0mv9,Better interview questions?,CmdrAstroNaughty,28,https://www.reddit.com/r/datascience/comments/10s0mv9/better_interview_questions/,"I’m a hiring manager for a Data Science team. I’m looking for a Junior DS and my methodology is:

- I don’t do coding interviews: I trust you know how to code, if you really want to be in this field you’ll learn tools and languages on your own.

- I don’t do case study: I don’t really have the time for this, I want my “technical interviews” to be 15-30 mins.

- I want to know if you understand the basics of math/stats and if you can reason through unknowns. I want to see how you process through problems.

Lately I’ve been getting candidates who make it to my round and when I ask them some basic concepts, they completely flunk it.

So I’m wondering…are my questions too hard?

The questions:

1) You are given a tabular dataset that you have no prior knowledge about. Walk me though how you would profile this data? What steps would you take to explore this data? Explain to me your EDA process.

2) How would you evaluate a linear regression model? What are the metrics and what does it represent?

3) Explain the difference between Standardisation vs Normalisation.

4) Explain what a type I and type II error are.

These last 2 questions I don’t hold against them and which is why I ask last, and I let the candidates know it’s good to know but not required. It would set them apart.

5) Say you’ve got a ETL job that needs to be ran daily, say it’s a dataset that you need to pull from a server, do ETL, and upload it to a database or storage of sorts. How would you go about automating this task? What tools, methods would you use? The world is your oyster, any and all tools out there in the world are available.

6) Can you give me a use case where you have worked in Cloud (ie. AWS/Azure) to support your data science projects?

Edit: I have dialog when they answer questions or if they have questions for me. My style is, dialogue! Just chat with me.

Are my questions too hard? Can I ask these questions in a better way without getting too elaborate? 


Appreciate the feedback and help.",54,"[""SolverMax: Assuming people can code competently is brave. Very brave.\n\nIf you don't have time to assess coding or case studies, then get your team to do that part. They might appreciate being part of the process of recruiting competent peers, as they'll need to deal with the consequences of someone who isn't technically competent."", ""Moscow_Gordon: >I don’t do coding interviews: I trust you know how to code, if you really want to be in this field you’ll learn tools and languages on your own.\n\nIn my opinion this is your problem. A huge number of people who apply to data scientist positions can't code. I ask people to do a basic group by and to write a for loop and half of the people I interview can't do it. \n\nOnce you filter out the people who can't code, your other questions will be more useful."", 'PryomancerMTGA: Being able to set up and automate ELT tasks, experience with cloud,  ability to independently solve a modeling project, and a solid math/stats background... That is a lot to ask of a Jr DS.', ""Coco_Dirichlet: Questions 1-4 are ok for a junior position.\n\nI think the issue here is that questions 5-6 depends on experience. What if this person doesn't have cloud experience? What if they haven't automated a job? This is JUNIOR position, so (5)(6) are not things everyone has had a chance to do in practice. However, if they know how to code, which you aren't assessing, they can learn (6) in a day and they can learn the basics of cloud computing (at least what they'll need for the job) fairly quickly."", 'NewEcho2940: I just ask people to walk me through their projects. I’m much better at explaining a project I did then handling hypothetical question so I assume other people are the same.\n\nI don’t like hypothetical questions because they’re usually vague and they don’t have much in common with the actual job. I’m gonna need an hour of your time to ask detailed questions about the data before I decide on the “best” ETL method. How many files, rows, columns, data types, format, cadence? What file system or database do you use? How many users will access the data. How often. How often do they want it updated it. What format do the users need. How much storage and memory does your system have. How much is dedicated to this particular task. I could probably go on.', 'None: [deleted]', 'purplebrown_updown: You really can’t find Junior persons can answer these? Hmmm. I get the feeling like the market is flooded with “data scientists” who are code monkeys and not statistically literate. Might want to start looking for statisticians and people who have advanced math degrees and not cs or ds people. The former can always be trained to learn sql and pandas.', 'Ataru074: Walking thought your questions they seem easy. Undergrad leve easy. \n\nAnd you aren’t asking the harmonic mean (first!)', 'CmdrAstroNaughty: I got to say this post has a lot of downvotes…not sure why it’s getting a lot of negativity. \n\nGatekeepers 🤔🧐', 'rupertalderson: For questions 2-4, those are entirely fair, and any data scientist should have a coherent answers for those in an interview, even if their answers are not complete.\n\nFor question 1, however, there is a clear problem with the question. If any reasonable data scientist were given a tabular dataset with no prior knowledge about the dataset, the first thing they should do is ask questions to the domain expert who gathered or pre-processed the data. In theory, if an interviewee just jumps into their technical approach to profiling and exploring the data, then they’ve already skipped step one. Are you willing to give an actual example or accept half of the answer involving questions and hypotheticals to make the rest of the answer reasonable? You said you don’t like case studies because they take time, but asking for someone’s approach to data profiling and exploration without any context is not realistic or likely insightful, in my view. What I have identified is, in fact, one reason why a coding or case study part of an interview process is vital.\n\nQuestions 5 and 6 are all right.', 'tea_overflow: It takes me probably just one morning to crank out perfect or near perfect responses to all these questions, and I can’t even get an internship. I would be very very happy if these are questions I get from a manager screening', 'Fun_Satisfaction_789: What tools and languages do you expect a good candidate to know for this role?', ""1234okie1234: Read through your post and your response. Seems like you're/will be a great manager. Out of curiosity, not that it matters because we probably won't bump into each other, but what's your range for the position?"", 'darkly-dreamer: I am very curious about question 5 - this is outside of my current job scope but would love to know more about how to do something like this. If anyone has a response, I would gladly hear it!', 'Happy_Summer_2067: Your questions look fine to me, the problem is most likely the screening process. A good hire starts with your own HR. Take ownership of the JD if you haven’t already, talk to HR about the kind of people you want, involve your team like other people here suggested. Trace back the cases of bad candidates to see where they should have been filtered out, like how you would debug a call stack.', 'arena_one: Going through your messages.. I really like how you think and your responses. I’m a senior MLE with a PhD and 4yoe, are you guys hiring? :D', 'VitaminWheat: I’m pretty new to this area, if you have the time would you be able to give a quick run down on what you’d expect for 1&5?', 'Living_Teaching9410: Maybe as a first step, put a HackerRank coding test just to make sure those who make it to the interview round have good coding knowledge', 'saiko1993: You have the most reasonable requirements for any Ds hiring manager honestly.\nLike another comment mentioned you can ask HR to have a  hackerrank test to test basic  coding logic( in a language of their preference) amd I think this would be perfect.', 'eomar2828: I feel like I could answer those well. I like to see some code proficiency tho. I’ve been wanting to test a new method, based on an idea I got from the book Range. In it they talk about expert chess players or something quickly recognizing things (something like that it’s been like a wl2 years) . Anyway:\n\nHave some “moderately complex “ sql with joins and CTE and ask them to tell you what it’s doing. Maybe drop an obvious error and see if they catch it. Basically see if they understand written code, if they do then I assume they can figure out how to write it as well. Same could probably work for Python. Seems faster to me and their comfort & knowledge would be how I assess their skill.', ""honor-: I'm confused on the part where you say\n>I don’t do case study: I don’t really have the time for this\n\nbut then\n\n>I want my “technical interviews” to be 15-30 mins.\n\nHow are you supposed to judge someones technical skill in 15-30 mins? I think that's pretty ambitious. It might help to leverage other member of the team to devote more time to technical eval if you're feeling time crunched. Maybe you can serve as a final check and culture fit test at the end instead."", 'nyquant: 3. and 4. sound too much like memorization of terminology questions. I would try to put those into a scenario, like what kind of error measure would be important in a medical trial, a costumer churn prediction model etc', ""tomomcat: You're really not selecting for practical skills *at all* with this approach. I have seen so many people who would do really well in this kind of interview then be pretty useless on the job. Conversely, I've also seen quite a few who would probably flunk it but then go on to be incredibly productive - often people with english as a second language or without an academic maths/stats background, or who haven't recently done a bootcamp."", 'LilJonDoe: I don’t like those questions at all. These are quiz questions where you’re looking for a specific answer and it might not be relevant at all.\n\nInstead ask them to walk through a project they did, including the business context and impact, and then if things such as linear regression come up, ask questions about it if you care to.\n\nBasically try to figure out whether they understand why they did certain things, from both a business perspective and technical perspective, and judge accordingly.', 'CmdrAstroNaughty: Oh man really? I figured people can piece together things. Might not be production level code, but it’ll get you 75% there.\n\nI’ll start incorporating my DS and developers.', ""MrEloi: >If you don't have time to assess coding or case studies, then get your team to do that part. \n\nThat's what I used to do.  \nI would give the 'cultural interview' first and if the candidate looked like a good fit then I would pass them on for technical interviews."", 'ThePhoenixRisesAgain: There is zero need to test coding itself. If people get the concepts and have a good understanding of how to apply certain techniques to a business problem, they can always code it. Not true the other way around: I’ve seen good coders who couldn’t help solving any business related problem. That’s why we always test for general understanding of concepts and business applications. Coding is the easy part of our job.', ""BlackLotus8888: I think the most effective interview approach is:\n\n1) Recruiter who explains upcoming interview process. \n2) Data scientist/ hiring manager that asks the questions you asked above in a 15-30 minute interview\n3) Take home coding exercise (kaggle or hackerrank)\n4) review exercise with data scientist\n5) 15 minute flash round interviews with reach team member to determine fit\n\nIt's lengthy for the candidate but it leaves nothing to chance when selecting the best candidate."", 'CmdrAstroNaughty: I’ll look into adding my developers and DS into the interview to screen their coding ability.', 'purplebrown_updown: Yeah but a lot of that can be learned as well. But I get it. You want someone who’s job is currently using those tools and software.', 'CmdrAstroNaughty: It is for sure. Which is why I don’t hold it against them in my decision making. It’s also why I ask them questions last so it doesn’t make them feel pressured, compared to if I ask it earlier.\n\nI also tell them, it’s more of a nice to have, not necessary.', 'CmdrAstroNaughty: I like the second part of your answer. It is vague I totally agree, and there are many paths you could take. I like where you headed honestly though its very well though out, and I haven\'t received those answers from my candidate. \n\nHave you found that, when you ask candidates to walk you through their projects they\'re sort of just copying another persons work, like the code and workflow just seems too perfect...almost staged and scripted? --- quiet honestly i don\'t know how to word this hesitation ---\n\nWhat I mean by this is for example:\n\nThe Kaggle Titanic dataset/work - a lot of it is well documented, there\'s plenty of guides out there for the code and explanation. I\'ve seen a lot of Titanic Kaggle notebooks, and whats to say someone isn\'t just ""copying"" another persons work and changing some values, some hyper-parameters and calling it their own work.\n\n&#x200B;\n\nI\'ll definitely look into it, I did have one candidate that provided his GitHub and I was able to look through it and now I wish I asked him to walk through one of those projects with me.', ""CmdrAstroNaughty: HR finds these candidates. The Business Manager interviews them first, then gets brought to me, and now that I'm typing this out...I'm thinking hold on that's stupid I should be able to screen these guys before the BM even interviews them.\n\n(Time to implement that change lol)\n\nI've had some candidates come through with a certification from DataCamp or Udemy. I'm wondering do these courses not teach basic stats and math? I'd like to audit one honestly just so I can see what is taught in those MOOCS.\n\nI teach an undergraudate course, at the 2000 level. And I am confident that my students can answer questions 1-4 similar to you."", 'CmdrAstroNaughty: A 10+ week DS Bootcamp level easy?', ""honor-: serious question: is harmonic mean a joke in DS circles? I've only ever used it in context of the F1-score"", 'CmdrAstroNaughty: Dude, what you gave for answer 2 is literally what I want for an answer. I want these candidates to say, well let’s pause here for a second…where is this data coming from? Who creates this data? What does it inherently mean apart from the values in the dataset?\n\nThere’s an aspect of EDA that is more than just “math” ie. giving me a distribution and what values are null…blah blah blah.\n\n\nIs there a better way I can frame that question to get that kind of answer?', 'CmdrAstroNaughty: Bummer. I see you’re doing a PhD. I know there are orgs that are hesitant in hiring individuals that are too “academic”. Makes no sense to me, don’t ask me why they do this.\n\nI sadly do not have an internship program, I REALLY want one because I love developing students and giving them exposure to the field and experience. In the past I’ve always treated my interns as full-time people, with all the rights and responsibilities (obviously with a mentor they can ask questions). The feedbacks have always been amazing….\n\nDamn budget reasons :(', 'CmdrAstroNaughty: I could care less if you want to program in R/Python/C#/Julia. My team has research people who script in R for stats and production people who script in Python to put models in production. We’ll put you wherever you want, and if you’re willing to learn, cool!\n\nTool wise, shit man no need for PyTorch or Tensorflow…yet lol. Scikit-learn at the entry level is good enough for me. Or any basic ML library. If you want to work in Pandas cool, if you want to work in exclusively Numpy…cool too. \n\nShow me you have some basic understanding, the team will give you opportunity to learn and hone your skills. We can’t teach you from ground floor but we can meet you halfway.', 'CmdrAstroNaughty: It’s a wide range, it was set by HR not me…85k-130k USD.', 'CmdrAstroNaughty: Some of the answers I’m looking for:\n\n1) Just write a Python script. And if that job needs to be daily executed at 4:00am…you bet I’m going to ask you to wake up at 4:00am and run that script.\n\n2) Use that same Python script and just attach it to a CRON job\n\n3) Use a cloud instance or function to trigger the script job on a time event basis.\n\n4) …use native OS built in scheduler (intuitively they are just CRON jobs with a GUI)', 'CmdrAstroNaughty: Can you answer those 4 questions? Lol I’m just joking.\n\nIf we have a MLE that leaves the team I can keep you in mind. But honestly the world needs more MLE and MLOps people. I’ve noticed my DS guys being outpaced by just the sheer volume of AutoML tools out there that can do their jobs, a bit quicker. Less accurate generally but still non the less quicker.\n\nYou in MLE have a super bright future ahead of you.', 'proof_required: I agree with testing basic coding skill. I have had DS managers who couldn\'t write a python script to save their life. It just doesn\'t look good especially when the same manager would go around touting how this or that is ""easy"" and make promises to higher level stakeholder.\n\nJust ask the usual fizzbuzz stuff or just sum of elements in a list of numbers or calculate mean etc. It shouldn\'t be implementing some binary tree search etc. I will also add some simple SQL testing questions. Again nothing complicated but just group by etc. I am also explicit about the fact that we haven\'t hidden any trick under simpler question. It is exactly how it looks like.', ""FrostTrapsRGhey: Honestly this is a huge problem with the tech sector as a whole right now. Hiring in incompetence that talks a good game has seen a huge number of people unqualified for their roles get in. The best way for you to find the best candidate is to learn some of the skills and jargon yourself so you can get a better read on people's abilities first."", ""Moscow_Gordon: I mean yeah anything can be learned to an extent. But if a person can't figure out how to do a group by the problem is most likely that they just don't know how to work with data."", 'NewEcho2940: Juniors don’t have the confidence to reject the premise of a vague question. They’re too worried about finding the “right answer”. I don’t review code. Coding is the easy part. Thinking is the hard part.\n\nI assume every college grad did at least one project to graduate. So they usually mention that. Lets say part one of the project included web scraping. I would ask how did you web scrap? Why did you decide to do it that way?\n\nIf they stole code they didn’t understand they wouldn’t be able to answer that. If they stole code and worked to understand it, that’s fine. I do that all the time.', 'graphicteadatasci: Does the business manager have anything to gain from the position not being filled? If you used to be able to find good candidates and that has changed then maybe there\'s nothing wrong with your questions. The salary looked pretty sweet for a junior position and maybe some people disagree with that?\n\nI would add some very simple code questions, based on the languages mentioned in their resumes. One with ""this code is supposed to output this but outputs this - can you find and explain the problem?"" and ""can you try to explain what this code does?"". Partially to weed out anyone with no clue but mostly to have some very concrete things to talk about and get the nerds warmed up. Your questions are generally open-ended and it takes a bit of time to get in the right headspace.', 'Ataru074: I guess it depends on the boot camp I guess. \n\nOut of 1,2,3,4 the only one which might get someone is IMHO the first. There are so many things you can do wrong on the data… as wrong imputation methods for NA, wrong way to handle categoricals, likert scales, and so on. \nOutliers is a big one as well. \n\n2,3,4 are basic, I did teach them to undergrad in business… now. Would they remember or have an idea to refresh? Different story. So it’s good to check.   If you ask about type 1 and 2 error is nice to follow up with the confusion matrix. \nELI5 precision, accuracy, and recall and if you want to push it a little, when would you use one or the other metric. \n\n5 gives some idea of previous experience as well 6. \n\nI don’t like asking coding questions as well, but some easy SQL comes handy.', 'rupertalderson: I think it’s phrased well. I was checking that you want that and, if not, I wanted to be sure you are reasonable about it!', 'Fun_Satisfaction_789: Thank you for sharing! I appreciate your open mindedness', 'CmdrAstroNaughty: I also want to say No 1. is a bad way of doing the job. You should always try to automate...', 'arena_one: Thank you for replying! Regarding your questions I must say.. as an MLE the 5 is not as easy as one might think. And finally, I hope someone replies to 4 with the meme where type I is a doctor saying to a man you are pregnant, and type ii a doctor telling a very pregnant woman that she is not pregnant Lol\n\nThank you for your kind words! Actually sometimes being an MLE is hard because you are expected to have all the stats/ml of a data scientist, all the engineering of a SWE, and the pipeline knowledge of a DE..\n\nBut at least I’m getting some calls, which given this job market is probably an achievement already!', 'CmdrAstroNaughty: I love everything you just said. I’ll take it into consideration and implement! Thank you', ""CmdrAstroNaughty: Lol I meant the bootcamp as a joke...sorry text doesn't translate sarcasm well.\n\nI do see what you mean with 1-4...they're meant to be a conversation starter (and I hope it does that). More of open ended questions so the candidate and I can have a discussion. \n\nHonestly when I ask the type 1 type 2 question...I'm just waiting for a candidate to say Confusion Matrix...that to me would be a perfectly acceptable answer in a very condense form. It has yet to happen."", ""CmdrAstroNaughty: Of course, good luck if you aren't in the field yet, and are trying to break into it. \n\nIf you are in the field, keep on learning!"", 'Ataru074: My GF did a post grad certificate in analytics and now she’s half the way in her master and she could have got all your questions (potentially) with that 6 months cert.']"
1675188424.0,31-Jan-2023 10:07:04,language model,datascience,10q5ydf,Classification model correction for interfering classes,argminUsername,1,https://www.reddit.com/r/datascience/comments/10q5ydf/classification_model_correction_for_interfering/,"Hello everyone, 

i'm training a multi-class classification model  and I would like to ask the following question:

Assuming the model should differentiate between 10 variations (A to J), if A and B are similar (or whatever letters), then how can I somehow give a bias to the model to take that into consideration ? Currently, I am doing a ""hot zone"" in the confusion chart, so that the user knows to take these results with a grain of salt. 

Any hints would be appreciated!

P.S: programming language doesn't matter even a math wikipedia page would be nice!",0,[]
1675180725.0,31-Jan-2023 07:58:45,language model,datascience,10q2nb5,Career progression for fledgling Data Scientists,AloneBreak4574,1,https://www.reddit.com/r/datascience/comments/10q2nb5/career_progression_for_fledgling_data_scientists/,"Hi, I'm hoping someone with more experience can give me some advice on my DS career, which I'm at a bit of a crossroads with. 

For some background - I have a PhD during which I learned about data analysis, visualisation, statistics, and R. Since receiving my PhD I've had two analyst roles where I've found my knowledge of R and statistics have left me feeling a little overqualified/not having good enough data to do 'proper' analysis work. 

Last year I started an entry-level DS position. I got the job because I have domain expertise and good proficiency in R, which is the language of choice where I work. The job is well paid, comfortable (I'm able to WFH) and I find the work mostly interesting. 

My issue is this: I still feel like a glorified data analyst. There aren't really opportunities for ML (the thing that - possibly incorrectly - I most associate with DS), and I feel like experience with ML in a workplace setting is needed for possible next steps in my career. At the same time, I enjoy analysis and I'm not sure I'd find designing and tuning models full time particularly satisfying. 

I suppose my question is what direction can I go in/skills can I learn to continue to develop my analysis skills, and what types of senior roles support this. It feels from job adverts that Senior/Lead Data Scientists are primarily concerned with pipelines and models etc, rather than working with stakeholders. Interested to hear other people's experiences if any of this sounds familiar to anyone.",3,"[""BCBCC: I'm in a vaguely similar position. I think there will eventually be a separation between data scientists as analysts and data scientists as engineers. Like me, it sounds like you're a DS analyst and aren't interested in being an engineer. There are DS jobs out there that want your skills, they're just a little harder to find with all of the SWE job postings that are called DS.\n\nIf you're enjoying the work you do, and it pays well enough, then don't stress yourself about a non-existent problem. Grow in your role, let the people in your company learn the value of a DS doing analysis and working with stakeholders, and if the right opportunity somewhere else comes along you'll be ready for it."", 'None: Many larger companies are now distinguishing between ""Data Scientists"" and ""ML Engineers."" Obviously the line between them is quite blurry, but it may help to think about them in terms of the work outcome:\n\n* Data Scientists use data to drive business decisions. Junior DS may use data to help inform product features (my A/B test suggests that we should move the checkout button to the top of the page). Senior DS will use data to direct overall business strategy (we should build a product to fulfill this unmet customer need). DS may or may not use ML, depending on whether or not it\'s the right tool for the job (sometimes it is, very often it is not).\n* ML Engineers build productized ML models. They concern themselves with things like feature engineering and loss functions. A ML engineer may produce things like an image classifier or a recommender system.', 'data_story_teller: I would try to get a job at a company that does do ML. If you can’t land a job doing ML, try to get a job doing data analysis and start networking with the ML folks and see if you can shadow them or work on projects together. Figure out what skills you’re lacking and focus on learning those, then apply for an internal transfer when a role opens up.']"
1674348298.0,21-Jan-2023 16:44:58,language model,datascience,10i6kg4,Large Language Model,ashishtele,0,https://www.reddit.com/r/datascience/comments/10i6kg4/large_language_model/,"Hi,

I am looking for a course to learn the Large Language Model. Please provide some sources.

\#LLM #ChatGPT",1,"['acewhenifacethedbase: Learn what about them? How to build them? How to use them? What they are? ChatGPT has been public for like 2 months tops, there won’t be many formal courses in it yet. Best to read what its creator OpenAI has to say about it']"
1675077970.0,30-Jan-2023 03:26:10,language model,datascience,10oznkj,Python or R for simple Data Analysis and Visualization for a Bachelor Thesis,Ja_Bo,0,https://www.reddit.com/r/datascience/comments/10oznkj/python_or_r_for_simple_data_analysis_and/," I'm currently doing my Bachelorthesis about Microclimatic Effects of Green Roofs and I model the clima for 3 days with ENVI-met.

I will get lots of data which I have to generalize and analyse and then visualize. Basically I want to make some statistical tests if there are significant differences between different fractions of Roof Greening (Compare 100% Greening to 75%...).

I'm asking myself which language would be the most easy for this. What I found out is, that visualization in R can be more ""sexy"". On the other side I have no experience with R and a tiny little bit with Python and I don't have much time for learning.

I already plotted one graph with Python and I know how to load csv's and can work a bit with pandas. I don't know how easy it will be to do the statistical tests with Python or if doing the tests with R would be more straightforward.

I'm happy if someone could tell me what approach would be better: Combining Python and R or just using Python because I already somehow understand how the language works.",5,"['None: If you were completely not familiar with Python, then R would be a bit more convenient for the purpose. In your particular case, almost no difference except for R ggplot2 visuals are sexier.', ""nebkla: Personally I'm a fan of R more for data analysis and visualization. Tidyverse and ggplot2 are the go to packages. I'll admit that the syntaxes are bit weird at first but once they click it becomes so easy to do what you want. Ggplot has some great tools for customizing charts and easily adding things like callouts right into the plot area."", 'zanderman12: Check out the pingouin python package for the statistics and seaborn for graphing.\n\nOverall I don’t think one language is better than the other in your case but since you started with python and pandas those 2 packages play nice with pandas', ""Ja_Bo: Yeah I also think thats probably true. Maybe I will have a look in R Syntax and how fast I could learn it, but it probably just doesn't make that much sense."", 'Ja_Bo: thank you, i will have a look in the packages :)']"
1675085333.0,30-Jan-2023 05:28:53,language model,datascience,10p2321,Local Natural Language Processing,ikbal_gambit,1,https://www.reddit.com/r/datascience/comments/10p2321/local_natural_language_processing/,"Hi dear community, 

I'm a big data analytics student. 

working on my end of studies project, it's about sentiment analysis on Facebook posts (Comments), comments usually are written in a local language ( Algerian Dialect) 

I want to know if there are some technics I should be using to train ML models on this dataset . 

Your help would mean a lot. 

I'm also open for any questions regarding this topic , let's exchange on this one .",0,[]
1676816081.0,19-Feb-2023 06:14:41,image model,datascience,116bd5x,Do you want an easy and quick way to explain your image models?,Nice-Tomorrow2926,0,https://www.reddit.com/r/datascience/comments/116bd5x/do_you_want_an_easy_and_quick_way_to_explain_your/," Through the easy-explain package, you can achieve it without the need to write long scripts (only in 2-3 lines of code you can have your XAI results).

Read more info in this article: [https://medium.com/towards-artificial-intelligence/easy-explain-explainable-ai-for-images-285777a004e3](https://medium.com/towards-artificial-intelligence/easy-explain-explainable-ai-for-images-285777a004e3)

Find the package in Gh: [https://github.com/stavrostheocharis/easy\_explain](https://github.com/stavrostheocharis/easy_explain)

Find the package in Pypi: [https://pypi.org/project/easy-explain/](https://pypi.org/project/easy-explain/)

&#x200B;

https://preview.redd.it/cowm1xkkl5ja1.png?width=1390&format=png&auto=webp&v=enabled&s=b0a259ebe938838e580437e92641ae23182f972d",0,[]
1676880002.0,20-Feb-2023 00:00:02,image model,datascience,11714te,Is it normal in the data Science field to not have that many code checks?,skeletons_of_closet,30,https://www.reddit.com/r/datascience/comments/11714te/is_it_normal_in_the_data_science_field_to_not/,"So Basically I have worked on a data science project with a professor in Cannada and we got good results in a medical dataset and got a good AUROC of .85 and we published the paper for it and got selected , but what surprised me was the professor never went through my code and just gave suggestions and tips to do improve the model , but isn't this a lot risky.Everyday I am scared that some person would go through my code and invalidate my entire results based on some simple error which could have been corrected if there were proper code reviews.

Fast forward 6 months , I got a job in  a data science company as an intern and he told me to develop self supervised model for their image dataset and I did that and it good kind of okay results and he told me to move on to another project , still there was no code reviews or code checks , do people in data science just blindly trust each others code , I feel managers should at least give a look through to see if we taking the correct data split or if the model is correct or if there is any data leakage.There is a lot of red flags in trusting the AUROC results blindly. Is this a norm or maybe its just for me.

Moreover most of the data scientists in the company dont even write proper documentation and for interns like me its such a pain , it took  me 2 weeks to understand their entire training repo, data science people should talk with the data engineers and take some of their methods and practices.",39,"[""SolverMax: This is entirely normal in all of the analytical teams that I've worked in/with.\n\nAnalysts focus on getting results. Good programming practices, like testing and validation, are neither implemented or encouraged.\n\nA direct consequence is that many results are just plain wrong."", 'synthphreak: I think what you described is pretty normal, and as an MLE, I also hate it: Lack of documentation makes onboarding and orientation a nightmare. Lack of tests makes reproducibility harder and slows down refactoring. Almost every experiment seems to have at least one false start due to some silly bug.\n\nBut as I have gained experience, I’ve started to at least understand why things are this way, if not approve of it.\n\nA part is that I think lots of people in the analytics space never studied CS, so the importance of automation and good coding practices was never really instilled in them. Then again, I also never studied CS nor SWE, and I definitely see the value in its principles. So 🤷\n\nBut the biggest factor IMHO is that for all the benefits of documentation, tests, tidy code, etc., that stuff does introduce significant up front costs which slow you down at first. In the world of experiments and exploratory analysis, you have lots of little one-off needs and must iterate quickly through the space of possibilities in order to identify the promising ones. So while it’s a bummer for the uninitiated to get acclimated to messy code, in the end it’s usually just faster to make small tweaks and roll with it, rather than tweak, document, push, PR, etc.\n\nThe other thing to consider is that things like thorough documentation and testing are most critical when your code base will definitely persist and continue to undergo development for a while. But in the worlds of data science and machine learning, many lines of research - most with their own code bases - ultimately never bear fruit, so get dropped. For such work strands, any time spent documenting and testing was 100% wasted. So when you’re in that initial experimentation phase on the road to deciding whether to pivot or persist, the cost-benefit of taking time to write docs and tests is probably not worth it.\n\nSo yeah, in conclusion, I do think everybody’s life would be better and fewer false results would occur if everyone just adopted proper coding discipline. But doing so is not without drawbacks in the world of exploratory research, mostly in terms of productivity. So as always, it’s just a trade-off.', 'chock-a-block: At least on the commercial side, this is completely normal.\n\nI’ve sat in on meetings and watched others absolutely make up results because graphs… But, the code will never be reviewed or understood by the person making the decision.\n\nTo answer the obvious question: yes I was out of there soon after, as was the rest of my team..', 'Allmyownviews1: I was thinking the same years ago.. too this day no one has asked to review my code.. they (if they really wanted) would review the findings and assumptions, perhaps perform their own basic data inspection for any concern, and that’s it.', ""IpsoCranius: This is why the FDA is starting to classify ML products as clinical devices.\n\nI've seen obvious errors in the past week people write custom prediction functions and apply results to nondeterministically sorted lists as they predict bad outcomes. Among other things.\n\nCode review. Do it."", 'Randomramman: High-performing teams have detailed code reviews. This is the bare minimum. Even if they have some excuses for why they don’t have documentation or testing in place (which will almost all be bad excuses imo), you _must_ review each other’s code.\n\nMaybe I’ve been lucky with the teams I’ve worked on so far, but I’m alarmed by the number of people in here saying they have no code review practice on their team. I can see this being more common for analytics/SQL work (not that I agree with it), but it would be insane to ship an ML model without review.\n\nNo review/testing/etc is absolutely the norm in academia in my experience. The open code/data movement is helping here, but most fields have a long way to go. At least in academia you have many groups trying to study the same things, so if your paper makes a claim but the code had a catastrophic bug, it will likely be caught eventually if it doesn’t make sense in the context of everyone else’s work. That’s my take from my experience in physics anyway. YMMV.', ""slackington: What you're describing has been my experience too, but things are starting to change. \n\nIn analytics there is a shift towards writing production grade SQL to centralize KPI definition and optimize performance. \n\nIn data science / ML a lot of companies are moving towards data products with clear SLA.\n\nBoth mean code needs to be version controlled, peer reviewed, documented, and adhere to company standards."", 'IcyMammoth: Weird, my teams do PR reviews; any code/analysis gets reviewed and approved before we share results with stakeholders', ""eipi-10: what you're describing has been my experience too. at my last job: absolutely no code review whatsoever. most code wasn't even checked into git. I also have friends in academia / phd programs in stats that have no code review and their code is terribly written.\n\nat my current job we make a big effort to do code review, write tests, etc. it takes time, but it makes us all a lot more confident that what we're doing is actually right, and makes it easier to make changes to code in the future without worrying about breaking things.\n\nhonestly, it's insane to me that data scientists aren't writing tests. I would never ship something to production that hadn't been tested"", 'Fun-Angle7673: If you work for a strong team there will be peer review of analytical code.  I have worked in teams with and without code review and in the team without code review I partnered with another strong data scientist to review our work... You can create the good practices in your teams.', ""MrPatko0770: It's because DS is the field where a computer scientist goes when they can't be bothered to to proper software engineering and code testing because it's boring\n\n/s, but only partially because the real answer is a bit more nuanced, but it's also kinda true"", 'BlaseRaptor544: It depends on the company. In my DS rotation, they had code reviews, esp before a model was given to ML engineers for production', ""milkteaoppa: Yup, both academia and industry. Shouldn't be that way, but as long as errors aren't caught, then it doesn't matter they exist."", ""justAneedlessBOI: Yea that bothers me as well. I think it might be because the field is close to statistics and analytics and not just programming, so people just focus on preparing the report and don't care if it'll work later. But yea, some time ago I noticed that a dataset prepared by a colleague duplicated every record above a certain value, so there were like 1.5 as many records, but nobody looks at these things"", ""Series_G: Yeah, I see this all the time. Practices that should be standard on more of a data engineering track (like code reviews) aren't even considered for the analytics side of the house. It is....worrisome."", 'Artistic-Breadfruit9: Off-topic, but an AUROC of 0.85 isn’t “good” if the data are skewed in anything close to an 85:15 ratio. A simple guess would do that.\n\nThis is why ROC curves aren’t all that meaningful if the prior probability is significant different than 0.5. What you really need is AUPRC, where the baseline is equal to the frequency of the predicted event.', ""autumnotter: Yes, this is normal. It's more normal in academia where software engineering best practices are rarely discussion. No, this is not ideal. \n\nHalf my job is teaching data engineering and data science teams how to use Git, test, write documentation, collaborate, and deploy. I'm brought in to do much more advanced things, but then that ends up being the places they are nearly always deficient.\n\nYou should have simple code reviews, a basic version/artifact control workflow, testing, and documentation. It will be challenging convincing others to use good practices, but learning them will take you far."", 'AdditionalSpite7464: Yep.', 'protonpusher: It’s easier to publish/produce rubbish and put the burden on someone else than to verify your own work.\n\nIf the error is ever discovered, the person producing the crap has moved on, having acquired the false accolades, and will not be held responsible.\n\nThis is the norm in academia and data science and why there’s such a proliferation of frauds.\n\nWhy waste the time to potentially prove your model wrong when your teammate/competitor is a fraud and just turns in garbage positive results in half the time.', ""dont_you_love_me: In the funding and corporate world, they don't want to know what your code is. They just want the results to look good so they can cash in on it. You will suffer far more negative consequences by producing correct code that generates results that look bad for the organization. They would rather you mess up and make them look good."", 'skeletons_of_closet: >do you think its because the data science field is yet to incorporate proper industry standard practices that engineers follow?', 'skeletons_of_closet: Thanks for the detailed response , yeah we have really great data scientists in team who come up with amazing solutions but after the work is done , they are too lazy to go back document the code and just move on to something else and the interns or other members are made to pick up the pieces and make it production level or worse they send it to data engineers and they would scream looking at the code.\nMaybe a little effort to document a bit goes a long way for others but I understand the trade offs u mentioned and one needs to balance that while writing code.', ""jobeta: I 100% agree with this: essentially, in DS, writing tests before your model is going to be in prod is likely to end up being very wasteful. But as soon as it does hit prod, you should write tests or expose yourself to the same issues as with any production system. Psychologically, it sucks because you've shared your results, everyone is hopefully excited and you have to tell them to slow down so you can write tests. Engineering teams will look at you like you're crazy, and non-technical teams who feed off fresh hot results to stay motivated to even care about your relationship will be bummed. It sucks. So I guess one should always try to set expectations with the added time to write these tests. But not reviewing the code seems almost always crazy...\n\nHowever, OP talks about peer-reviewed articles. I had the same experience, you can publish tons of results relying on code that no one ever reviews. Even the journal \\*\\*\\*reviewers\\*\\*\\*, which quite frankly is a bit crazy. Even outside of the ML field, they often incentivize for you to share your code, but will not actually review it. If the code is accessible, at least it's out there for everyone to see and review when they need it? But if your code is not properly tested, it is likely that it will break constantly and no one will end up using  it anyways."", 'skeletons_of_closet: damn.. thanks , never knew this was the norm,', 'skeletons_of_closet: Yeah , this is how I feel going through their poor code : https://abstrusegoose.com/strips/you\\_down\\_wit\\_OPC-yeah\\_you\\_know\\_me.png', 'synthphreak: SLA?', 'skeletons_of_closet: Yeah in the production level there is some kind of code checking since it involves both engineers and scientists but converting to production level code from the shitty un documented analysis code repo is a pain', 'skeletons_of_closet: Yeah such mistakes are what scares me when I write code lmao', ""skeletons_of_closet: Ohh so the project I had worked on had previous AUROC of 0.78 so it was an improvement on the test set.\nyeah we had shown PRC curves but it wasn't that great so my prof moved it to supplementary lol"", 'SolverMax: I think it is because few analysts have studied computer science. Writing code is a means to an end, having been self taught with little or no knowledge of the lessons learnt by professional programmers over several decades of experience.', ""TheNoobtologist: I would say so. It's also been my experience that data science seldom has code checks. A lot of times the teams are small and the managers might not be technically enough to support code reviews."", 'synthphreak: A part of it is also probably a “not my problem” mentality.\n\nMost organizations have a Human Centipede structure with each job family forced to eat somebody else’s shit. In your org, I guess the DEs eat the DS’ shit. And in all likelihood, the DS’ probably have to eat the shit of the business folks, in the form of “let me barf some half baked, poorly articulated idea into your ears, now go monetize it with the *magic* of data science”.', 'chock-a-block: Ohhh.  Yeah.  For sure. It’s changed a little, but, not a whole lot. \n\nYou are in a tough situation.  If he decides to develop some emotional intelligence it’s going to take YEARS..', 'Allmyownviews1: In my present role I was advised I should use MATLAB rather than Python.. but was told there were scripts I can use to maintain common team output.. 6 months later, I’ve used one MATLAB GUI they shared which was externally produced for the team, everything else has been in Python using my own code as there was seemingly reluctance to share their code.', 'slackington: Service level agreement- if your data product becomes a feature clients pay for there are much stricter availability requirements.', 'Artistic-Breadfruit9: Just a little pet peeve of mine; ROC are everywhere, and are often used as a surrogate for “accuracy”. Unfortunately they fall apart when the “coin flip” baseline assumption doesn’t hold.\n\nCongratulations on the publication, BTW.', 'skeletons_of_closet: ahh.. makes sense, most of my team members have just done research with a proj with a CS degress or a few are mechanical engineers who did a course on coursera and a few projects and got a job in data science so have no idea what it means to write proper maintainable code.', ""skeletons_of_closet: >I have faced this too , initially I though its because they didn't want to share their ideas cause they were way up their ass , but later when I went through their github code , I realized it was poorly documented and there were a lot of mistakes in how the data was processed , I feel the reluctance is there because they know their code has a lot of flaws and don't want someone to point them out to and make them feel stupid or worse invalidate their entire hard work since they didn't have any prior good code maintenance experience or proper code testing."", ""SolverMax: Yep, that's typical of most teams.""]"
1676299387.0,13-Feb-2023 06:43:07,image model,datascience,111aiza,"Senior data scientist, 8yrs experience in deep learning + MS Electrical Engr... What's next?",Sleepy_Carrotz,0,https://www.reddit.com/r/datascience/comments/111aiza/senior_data_scientist_8yrs_experience_in_deep/,"30F living in Austin
MS in EE at 22, 8 years of work experience in deep learning in which I went from Engr I to Senior.

Have extensive experience under my belt   demonstrating everything from expertise in multiple languages and frameworks, aiding directly in exec level business decision making, excellent cost savings, and a large range of problem spaces and deep learning work. I have excellent communication skills and have served as project lead for several (small, 1-3 person) teams. Spaces include valuation modeling, chemometrics, object detection, image style (using triplet loss), consumer-product matching, consumer-agent matching, multi arm bandits in chatbots, and various clustering problems of locations, consumers, and products. I also was in charge of labeling training & QA over about 20 offshore folks for image projects.

I'm curious about dipping my feet further in deep learning but also in management by going for a tech lead / team rep position at my next company but I've no idea how to apply for something like that or what kind of position I should be seeking. Is this typically something you have to earn your way into via promotion rather than through applying? 

Medium sized company just acquired, wanting to look elsewhere. Have been working here for 4 years and just want something new.",6,"[""therealtiddlydump: >I'm curious about dipping my feet further in deep learning but also in management\n\nYou've described the two different paths -- individual contributor vs manager. You might want to get that sorted out first."", ""dfphd: >I'm curious about dipping my feet further in deep learning but also in management by going for a tech lead / team rep position at my next company but I've no idea how to apply for something like that or what kind of position I should be seeking. Is this typically something you have to earn your way into via promotion rather than through applying?\n\nIf there is a job posting for it, it means they couldn't find someone internally to do it. So the answer is simple - apply.  \n\nYes, at some companies they will very much focus on promoting internally, but that really only works at companies where you have a LOT of junior people and a huge recruiting pipeline. And even then - at some point, you are still going to be open to bringing people in from companies of equal or greater statute to yours.\n\nSo if you're  Google, you're probably not hiring a Manager from a low level tech company. But are you going to take a Manager from AWS or Apple? Probably. \n\nBut for most companies, as I said, if there is a job opening it's because they need someone to come in and they couldn't find someone internally."", 'Sleepy_Carrotz: Right,  which is why I stated cursory interest in a team lead type position', ""Sleepy_Carrotz: Gotcha!!! I have a way of overcomplicating. You're right. I just need to keep looking"", 'brrow: Many places are required to post the job externally even if the hire will eventually be internal', 'CliffDraws: Word of warning, if you hate lead/manager it can be extremely hard to get back to a singular role.']"
1676230139.0,12-Feb-2023 11:28:59,image model,datascience,110nzmv,Difficulties consuming AI API vendors like AWS Rekognition,Mkengelo,1,https://www.reddit.com/r/datascience/comments/110nzmv/difficulties_consuming_ai_api_vendors_like_aws/,"I'm a backend developer and nowdays I'm working on a project where I have incoming stream of images and I have to run object-detection task on them.

We chose as the company to use an external object-detection api provider rather than creating our own models.

Therefore I searched for different object-detection API out there and decided to use AWS Rekognition.

Seems like their API is not very easy to use and require many post-processing functions on the response that contains the bounding boxes. Other API's I have checked require post/pre-processing on the images/response labels as well.

&#x200B;

I'm wondering if its just me or consuming AI API's is very unstructured, complexed and has lots of overhead.

I would be happy to hear how you dealt with such cases when you had to consume a Computer Vision/ NLP API's.

1. Was it hard and required additional logic around the pre/post processing of the input/output ?
2. Do you have any tools/tricks to make this API integrations easier ?

Thanks !",0,[]
1674994895.0,29-Jan-2023 04:21:35,image model,datascience,10o6gpv,Can keeping channels more than '3' in images crash CNNs?,jhanjeek,0,https://www.reddit.com/r/datascience/comments/10o6gpv/can_keeping_channels_more_than_3_in_images_crash/,"I have an encoder model which was working fine with single channel 1024,1024 images, I'm trying to patch the original images (mega pixel images) to 256, 256, 64 images. I've changed my encoder input to match the images input that the model will get. The model call function is working fine, loss is getting calculated fine, but I'm getting the following error with tape.gradient:

    2023-01-29 17:11:01.868555: F tensorflow/stream_executor/cuda/cuda_dnn.cc:593] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, dims.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)batch_descriptor: {count: 10 feature_map_count: 64 spatial: 0 0  value_min: 0.000000 value_max: 0.000000 layout: BatchYXDepth}                      C:\ProgramData\Anaconda3\lib\site-packages\joblib\externals\loky\backend\resource_tracker.py:318: UserWarning: resource_tracker: There appear to be 2 leaked folder objects to clean up at shutdown                                               warnings.warn('resource_tracker: There appear to be %d '                                                              C:\ProgramData\Anaconda3\lib\site-packages\joblib\externals\loky\backend\resource_tracker.py:333: UserWarning: resource_tracker: C:\Users\kjhan\AppData\Local\Temp\joblib_memmapping_folder_12248_772bbeeeccff43089fa0e6d75271eebd_97f2f7c6edd04b468a4360bf96b91b84: FileNotFoundError(2, 'The system cannot find the path specified')                                    warnings.warn('resource_tracker: %s: %r' % (name, e))                                                                 C:\ProgramData\Anaconda3\lib\site-packages\joblib\externals\loky\backend\resource_tracker.py:333: UserWarning: resource_tracker: C:\Users\kjhan\AppData\Local\Temp\joblib_memmapping_folder_12248_29db2f1e8ff54416b9a78c6f69dcff23_40a85063390f46d38d15c1877f99acc8: FileNotFoundError(2, 'The system cannot find the path specified')                                    warnings.warn('resource_tracker: %s: %r' % (name, e))                                                                 [I 17:11:10.131 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports                                kernel 286d1cc6-8ddd-46f9-baf7-5e1b05a2d033 restarted

My code is as below

    class encoder(tf.keras.layers.Layer): def __init__(self,size:tuple):
        super(encoder, self).__init__()     #encoder Module
        self.input_cnn = keras.layers.InputLayer(input_shape=(size[0],size[1],size[2]))
        self.conv_1 = keras.layers.Conv2D(input_shape=(size[0],size[1],size[2]),filters=16,kernel_size=(3,3),padding='same',activation='relu')
        self.conv_2 = keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2))
        self.conv_3 = keras.layers.Conv2D(filters = 16,kernel_size=(4,4),strides=(2,2),padding='same',activation='relu')
        self.conv_4 = keras.layers.Conv2D(filters = 32,kernel_size=(4,4),strides=(4,4),padding='same',activation='relu')
        self.conv_5 = keras.layers.BatchNormalization()
        self.conv_6 = keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2))
        self.conv_7 = keras.layers.Conv2D(filters = 64,kernel_size=(8,8),strides=(8,8),padding='same',activation='relu')
        self.conv_8 = keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2))
        self.conv_9 = keras.layers.BatchNormalization()
        self.conv_10 = keras.layers.Conv2D(filters = 1 ,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')
        
        def call(self,inputs,training = True):
            x = self.input_cnn(inputs)
            x = self.conv_1(x)
            x = self.conv_2(x)
            x = self.conv_3(x)
            x = self.conv_4(x)
            if training == True:
                x = self.conv_5(x,training = True)
            else:
                x = self.conv_5(x,training = False)
            x = self.conv_6(x)
            x = self.conv_7(x)
            x = self.conv_8(x)
            if training == True:
                x = self.conv_9(x,training = True)
            else:
                x = self.conv_9(x,training = False)
            x = self.conv_10(x)
            return x

size 0 is 256 size 1 is 256 size 2 is 64

Train\_step from main model:

 

    def __init__(self, size: tuple, optimizer = keras.optimizers.Adam(learning_rate=1e-3),loss_fn = keras.losses.BinaryCrossentropy(from_logits=False),metric = tf.keras.metrics.Accuracy()):     
        super(BCDClassifier, self).__init__()         
        self.input_cat = keras.layers.InputLayer(input_shape = (2,))
        self.encode = encoder(size)
        self.flatten = keras.layers.Flatten()
        self.concat = keras.layers.Concatenate(axis = 1)
        self.classify = classifier(32)
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.loss_tracker = keras.metrics.Mean(name=""loss"")
        self.acc_tracker = metric
        self.f1_tracker = tfa.metrics.F1Score(num_classes=2, threshold=0.5, average = 'micro')
        self.sk_metric_acc = accuracy_score
        self.sk_metric_f1 = f1_score
        self.acc_history = []
        self.loss_history = []
        self.f1_history = []
        
        def call(self, cat_batch, view_batch, images_batch, training = True):
            x1 = self.encode(images_batch,training)
            x2 = self.input_cat(cat_batch)
            x1 = self.flatten(x1)
            x12 = self.concat([x1,x2])
            x12 = self.classify(x12)
            return x12
    
        def train_step(self,cat_batch, views_batch, images_batch, target_batch, training = True):
            with tf.GradientTape() as tape:
                logits = self(cat_batch, views_batch, images_batch,training)
                loss_value = self.loss_fn(target_batch, logits)
    
            grads = tape.gradient(loss_value, self.trainable_weights)
            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
            self.loss_tracker.update_state(loss_value)
            pred = []
            target = []
            threshold = 0.5
            for val in logits.numpy():
                if isinstance(val,np.ndarray):
                    for v_1 in val:
                        if isinstance(v_1,np.ndarray):
                            for v_2 in v_1:
                                if v_2 > threshold:
                                    pred.append(1.0)
                                else:
                                    pred.append(0.0)
                        else:
                            if v_1 > threshold:
                                pred.append(1.0)
                            else:
                                pred.append(0.0)
                else:
                    if val > threshold:
                        pred.append(1.0)
                    else:
                        pred.append(0.0)
            for val in target_batch:
                if isinstance(val,np.ndarray):
                    for v_1 in val:
                        if isinstance(v_1,np.ndarray):
                            for v_2 in v_1:
                                target.append(v_2)
                        else:
                            target.append(v_1)
                else:
                    target.append(val)
            acc = self.sk_metric_acc(target,pred)
            f1 = self.sk_metric_f1(target,pred)
            #self.f1_tracker.update_state(target_batch,logits)
            return {""Loss"": self.loss_tracker.result(), ""Accuracy"": acc, 'F1-score':f1}

Can someone please help me figure out this error?",0,[]
1673968414.0,17-Jan-2023 07:13:34,image model,datascience,10een7m,🚀Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation,oridnary_artist,0,https://youtu.be/AntQHwfyrnk,,1,[]
1674114864.0,18-Jan-2023 23:54:24,image model,datascience,10fw1a3,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,123,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&v=enabled&s=fbfff287d1b9ced59f0aa55aeaf1df73eec90812)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver",14,"['timelyparadox: Yea 100T parameters always seemed off without any revolutionary change in optimisation or hardware. Hardware side does not seem to be the case since it did not jump too much from the time of the GPT-3. Maybe they found a hacky way to optimize it but not in the degrees of freedom needed.', ""jturp-sc: That's all very well reasoned technical explanations, but I can give you the very clear business reasoning: COGS.\n\nAt the end of the day, OpenAI needs to deliver a model that can be run at a business viable operating expense for companies that will consume it. Right now, there are certain use cases where the presently available GPT models are NOT economically viable for high frequency applications -- especially if fine-tuning is necessary. If GPT-4 was comically larger than GPT-3, then it would certainly balloon costs to where very few business applications would be able to absorb the impact to their COGS at a gross margin that made their business operators comfortable."", 'n3rder: Will they have to retrain GPT4 completely or can then reuse the parameters from GPT3.5 and build a differential model on top? I thought that’s what was done with ImageNet for Image detection.', 'CatalyzeX_code_bot: Found relevant code at https://github.com/nvidia/megatron-lm + [all code implementations here](https://www.catalyzex.com/paper/arxiv:2104.04473/code)\n\n\n\n--\n\nTo opt out from receiving code links, DM me', ""NittyGrittyDiscutant: >Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model\n\nOk, but how many GPU are out there in the wild? I think I won't be wrong when I say more then 1B."", 'LesleyFair: Fully agree!', 'koolaidman123: a 100T sparse moe model is very much in line with the order of magnitude jump in (dense equivalent) param size from gpt2 to gpt3', ""Deto: It would have to be used for some dramatically more specialized purpose.  Like, wouldnt' be able to use it to provide customer service, but if it's smart enough, I could see someone paying $10k to say, take some data and write a draft business plan.\n\nI don't disagree with the post's argument, though, on why the model won't be this big.  Just speculating on how you would justify a model that large economically."", 'LesleyFair: Well said, friend! <3  \nDo you have specific applications in mind?', ""Tundur: We've already got a heap of usecases for GPT that *easily* beat the cost of GPT-3. I'd say it could even be beyond a dollar per token before it stops making sense.\n\n\nThe problem at the moment is just regulatory - no one is sending customer data to a black box API in California and using that to make decisions, and absolutely no one is relying on it for a critical process.\n\nUntil GPT can be hosted in-country and has more visibility of what happens to traffic hitting it, there is no business case."", 'mo6phr: Still about an order of magnitude cheaper than comparable human labor. Your argument doesn’t make sense because the unit economics for running GPT4 on a cloud server is still better than the unit economics for labor', 'timelyparadox: Hey we can always hope they have a magical breaktrough and use quantum computing for training', 'timelyparadox: Gpt2 to 3 was 100x jump, we talk about 1000x jump here + the need of more data', ""koolaidman123: that's why i said a moe model, which when matching flops is equivalent to dense models 10-100x smaller in size...\n\nnot to mention you (and op) seem to be very confused about how scaling laws work?""]"
1674079202.0,18-Jan-2023 14:00:02,image model,datascience,10fjnz2,We used Stable Diffusion to create a game of image telephone that can de-identify images,Djinn_Tonic4DataSci,0,https://www.reddit.com/r/datascience/comments/10fjnz2/we_used_stable_diffusion_to_create_a_game_of/,"We combined image captioning using CLIP and image generation using the Hugging Face Stable Diffusion model to create an image de-identifier modeled after the game of telephone, [Imafake](https://hubs.la/Q01wrX4l0)! All you have to do is upload an image, convert it to a caption, then convert that caption to an image with a few clicks! You can also play with the parameters of the diffusion model depending on how gnarly you want your resulting image to be. And caution, they can get rather gnarly, but that’s what makes it fun :) Thoughts and your own generated images welcome!!

https://preview.redd.it/ct6263mejvca1.jpg?width=1276&format=pjpg&auto=webp&v=enabled&s=6321342e1720290b7cad637cb958425e8c503b46",0,[]
1673646118.0,13-Jan-2023 13:41:58,image model,datascience,10b6fpc,Creating a Machine Learning Library,daddyaries,2,https://www.reddit.com/r/datascience/comments/10b6fpc/creating_a_machine_learning_library/,"This post is really a few questions with some context from a beginner in machine learning.

This past semester I had taken a course at my uni on Deep Learning where we had done weekly assignments in python related to different models with the primary use of sci-kit learn. I have always been a fan of lower-level languages and disliked abstraction so this inspired me to work on my own library but related to mathematics in a broader sense (that also includes machine learning methods and techniques).

The source code being all in C++, formal installation with CMake, language binding for Python with easy installation using pip, a binding for R, and some decent documentation and samples to get running, this led me to start using the project for some Machine Learning work in my research lab. The project is still very small with limited functionality but has proven useful as a lightweight tool for some of the work we do in the lab, I've grown curious how I can take my project to the next level.

To my naive understanding and brief 16-week experience, sklearn serves as a tool with prebuilt models for a variety of machine learning methods while TensorFlow acts as building blocks primarily for creating Neural Networks (is this fair to assume?). Also sklearn I've noticed is written mostly in Python while projects like TensorFlow and Torch utilize C++ and C to a much greater degree.

How do these projects provide efficient computations for end users wanting to make use of the library? I am familiar with the concepts of threading but not in a cross-platform sense, with some previous worked with tools like openMP, openCL, openGL, and a bit of CUDA but not in an applicable sense. My project so far does not deal with image or video related data so I am unsure of how the use of GPU exploitation could serve purposeful in my case. What methods and perhaps tools do lower-level projects like TensorFlow use to be a fast and efficient tool while also being portable?

I understand these projects have an abundance of developers and brilliant brains contributing, but any insight or references will be greatly appreciated and help me wrap my head around the idea of creating a fast, efficient, and cross-platform library!

ALSO: Reinventing the wheel isn't a concern, this served as a great project for learning an array of topics in my free time aside from just mathematics.",12,"['abdoughnut: I am inspired, it’s fun to nerd out sometimes. \n\nLet me preface this by saying I have no idea what I’m talking about, but don’t GPUs help us process tensors, regardless of whether they represent images or any other data?', 'renok_archnmy: They use C/C++ bindings like you did. \n\nParallel processing is possible with Python.\n\nTake courses on parallel processing, concurrency, and distributed systems programming if you haven’t yet. That’s how the libraries are efficient.', ""Delpen9: GPUs are good for computing a massive number of parallel tasks that are very similar.\n\nThey're called Graphics Processing Units because traditionally they are used for games and design software."", 'Pentinumlol: Also, in cases where SK-Learn uses SciPy to run its algorithm such Linear Regression, the scipy code is actually written in Fortran (.f)', ""daddyaries: That makes sense, but how are these libraries doing so in a cross platform way? To my understanding these libraries don't leverage openCL or openGL but they do make use of CUDA. This confused me because using CUDA requires a machine with Nvidia hardware therefore not producing something inherently cross platform?"", 'abdoughnut: Right, so they should still be used to speed up computations on non-graphic data, correct?', ""daddyaries: now this is very interesting. i'll have to research this and dive into the source code of both projects."", 'renok_archnmy: Correct, non CUDA hardware did not accelerate libraries like tensorflow. See nvidia stock price.\n\nThere has been work to establish compatibility with AMD and Apple hardware with those libraries. \n\nThere are subtle performance increases even between CPU hardware. Some instruction set or whatever in intel allowed for a small efficiency bump over AMD and ARM, core for core, GHz for GHz. \n\n“Historically” these packages and libraries have not been cross platform. However, all of the Scikit learn stuff is CPU based anyways. \n\nThere are also things like the Intel Neural Compute stick and other niche ARM acceleration hardware. \n\nI don’t think anyone ever was under the misconception that these libraries were cross platform. They are instead portable and can run on consumer hardware. That was the big lift in popularity. Just that it required CUDA drivers to do it until more recently.\n\nAlso consider subtle features of hardware for performance like ECC ram. It may be slower ram, but the reduction in faults over consumer fast “gaming” ram is a boon to an entire system expected to run 24x7 for weeks at a time training models.', 'Delpen9: Yeah, anything with a massive number of simple parallel operations.', 'renok_archnmy: It’s the floating point ops that are of interest specifically. Tensorflow is just matrix math library riding on top of CUDA drivers to offload metric crap tons of flops to the GPU for parallel processing. Same reason they’re popular for crypto mining and occasionally to accelerate database servers. \n\nUsing them to accelerate anything incurs overhead that may be more expensive than just keeping the process in the CPU. Basic maths on small data, CPU is fine. Transposing a multi gigabyte matrix of floats, yeah just take the overhead hit and send it to the GPU.', 'daddyaries: This was an excellent breakdown and incredibly helpful thank you', 'renok_archnmy: Never a problem']"
1673602987.0,13-Jan-2023 01:43:07,image model,datascience,10aqj7f,Transformer not converging because it cannot find EOS token,Defiant-Nobody642,0,https://www.reddit.com/r/datascience/comments/10aqj7f/transformer_not_converging_because_it_cannot_find/,"Dear all,

I've been building a codon optimizer (translating amino acids to DNA codons) using the transformer model 'Attention is all you need'. I've been training it on 1,5 million tokens. As the image shows the loss is stuck and very stable at 1.05, even after 4000 epochs (see figure A). When I use the model for interference I see that it does a lot right, how ever it seems not possible for it to find the <EOS> token. Figure B shows my inference with the bold tokens showing the desired length. It goes all the way untill it reaches the max\_len constraint. The average length of the translated sequence is 300 tokens and a maximum of 1000 tokens. My questions: How can I stimulate the model to find the <EOS>? Since the length of the source is always the same length as the target can I hard code the EOS, would this help? Do I have enough tokens to train on or should i get more? If you need more information I can provide it :)

Thank you for helping me out,

Greetings from the Netherlands,

Daan

&#x200B;

https://preview.redd.it/r40zme9a7sba1.png?width=1868&format=png&auto=webp&v=enabled&s=1b54ef9d7014f9ec3b41e23a0a68a263ff1652f4",1,"['HesaconGhost: It could just be that this is as accurate as you can get with these data and this technique. Adding more data may not help you.\n\nWhat can help is quantifying uncertainty. If you can get a confidence interval around your model, you can find the points it fails and build another model on those points. What you then have is an ensemble model where the models best fit subsets of the data, rather than trying to force one model to do everything.\n\nThe patterns you find in splitting the data might also be useful for classifying different groups, which you could use as a categorical factor in other models.']"
