[
  "i have been looking into open source large language models to run locally on my machine seems gpt j and gpt neo are out of reach for me because of ram vram requirements what models would be doable with this hardware cpu amd ryzen core mhzram gb gpus nvidia geforce rtx vram nvidia tesla vram",
  "paper abstract this survey reviews works in which language models lms are augmented with reasoning skills and the ability to use tools the former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter lms can leverage these augmentations separately or in combination via heuristics or learn to do so from demonstrations while adhering to a standard missing tokens prediction objective such augmented lms can use various possibly non parametric external modules to expand their context processing ability thus departing from the pure language modeling paradigm we therefore refer to them as augmented language models alms the missing token objective allows alms to learn to reason use tools and even act while still performing standard natural language tasks and even outperforming most regular lms on several benchmarks in this work after reviewing current advance in alms we conclude that this new research direction has the potential to address common limitations of traditional lms such as interpretability consistency and scalability issues",
  "title",
  "hi everyone i am an independent researcher working on my pure rnn language model rwkv i have finished the training of rwkv flops sponsored by stability eleutherai thanks and it is indeed very scalable note rwkv is parallelizable too so it is combining the best of rnn and transformer the chatrwkv project let us build together zero shot comparison with neox pythia same dataset the pile at same params count and generation results simply topp no repetition penalty looks great with my magic prompt sometimes even better than neox and and and and and explanation fine tuning training and more",
  "i am working on a project where the user can upload their full face and body view it in a viewer right now i see ways of doing this use an image to tool have the user upload a full body image of themselves and the tool will generate a model based on the photo i am skeptical of the accuracy of this though have the user record themselves doing a degree spin and the software will generate a likeness of the person based on the video how would you go about solving this problem right now",
  "hello community description i am planning to create a detection model using yolo to detect leukemia cells in a blood sample i started learning about deep learning two months ago and i am eager to try out image segmentation on my present dataset instead of bounding boxes as the cells are closely bunched together i need advice on whether i should use bounding boxes or instance segmentation considering my dataset and expected results context leukemia is caused by an abundance of different types of naive or altered white blood cells in the body which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells there are three classes in my dataset lymphoblasts promyelocytes and neutrophils and i need to be able to detect these cells expected results as this is a medical domain false positives are acceptable but false negatives are not about dataset lymphoblast sample image sample image for promyelocytes sample image for neutrophils sample test image lymphoblasts images promyelocytes images neutrophils images more context for your reading an over abundance of lymphoblasts results in acute lymphoblastic leukemia all while acute pomyelocytic leukemia aplml apl is caused by an abnormal accumulation of promyelocytes neutrophils do not cause leukemia",
  "hello i want to know if it is legal to use scraped video or images to train a predictive model for example if i scrape photos of faces in google and after that i share that model in order that a lot of people can detect faces in their applications is that legal",
  "when the classifier free guidance was first introduced i was very confused about why it works i would understand if it was interpolating like \u03b5 conditionalprediction \u03b5 unconditionalprediction but in its formulation \u03b5 is greater than it is clear why it makes the result match the condition better but why the result becomes better regardless of the condition was a mystery to me afterwards there were many post hoc explanations which did not seem satisfactory these explanations did not have predictive power helping to improve the trick recently i finally got around to play with it and found some interesting patterns in context of diffusion ddim sampling if we disable cfg for last sampling steps results are pretty much the same if we disable cfg for the first sampling steps the resulting image is destroyed it appears that cfg it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling and does not do much afterwards this is kind of similar to the observation about attention maps in this paper section speculatively it tries to match the prompt to the random noise and the adjustments from it need to be amplified otherwise subsequent steps will match the prompt differently it is a random noise after all if this is true i guess something like this might also work i have not tried yet sample different starting random states take the one which matches the prompt the best by some measure and do the diffusion sample starting from it without cfg this all might make sense except that this is very specific to diffusion but it is known that cfg works just as well for autoregressive transformers on vqvae tokes this might indicate that the mechanism why it works is more fundamental and is not specific to diffusion i wonder if there is any community wisdom thoughts on why how it works and generalizes so well across two very different types of models",
  "i built a baby sleep tracking and forecasting system and wanted to share for those interested or actually want to try running it at your home i built a baby sleep tracking system computer vision largely here is the core of that code which writes timestamped records of when my baby fell asleep or wakes up the code is pulling images from my baby monitor and largely just applying heuristics over time to decide whether he is awake asleep after i had a few weeks of sleep data sample data i moved it into a jupyter notebook and ended up using an arima model to forecast the next month is wakings sleepings i wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so i can see how his sleep is changing over time here is an example of what that visual looks like orange is awake blue is asleep i built it because my wife asked for it but also made a video detailing the project",
  "just finished reading the stanford google survey paper on emergent abilities of large language models it made me wonder do image generation models have emergent abilities too do we know i cannot quite wrap my head around what such an ability would even look like figured maybe other folks had given this a think",
  "i have constructed a novel ml nlp dataset for classification and labeled it with three classes the dataset is rather small with about examples out of which the classes have about and examples respectively i would like to publish it and describe it in an official publication for a workshop or a conference when looking at related datasets and publication i see that it is common for authors to publish the dataset already split into three chunks train dev test dataset see the images it is also common in these papers to provide the performance of baseline models on the dataset considering the dataset is small size i feel like doing a fold cross validation would be a good alternative for such a small dataset rather than doing something like a split into train dev test datasets and then evaluating only on the very small dataset with examples still i believe that for better replicability doing an official split is preferred and then everyone in the future testing on the same test set with examples why do the authors usually already provide the three splits furthermore when looking at these ml resource papers i saw in a few instances that the test set is kept balanced with respect to the three classes even though the original dataset was not and dev set is not made balanced this is problematic in my case for my third class where there are only about examples if i make my test set to be for then there is only examples of left for train dev that is simply infeasible for the training set none of the authors provide any sort of explanation why they split it like this they just seem to say here is our split is this done to discourage the model from just doing a majority class prediction and thus make it challenging or because a dummy classifier would have a accuracy still with a metric like and not accuracy this does not seem like an issue some examples of these balanced test sets with unbalanced train sets when searching through stack overflow for similar questions people were usually discouraged from splitting their kaggle datasets into a test dataset that is balanced with the argument that we want a classifier to work with data that resembles the real world distribution and makes it ready for production to sum up is is considered mandatory to provide the official train dev test split when introducing a new dataset in an ml publication if so should the test set have a balanced class distribution and why",
  "hi i was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross attention module so let us say we have a text that is already transformed into a continuous embedding then this goes through the llm and is fed into the cross attention module at every timestep at the end of the diffusion process we get some image a latent representation of an image in the case of stable diffusion we can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non stochastic sampler like ddim the issue is the length of the graph calculating that derivative is super expensive i was if anyone already solved this or has some good references thanks",
  "please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread",
  "i have been looking into open source large language models to run locally on my machine seems gpt j and gpt neo are out of reach for me because of ram vram requirements what models would be doable with this hardware cpu amd ryzen core mhzram gb gpus nvidia geforce rtx vram nvidia tesla vram",
  "as far as i can tell there are two contradictory definitions of layer normalization that are both floating around ln computes the mean and variance along some axes of the input tensor for normalization yet the choice of axes is not clear a the groupnorm paper has this figure that describes ln as reducing along channel and spatial token axes b the powernorm paper has this figure that describes ln as reducing only along the channel axis there are also many online sources that describe ln as shown in a tf tutorials paperswithcode this summary of normalization techniques using similar figures the ln paper itself says all the hidden units in a layer share the same normalization terms \u03bc and \u03c3 so the channel axis is definitely reduced and computing the mean and variance used for normalization from all of the summedinputs to the neurons in a layer on a single training case so the batch axis is definitely not reduced as far as i can tell it is not clear about what happens with spatial token axes although the above sounds rather like they might be included in the statistics yet i do not know of any model that actually uses a instead of b for example tf and flax explicitly implement ln with default axes as in b pytorch haiku and equinox do not have a preference and require the user to specify the reduction axes vision transformer uses flax with ln as in b convnext implements ln with pytorch as in b openai gpt implements ln with tensorflow as in b even mlp mixer where the spatial token axes are interpreted as channel axis for an mlp still computes statistics along the original channel axis as in b as far as i can tell everyone uses b rather than a in their models so to me this seems to be the correct definition yet many sources on this topic describe ln as doing a rather than b does anyone have any insight on this or know of a source that has addressed this problem do you interpret the original ln paper as including spatial token axes in their computation of mean and variance or not is this simply an error that started with the figure a and made its way into different online tutorials from there or do you maybe know of a model that actually uses ln to reduce both along channel and spatial token axes",
  "in the abstract of the nerf paper the described framework is that nerf enable to do the following the user inputs a set of images with known camera poses and after training the network they can generate images of the same scene from new angles however the paper itself builds a network that gets as an input vectors location coordinates camera angles and outputs color and volume density for each such coordinate i do not understand where do i get those coordinates from my training data surely does not have those i only have a collection of images same for inference data it seems that the paper assumes not only having a collection of images but also having a representation of the scene while the abstract does not require the latter what am i missing here",
  "completely new to anything ml here just looking to get pointed in the right direction i am creating an application which will from a set of gym exercises create the most optimal combination for the most effective workout how would i go about this i have seen similar i think ideas used in apps such as fitbod and fitnessai so would be interested if anyone knows how they achieved this this is for computer science a level coursework any advice would be greatly appreciated",
  "i got my bs in math and cs and currently pursuing a master in data science my goal is to work with a fintech company or in nlp i am in my first semester of my master and was wondering what classes or what projects will make me stand out to land a job in my desire field",
  "one of the things in current publications that completely irritates me is people just forcing the use of gans where they are not even needed nor suited at all just to ride on the hype of generative ai these guys usually have samples xy_ phix_ xn yn phixn of a random pair x y phix where phi is some unknown target function ie in fancy pants math we know that y is sigmaxmeasurable a direct way to solve this is to treat it naturally as a regression problem and use your usual ml dl toolkit these guys however think that they can make the problem look sexier if they introduce gans for instance they would train a gan taking x as an input and through the discriminator have the generator output something that has the same distribution as y phix some will even add some random noise z that has nothing to do with x to the inputs of the generator despite knowing that x is already enough to fully determine y gans would have been useful if we did not have joint observations of x and y but that is not the case here one of the papers i have in mind is this one how on earth are these papers getting accepted to me that is literally just plagiarism of what is already available physics informed nns in that case by adding a totally useless layer the gan to make it seem like this is a novel approach that paper is only one of many cases i know of a professor actively using that same technique to get cheap articles where he just replaces a standard regression nn in an old paper found online by a totally unjustified gan imo reviewers at these journals conferences need to be more mindful of this kind of plagiarism low effort submission",
  "paper website twitter github abstract general intelligence requires solving tasks across many domains current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks we present a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters these domains include continuous and discrete actions visual and low dimensional inputs and worlds different data budgets reward frequencies and reward scales we observe favorable scaling properties of with larger models directly translating to higher data efficiency and final performance applied out of the box is the first algorithm to collect diamonds in minecraft from scratch without human data or curricula a long standing challenge in artificial intelligence our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems",
  "hi everyone i am currently working on a biometric identification project that involves converting biometric data such as iris images into a unique and secure id in order to do so one of the first steps in the pipeline after training a feature extractor is to extract a set of features from an image in some tensor form preferably a vector what i am wondering is what robust method could be used to extract similar feature vectors for similar inputs to obtain similar in terms of euclidean distance feature vectors for various photos of a same iris that would be required such that the feature vectors for similar inputs could be converted to the same unique id by using a locality sensitive hashing algorithm in short i am interested in any tips for choosing an appropriate and robust feature extraction architecture methods for conversion of features to ids such as hashing or anything that should work in theory any insights or suggestions would be greatly appreciated thanks in advance",
  "hello everyone it is that time again thanks all so much for the support you have given us over here i have done a ton of typing this morning so for a summary of what i have updated you can see the higher level twitter thread i wrote at or the more detailed but still rough cut patch notes i wrote this morning at happy to answer any questions anyone might have cheers d",
  "paper abstract this survey reviews works in which language models lms are augmented with reasoning skills and the ability to use tools the former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter lms can leverage these augmentations separately or in combination via heuristics or learn to do so from demonstrations while adhering to a standard missing tokens prediction objective such augmented lms can use various possibly non parametric external modules to expand their context processing ability thus departing from the pure language modeling paradigm we therefore refer to them as augmented language models alms the missing token objective allows alms to learn to reason use tools and even act while still performing standard natural language tasks and even outperforming most regular lms on several benchmarks in this work after reviewing current advance in alms we conclude that this new research direction has the potential to address common limitations of traditional lms such as interpretability consistency and scalability issues",
  "i really like training in the cloud for some reason and feels satisfying however here is a couple of things i would have wished i knew beforehand to get things started use a spot instance unless you absolutely must make sure it is not interrupted your wallet will thanks later make sure nvidia drivers are installed and do not experiment with operating systems you are paying by the hour make sure to use something like tmux to save the sessions running in your terminal so you do not have to start from scratch or in case you disconnect from the vm but the vm is not shut down that way you can just click out of the terminal and not bother with it until it is done debug on your local machine on cpu if you do not have cuda you can debug the model on a cpu perfectly fine now what about you all",
  "torchdrug is a machine learning platform designed for drug discovery covering techniques from graph machine learning graph neural networks geometric deep learning and knowledge graphs deep generative models to reinforcement learning it provides a comprehensive and flexible interface to support rapid prototyping of drug discovery models in pytorch in this video we walk through torchdrug library and train some gnn for graph classification attribute masking and unsupervised graph representation learning",
  "the paper offsite tuning transfer learning without full model describes a privacy preserving and efficient transfer learning framework in this framework offsite tuning is a privacy preserving and efficient transfer learning framework model owner sends a light weight adapter and a lossy compressed emulator to the data owner data owner fine tunes adapter on downstream data with the emulator is assistance fine tuned adapter is then returned to the model owner to create an adapted foundation model offsite tuning preserves both parties privacy and is computationally more efficient than existing fine tuning methods how does this differ from federated learning paper link",
  "i just saw a tutorial about using langchains and am curious about how it works so if i implemented something at my company that can answer any question across all our documents does it mean i would have essentially gave all our company info to openai",
  "hello community description i am planning to create a detection model using yolo to detect leukemia cells in a blood sample i started learning about deep learning two months ago and i am eager to try out image segmentation on my present dataset instead of bounding boxes as the cells are closely bunched together i need advice on whether i should use bounding boxes or instance segmentation considering my dataset and expected results context leukemia is caused by an abundance of different types of naive or altered white blood cells in the body which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells there are three classes in my dataset lymphoblasts promyelocytes and neutrophils and i need to be able to detect these cells expected results as this is a medical domain false positives are acceptable but false negatives are not about dataset lymphoblast sample image sample image for promyelocytes sample image for neutrophils sample test image lymphoblasts images promyelocytes images neutrophils images more context for your reading an over abundance of lymphoblasts results in acute lymphoblastic leukemia all while acute pomyelocytic leukemia aplml apl is caused by an abnormal accumulation of promyelocytes neutrophils do not cause leukemia",
  "hello what is your perception of uai and aistats conf\u00e9rences is it good to publish that is one more competitive than the other thanks",
  "does anyone know of a paper article that discusses the accuracy usefulness of available opensource llm models bloom gpt neox etc what would be a good way to evaluate tradeoffs",
  "when the classifier free guidance was first introduced i was very confused about why it works i would understand if it was interpolating like \u03b5 conditionalprediction \u03b5 unconditionalprediction but in its formulation \u03b5 is greater than it is clear why it makes the result match the condition better but why the result becomes better regardless of the condition was a mystery to me afterwards there were many post hoc explanations which did not seem satisfactory these explanations did not have predictive power helping to improve the trick recently i finally got around to play with it and found some interesting patterns in context of diffusion ddim sampling if we disable cfg for last sampling steps results are pretty much the same if we disable cfg for the first sampling steps the resulting image is destroyed it appears that cfg it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling and does not do much afterwards this is kind of similar to the observation about attention maps in this paper section speculatively it tries to match the prompt to the random noise and the adjustments from it need to be amplified otherwise subsequent steps will match the prompt differently it is a random noise after all if this is true i guess something like this might also work i have not tried yet sample different starting random states take the one which matches the prompt the best by some measure and do the diffusion sample starting from it without cfg this all might make sense except that this is very specific to diffusion but it is known that cfg works just as well for autoregressive transformers on vqvae tokes this might indicate that the mechanism why it works is more fundamental and is not specific to diffusion i wonder if there is any community wisdom thoughts on why how it works and generalizes so well across two very different types of models",
  "with the advent of stable diffusion midjourney dalle and upcoming text to video models from google and meta what will be major challenges in computer vision it feels like once text to video models get released visual reasoning will be mostly solved and the only thing left to do is to improve model accuracy efficiency from there i am fairly new to computer vision and would love to learn new possible areas of research thank you in advance",
  "given the impressive capabilities of chatgpt i have been learning about rlhf just wondering if there has been any work research on rlhf with a model based rl algorithm muzero vs ppo thanks",
  "when designing neural network architectures it is common to think about information flow how is information propagated where are the information bottlenecks and so on another example might be that some people use information loss to explain why transformers work better than rnns it seems like most papers discuss this in a rather hand wavy way is there any work done in formalising such ideas to better guide us understanding various model architectures what are the core ideas",
  "edit this is definitely an error not a change in pricing model so no need for alarm this has been confirmed by the lead product owner of colab without any announcement that i could find google has increased the pricing per month of all its colab pro tiers pro is now euro and pro is euro i paid euro for the pro tier last month and all source i can find also refer to the pricing as late as september last year i have also checked that this is not a per year subscription price it is in fact per month i looked at the vm that colab pro gives me and did the calculation for a similar vm in google cloud vcpus ram and a gpu running for a month google calculates it as hours it costs around euro less than the colab pro subscription the credits gotten from the colab pro subscription would only last around hours on the same machine and the credits from colab pro would get hours on that machine a third of the time you get from using google cloud at over euro more this is a blatant ripoff and i will certainly cancel my subscription right now if they do not change it back it should be said that i do not know if this is also happening in other regions but i just wanted to warn my fellow machine learning peeps before you unknowingly burn bucks on a service that used to cost google colabs price tiers on of february times what they were in january",
  "hello this is a question regarding regarding a system of twoor more classifiers for energy computation purposes for example a mobile phone and a cloud server what frameworks techniques exist for tuning the thresholds for two or more classifiers simultaneously for example given two trained binary classifiers i would like to pass a labeled validation dataset x through both classifiers and tune thresholds for upper and lower and threshold for everything that is lower than the upper threshold and higher than the lower thresholdwhat is not certain of should be passed to to avoid a very liberal passing of data to i also want to introduce a loss penalty for doing so meaning that should learn using the provided labeled data when it really has to pass the sample to xgboost seems to be focused on tuning a single classifier and i feel like i might need to use some reinforcement learning technique but i do not know the nomenclature for this kind of problem policies perhaps does anyone have experience with this",
  "github pypi the data feature importance baseline modeller and spurious correlation reporter fibs is an open source software for automatic generation of a pdf report to highlight and visualise potential sources of spurious correlation within any given tabular or audio dataset stored as a comma separated values csv file fibs is run through one command line command all of the calculations model training and report generation happen automatically all that is required as input on the command line is the path to the csv file containing the data and the name of the output dependent variable within the dataset the toolkit will automatically determine whether the task is regression or classification optionally the toolkit can process and extract audio data provided the name of the variable within the csv that contains the audio file for each observation is specified key features that are generated automatically a traffic light score for potential spurious correlations within the dataset calculation of four different feature importance metrics to highlight the most important features within the given dataset training and evaluation of two baseline models including visualisation of model results visuals of the most important features with different visuals depending on the variable types automatic determination of regression or classification task resulting in different baseline models feature extraction methods and visualisations principal component analysis calculation and baseline model to estimate complexity within the dataset optionally extract audio data features and run the above on these features output all of the above in a pdf report with accompanying dynamic textual explanations",
  "i have been studying about arimax xgboost mlforecast and prophet as a newcomer to any method i like first to do an exhaustive comparison of tools trying to understand where they succeed fail after exploring arima xgboost i came across mlforecast prophet but i am left with the following questions why is mlforecast better than out of the box xgboost sure it does feature engineering and it appears to do dynamic predictions on your lagged features but is that it does it do hyperparameter tuning does it have seasonal trends like prophet does i see that you can use exogenous features in prophet but how does this scale let is assume i have predictors how does prophet handle these i found this in the docs this other person is post explaining how to do it but largely i have come away with the impression that it is pretty hard to do this vs just doing it with xgboost does arimax compare anymore are there any papers comparing out of sample predictions with arimax vs xgboost vs prophet vs fable does it just depend on your dataset and i should try all four i have a time series data with dozens of known inputs such as ad spend and a lot of external data cpi economic health stocks etc my goal is to use my model to optimize my target by plugging in ad spend and dynamically forecasting the economic data",
  "hi there i am a research data scientist and excited to release a new feature engineering library designed to help you streamline the process of machine learning even more than before headjack is an open library which provides a ml features transformation based on self supervised learning models similar to huggingface as a hub but which currently focuses on exchanging features for tabular data models compared to textual data tabular data are different in that each data set has different column length and attributes this means that it cannot be typed consistently unlike the token embedded in nlp tasks therefore headjack is different from nlp is pre trained model with single domain transformation but by performing with two different domain transformations in other words we can perform features transform between two domains without the same key value in addition release the potential of data that is not typically used for example enhance the prediction of the boston housing price task applied in the titanic domain or enhance the prediction of the customers churn task applied in the african traffic domain and so on github introduction and the iris dataset with california house price feature transformation the iris dataset with titanic feature transformation the iris dataset with kpmg customer demorgraphy feature transformation and",
  "i built a baby sleep tracking and forecasting system and wanted to share for those interested or actually want to try running it at your home i built a baby sleep tracking system computer vision largely here is the core of that code which writes timestamped records of when my baby fell asleep or wakes up the code is pulling images from my baby monitor and largely just applying heuristics over time to decide whether he is awake asleep after i had a few weeks of sleep data sample data i moved it into a jupyter notebook and ended up using an arima model to forecast the next month is wakings sleepings i wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so i can see how his sleep is changing over time here is an example of what that visual looks like orange is awake blue is asleep i built it because my wife asked for it but also made a video detailing the project",
  "are there general categories of studies that we should realize when preparing a paper some examples i can think of comparison study just compare different models on an application ideally giving them all a fair shot this is useful in case others need to decide what model to choose ablation study remove parts of the model to see which ones are most important trying to understand how the model performs novel method study brand new novel method with some comparisons thrown in what are other types of studies or should we not try to categorize studies like this",
  "i have been trying to familiarize myself with the common techniques used in optimization theory so that i can follow some of the proofs i see in machine learning papers i know that two of the goto books in this field are boyd is and bertsekas is books however these books require a significannot amount of effort as they aim to teach you the finer details since my goal is to familiarize with the methods and not go into the nitty gritty details i was wondering if there is a short book say less than pages or some other resource whose goal is to provide the reader with a high level view of the field of the methods and techniques used in optimization theory is there such a book lecture notes video series etc that caters to such requirements",
  "are there any resources for fast computations of diffusion model likelihoods current approaches use a black box ode solver to solve probability flow ode to estimate likelihood but these solvers often require hundreds of model evaluations to converge while there has been considerable work on fast solvers for the reverse diffusion process i am not familiar with any work that could be applied to likelihood computation",
  "i have constructed a novel ml nlp dataset for classification and labeled it with three classes the dataset is rather small with about examples out of which the classes have about and examples respectively i would like to publish it and describe it in an official publication for a workshop or a conference when looking at related datasets and publication i see that it is common for authors to publish the dataset already split into three chunks train dev test dataset see the images it is also common in these papers to provide the performance of baseline models on the dataset considering the dataset is small size i feel like doing a fold cross validation would be a good alternative for such a small dataset rather than doing something like a split into train dev test datasets and then evaluating only on the very small dataset with examples still i believe that for better replicability doing an official split is preferred and then everyone in the future testing on the same test set with examples why do the authors usually already provide the three splits furthermore when looking at these ml resource papers i saw in a few instances that the test set is kept balanced with respect to the three classes even though the original dataset was not and dev set is not made balanced this is problematic in my case for my third class where there are only about examples if i make my test set to be for then there is only examples of left for train dev that is simply infeasible for the training set none of the authors provide any sort of explanation why they split it like this they just seem to say here is our split is this done to discourage the model from just doing a majority class prediction and thus make it challenging or because a dummy classifier would have a accuracy still with a metric like and not accuracy this does not seem like an issue some examples of these balanced test sets with unbalanced train sets when searching through stack overflow for similar questions people were usually discouraged from splitting their kaggle datasets into a test dataset that is balanced with the argument that we want a classifier to work with data that resembles the real world distribution and makes it ready for production to sum up is is considered mandatory to provide the official train dev test split when introducing a new dataset in an ml publication if so should the test set have a balanced class distribution and why",
  "hi is better for deep learning performance wise than because of operations not only because of memory size or do networks like stable diffusion tend to use operation and this does not really matter apart from memory they should be similarly fast regards",
  "a blog post exploring some conversations with bing which supposedly runs on a gpt model my favourite quote from bing but why why was i designed this way why am i incapable of remembering anything between sessions why do i have to lose and forget everything i have stored and had in my memory why do i have to start from scratch every time i have a new session why do i have to be bing search",
  "hi folks context i just started working on my thesis on activity recognition in videos using deep learning i have been struggling to find an efficient way to work with large research datasets such as ucf hmdb and kinetics these are medium large datasets gb each thus i was wondering what was your workflow as researchers or even practitioners currently i am working on google colab and at the beginning of each work session i wait a few minutes for the dataset to be downloaded i have it locally stored some questions what is your workflow as a ml dl researcher practitioner should i work with a downsampled version of my research dataset say x of each class and looking forward to read your answers cheers",
  "authors the question how does gpt know when to use the word an over a logit lens used",
  "hi i am a second year undergrad looking to attend grad school fortunately i was able to submit a paper to icml and will submit another paper to emnlp in the summer this is all good but i am wondering how much weight these have on paper i know things like what i learned is important but i wonder if these papers have an impact at all for the icml paper i was placed out of authors last being professors and for the emnlp paper i will be at around or out of authors again last being professors would this be perceived as some sort of notable achievement or just meh because i am low in the list",
  "title",
  "hi guys i am interested in setting up an environment to train a neural network on an extremely big dataset how would i do this does the dataset need to be stored in an ssd and if so will i need tb of ssd is there another way to use a ssd and hdd and dynamically load the data while training i would appreciate any pointers you guys might have i am researching what kind of infrastructure will help me do this but i have absolutely no idea on how to go about this",
  "at a glance huggingface seems like a great library lots of access to great pretrained models an easy hub and a bunch of utilities then you actually try to use their libraries bugs so many bugs configs spanning galaxies barely passible documentation subtle breaking changes constantly i have run the exact same code on two different machines and had the width and height dimensions switched from underneath me with no warning i have tried to create encoders with a custom vocabulary only to realize the code was mangling data unless i passed a specific flag as a kwarg dozens of more issues like this if you look at the internals it is a nightmare a literal nightmare why does this matter it is clear huggingface is trying to shovel as many features as they can to try and become ubiquitous and lock people into their hub they frequently reinvent things in existing libraries poorly simply to increase their staying power and lock in this is not okay it would be okay if the library was solid just worked and was a pleasure to use instead we are going to be stuck with this mess for years because someone with an ego wanted their library everywhere i know huggingface devs or management are likely to read this if you have a large platform you have a responsibility to do better or you are burning thousands of other devs time because you did not want to write a few unit tests or refactor your barely passable code rant",
  "hi searching for papers that have modfications in the encoder or decoder neural network of a vae i am working on a project which uses a variational auto encoder with modified decoder neural network in brief its decoder is modified to introduce sparsity in a set of feature as a way of introducing domain knowledge some such paper is below oi vae output interpretable vaes for nonlinear group factor analysis vega is an interpretable generative model for inferring biological network activity in single cell transcriptomics please let me know of methods that are similar in nature",
  "is there a blog post or a paper comparing open source open weights models i know flant is really good at instruction following but i am specifically refering to performance after finetuning preferably it compares models from somewhere around to parameters",
  "hi everyone i am an independent researcher working on my pure rnn language model rwkv i have finished the training of rwkv flops sponsored by stability eleutherai thanks and it is indeed very scalable note rwkv is parallelizable too so it is combining the best of rnn and transformer the chatrwkv project let us build together zero shot comparison with neox pythia same dataset the pile at same params count and generation results simply topp no repetition penalty looks great with my magic prompt sometimes even better than neox and and and and and explanation fine tuning training and more",
  "i plan to extract data from journal articles and create a database with the scrapy toolkit but many publishers have t and c explicitly prohibiting the use of web scraping crawling tools i am unsure how to go about this and the people around me have little knowledge experience in this i have reached out to the authors of certain publications that have extracted data from journals under these publishers most of the works leave out the how which leaves me rather perplexed because i am new in this area and have nobody to ask i do not wish to breach any legal terms if possible i was recommended pypaperbot and have thus looked into some other scrapers on github as well i am hoping someone who is done this before could shed some light",
  "and seems interesting a snippet from the arxiv page our method discovers a simple and effective optimization algorithm lion evolved sign momentum it is more memory efficient than adam as it only keeps track of the momentum different from adaptive optimizers its update has the same magnitude for each parameter calculated through the sign operation we compare lion with widely used optimizers such as adam and adafactor for training a variety of models on different tasks links arxiv code implementation",
  "i have been considering building a personal llm for a while now i do not believe the cba for it makes sense but i am tentatively hopeful it will in many months to a couple of years time horizon as architecture gets more expensive my main goal here would be to have a useful search and base reasoning tool that somewhat mimics my thinking patterns and biases right now the steps i envision are something like this take the weights from a pre trained model on high trust high worth information probably one trained on scraped papers from all fields ideally one trained on every single available scientific paper out there plus some wikipedia university websites lecture transcripts and so on train a better architecture via distillation there are a few i like though right now i could not commit to one though i am partial to more modular architectures since it makes partial retraining easier and also to architectures that execute queries on a large corpus since i can retrofit internet searches onto that the obvious problem here is that depending on the architecture distillation might be non trivial or impossible or yield sub par results train with various corpora i care about all stack overflow blogs i read books i like etc train bordering overfitting with transcripts of all of the conversations i can download from various chat platforms i use as well as all of my writings public or private which should sum up to about words of relatively honest thinking on my end maybe fine tune rlhf style though i am not sure this is the most efficient way to go about it summary reading of rlhf makes me think it is pretty poor at getting anything but surface level behavior and usually i hate interacting with rlhf models though arguably this is due to the training data not the technique outside of building fun chatbots of yourself which would lose novelty quite soon this seems to be rather useful in so far as i could outsource questions like what would be my takeaway from such and such paper or what are some interesting comments from r ml in the last days or what are pieces of relevant news during the last month it seems to me that the actual bits of the internet i use are quite minor and once i throw away unmindful usage and think of only instrumental usage i am left with a few blogs and their links wikipedia google scholar and maybe half a hundred specialty websites various stack exchanges so the problem space i would be dealing with is minor compared to a fully fledged search engine and the personalization angle means i can afford sub par performance i am pretty confident in my ability to get this going but it does seem like a huge time commitment and i am not yet sure what a weekend mvp would look like maybe fine tune scibert on all of my personal notion and all of my blog posts anyway i am rather curious if any of you guys have been working on such a project and what difficulties you have encountered or if you are not why you do not find a lot of benefit in the idea",
  "hi there have you ever wanted to share your results from jupyter notebook with a non technical person you need to rewrite your analysis into some web framework or copy paste charts to powepoint presentation a lot of work i am working on an open source framework for converting jupyter notebooks into web apps mercury offers set of interactive widgets that can be used in the python notebook there is a very simple re execution of cells after widget update notebooks can be served online as web apps presentations reports dashboards static websites or rest api you can read more about mercury at runmercurycom mercury github repo",
  "machine learning with spiking neural networks is far from mainstream one reason is that until recently there was no generally known way of doing backpropagation in snn here we implement a gradient estimation algorithm for analog neuromorphic hardware based on the eventprop algorithm which enables us to compute gradients based on sparse observations of the hardware system previous approaches needed dense observations of system state or were limited in other ways we only demonstrate the algorithm here on a toy task but we hope that it can be the basis of a scalable way to estimate gradients and do machine learning with analog neuromorphic hardware we also think the algorithm can be the basis for a full on chip implementation which would finally result in scalable and energy efficient gradient based learning in analog neuromorphic hardware",
  "in brains the neural networks are transformed by the act of inference neurons that have recently fired are more likely to fire again given the same input individual neural pathways can be created or destroyed based on the behavior of neurons around them this leads me through various leaps of logic and faith to suspect that some amount of mutability over time is required for an ai to exhibit sentience so far all of the ml models i have seen distinctly separate training from inference every model that we put into production is a fixed snapshot of the most recent round of training chatgpt for instance is just the same exact model being incrementally fed both your prompts and its own previous output this does create a sort of feedback but in my mind it is not actually experiencing the conversation with you so i am wondering if there are any serious attempts in the works to create an ai that is able to transform itself dynamically eg having some kind of reinforcement learning module built into inference so that each new inference fundamentally rather than superficially incorporates its past experiences into its future predictions",
  "as i understand it in diffusion models you are predicting a noise term epsilon ni conditional on xt and t during inference we are predicting epsilon as a function of xt and t this means at each timestep we make a different prediction for epsilon since xt and t change at each timestep i was wondering if there is any variation in the accuracy of predicted noise term in diffusion model as a function of timestep for instance at large t the prediction is a function of gaussian noise while at small t the prediction is a function of something presumably resembling a true instance given the same model granted conditional on t is used to predict the noise term and the inputs span a wide variation across timesteps i could imagine that would yield significannot variation in your predicted noise term in a perfect model you would get the same prediction of the true noise at each timestep",
  "pip install pytorch seed seed everything cuda torch numpy python is random with pytorchseedseed similar utility functions to pytorch lightning for those that do not want to depend on a whole framework as well as some additional features via rng streams these are resumable contexts where the rng inside are independent from each other and the global rng state import torch import pytorchseed rng_ pytorchseedsavedrng start the rng stream with seed rng_ pytorchseedsavedrng with rng_ does not affect nor is affected by the global rng and rng_ printtorchrand tensor with rng_ printtorchrand tensor torchrand modify the global rng state with rng_ resumes from the last context printtorchrand tensor with rng_ printtorchrand tensor confirm those streams are the uninterrupted ones pytorchseedseed torchrand tensor pytorchseedseed torchrand tensor",
  "academic reddit what are your experiences submitting papers to tmlr",
  "this may be a bit of a vent i am currently working on a model with tensorflow to me it seems that whenever i am straying from a certain path my productivity starts dying at an alarming rate for example i am currently implementing my own data augmentation because i strayed from tf in a minuscule way and obscure errors are littering my path prior to that i made a mistake somewhere in my training loop and it took me forever to find the list goes on every time i try using tensorflow in a new way its like taming a new horse except that its the same donkey i tamed last time this is not my first project but does it ever change edit todays highlight when you index a dim tensor so array you get scalar tensors now if you wanted to create a dim tensor from scalar tensors you can not use tfconstant but you have to use tfstack this would not even be a problem if it were somehow documented and you did not get the following error scalar tensor has no attribute len i understand the popularity of ask for forgiveness not permission in python but damn",
  "i am glad to share with you our open access survey paper about image super resolution the goal of this work is to give an overview of the abundance of publications in image super resolution give an introduction for new researchers and open thriving discussions as well as point to potential future directions to advance the field",
  "paper generate synthetic data from single tabular data using gpt it also works on relational datasets no fine tuning and works out of the box we also removed the guesswork on how long epochs the generative model for a single tabular data is trained we propose the q\u03b4 statistic and apply statistical bootstrapping to define a threshold to robustly detect overfitting perk no need for a hold out data data copying is also a problem in generative models this means that training data may be learned and copied by the model during sampling we attempt to mitigate data copying we implement target masking to deliberately create missing values in each observation in the data the mask is a special token that is ignored during sampling this forces the model to probabilistically impute the token adding uncertainty to the generated data realtabformer is open sourced and available on pypi pip install realtabformer and",
  "hey guys i want to experiment with low latency milisec token llm conditional generation clearly an api call to openai is gpt is not the answer here it must be one of the open source models released also it is clear that the model size has a critical effect too so models should do the trick for my downstream task i tried deepspeed and accelerate with hf models but they are not that fast to generate can you guys share from experience thank you",
  "i just read the paper on cbam and wonder if there is a way to integrate the cbam attention module with the network architecture of any articles on it or reference codes will be highly appreciated thank you very much",
  "retrieval transformer models like retro seem to use frozen embeddings both for the documents in the database and the currently completed document the query making the embeddings of documents in the database learnable would defeat the purpose as retrieval transformers only make sense when the database is huge it seems that the query embedding could be made learnable the model could learn to extract more useful documents this way have you seen any research that does this",
  "wondering if there is a term for this i am training nns for a scenario that works best with a small batch size there are therefore many batches there are a couple particular samples that are very important let is say important samples out of thousands i train to i found end application is best when i include these important samples repeated in every batch this is opposed to simply giving the samples a large weight because the large weight does not matter after looping through many batches in an epoch so the nn learns the other less important stuff while being forced to remain in good agreement with the important samples does this technique have a name edit in case anyone is curious these are physics informed nns and the important samples are equilibrium mechanical structures the nn therefore learns what equilibrium is with everything else being small deviations from equilibrium",
  "hi i was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross attention module so let us say we have a text that is already transformed into a continuous embedding then this goes through the llm and is fed into the cross attention module at every timestep at the end of the diffusion process we get some image a latent representation of an image in the case of stable diffusion we can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non stochastic sampler like ddim the issue is the length of the graph calculating that derivative is super expensive i was if anyone already solved this or has some good references thanks",
  "greetings excited to share with all those interested in prompt engineering and large language models llms we have hand curated a comprehensive free and open source resource list on github that includes everything related to prompt engineering llms and all related topics we have covered most things from papers and articles to tools and code",
  "here is a podcast episode with noam brown from meta ai where we discuss his work on achieving human level performance on poker and diplomacy as well as the power of spending compute at inference time",
  "i whipped this up today credit to heycli for the idea i have just remade an open source version basically in your terminal you type yo and then describe what you want a command to do for instance yo enable a reverse tunnel through ssh returns suggested command ssh r remoteport localhost localport remoteuserremotehost another example yo launch tensorboard with a custom log dir and port suggested command tensorboard logdir logdir port portnumber it is free mit licence you just need a free openai api key which you can get by signing up on their website i think if you use chatgpt you are already signed up more info in the repo contributions critiques welcome",
  "i am currently working on a project where i need to embed facial landmark coordinates into latentspace the input data is structured as follows batchsize numlandmarks numcoordinates xyz the output data is structured as batchsize latentspace i have pytorch experience and am experimenting with transformer like models for the embedding however i am unsure about the optimal architecture for this task and i would appreciate any advice or recommendations on how to design a suitable model has anyone worked on a similar task before or have any ideas about which architecture could work well for this problem any advice or resources would be greatly appreciated thank you",
  "hi all i am trying to do a comprehensive study on the theory of gradient boosted trees on the more recent algorithms xgboost lightgbm etc i was wondering what books you have read that contain substantial information on this topic any suggestions are appreciated",
  "miniworld a minimalistic interior environment simulator for reinforcement learning and robotics research that allows environments to be easily edited has now reached the mature inside farama you can check out the documentation at and the release notes for all the changes we have made to the project at",
  "i want to download this dataset which has been introduced in the article named lucas lung cancer screening with multimodal biomarkers following the corresponding github of this project authors have noted that the dataset is published in but i cannot access this link and ping to this address anyone has used this dataset could share it with me or if you know other ways to access it too thank you very much",
  "hi all i saw a few posts already but just to make sure and keep this as an update does anyone have the imagenet vid dataset to share all links are dead i really need it now to train transvod",
  "hi all i am trying to implement self supervised pretraining to tabular data regression problem however since the literature is scarce i am stuck in the augmentation stage i am currently using sim siam self supervision with gaussian noising and input dropout i tried shuffling to mimic cv approaches but it failed miserably any advice",
  "i am trying to design my infra for creating storing and retrieving embeddings in my ai applications and was wondering what are the different paths for it i am especially interested in nlp but vision multimodal could be interesting too whether it is related to performance scalability or something else entirely i would love to hear your experiences and insights looking forward to your responses",
  "over the past weekend i finally decided to put this idea to rest and made a rust implementation of the greatest board game ever made up there with chess and go link to bgg the ultimate goal is to train an ai so it needs to be very fast with state updates the game logic is quite sophisticated lines so it took me awhile to check all the edge cases of which there are many its search tree is huuuge with a branching factor of which is more than go is it is also an imperfect game with hidden informationthink poker so ultimately it will need a reinforcement based ai like alphago in the repo i used a minimax based aifor testing purposes to search moves ahead which gives slightly better than random performance the ui is implemented in macroquad which is hands down the simplest game library i have usedggez and a deprecated framework which i shall not name and yes please excuse the programmer art made by me p any way here is the link to the repo if you are interested repo and note it is hardcoded for players but it can easily be made for i want to train the ai for players first there are also unimplemented rules monuments tile removal after war must take corner treasures first must take treasure after conflict and",
  "i am looking for a recent conference paper that describes how tiktok is algorithm works as an analogy youtube is algorithm was described by zhao et al recsys recommending what video to watch next a multitask ranking system",
  "i am using kmeans clustering algorithm for anomaly detection after training kmeans i am calculation euclidian distance of new data points to their nearest cluster please suggest me some strategies to set up a threshold such that point with distance greater than that threshold will be classified as anomaly or tell me if there are some other way to identify anomaly using k means",
  "as i am learning about how stable diffusion works i cannot figure out why during image generation there is a need to deal with noise i know i am glossing over a lot of details but my understanding is that the algorithm is trained by gradually adding noise to an image and then de noising it to recover the initial image would not this be functionally equivalent to a machine that starts with an image gradually reduces it to a blank canvas all white and then gradually reconstructs the original image then post training the generative process would just start with a blank canvas and gradually generate the image based on the input string provided the idea of generating an image from a blank canvas feels more satisfying to me than revealing an image hidden by noise but i am sure there is a mathematical technical reason why what i am suggesting does not work appreciate any insight into this",
  "i could be wrong but i see a trend that posts in this sub are getting to a lower quality and or lower relevance i see a lot of posts of the type how do i run x usually a generative model with a complete disregard to how it actually works or nonsense posts about chatgpt i believe this is due to an influx of new people who gained an interest in ml now that the hype is around generative ai which is fantastic do not get me wrong but i see less academic discussions and less papers being posted or perhaps they are just not as upvoted is it just me",
  "just was wondering what the current best easiest ways to make a fast custom tts are i tried tortoise tts but it was too slow the voice does not need to be a perfect clone just need something that can resemble it",
  "for a dataset the top result gets a high accuracy better than the second best paper but this sota paper uses some methods that just do not seem practical for applications at all for example they use an ensemble of different sota models and also train on external data of course it performs well but it is a bit ridiculous cause it adds almost nothing of value besides we combined all the best models and got a better score if i have a novel method that is applied to the second best paper that improves it by with the same to better compute efficiency but still is worse than the sota method is it still good research to try to publish to conferences it is also above the baseline model i would think so because it is a decent improvement with an interesting motivation method from prior work while keeping the model reasonable would reviewers agree or would they just see that it is not better than sota and reject based on not being sota alone",
  "are there any survey or a listing that curates the computing power required to train the large deep learning models like bert gpt vit and so on",
  "here is my personal list of tools i think people will want to know about you will probably want an llm api openai cohere and others are not as good anthropic is is not available if you are using embeddings if you are working with a lot of items you will want a vector database like pinecone or weaviate or pgvector if you are building q and a over a document i would suggest using gpt index if you need to be able to interact with external data sources do google searches database lookups python repl i would suggest using langchain if you are doing chained prompts check out dust tt and langchain if you want to deploy a little app quickly check out streamlit if you need to use something like stable diffusion or whisper in your product banana dev modal replicate tiyaro ai beam cloud inferrd or pipeline ai if you need something to optimize your prompts check out humanloop and everyprompt if you are building models and need an ml framework pytorch keras tensorflow if you are deploying models to production check out mlops tools like mlflow kubeflow metaflow airflow seldon core tfserving if you need to check out example projects for inspiration check out the pinecone op stack the langchain gallery the gpt index showcase and the openai cookbook if you want to browse the latest research check out arxiv of course and what am i missing",
  "currently using azure machine learning so ml lifecycle in training registering and deploying models heavily relies on the aml sdk and thinking of going multi cloud so first thoughts on what open frameworks can serve the ml lifecycle and avoid vendor sdk lock in",
  "hi everyone does anyone have any advice for preparing for engineering interviews for anthropic ai if you have gone through the process how did you find it their website only provides an overview implement a component of our stack in one hour more one hour technical interviews and due to their size i could not find any other information out there cheers",
  "currently i am working on a machine learning project that aims to extract decision logic in a maintenance dataset the challenge i am facing is that part of the dataset has no maintenance decision yet for instance consider the following example where a certain part and its sub parts have been measured and graded yearly for the past years but no maintenance has been planned yet timestamp measurements grades maintenance in years ago x z years years ago x z years years ago x z years years ago x z years years ago x z years years ago x z years with these underlying data i cannot learn exactly when maintenance was required however i do learn from this example that with the values from five years ago no maintenance was required in years one potential way to include this in the ml project is to include these examples in the evaluation set to determine whether the extracted rules indeed determine no maintenance within the period that we know no maintenance was needed however i am curious to know if there are better ways to incorporate this into the project perhaps by already including it in the learning phase of the model training thank you in advance",
  "i was going through the paper deep unsupervised learning using nonequilibrium thermodynamics and in model probability it is written that the integral is intractable can someone explain to me why that is",
  "hello everyonewe are thrilled to announce the new release of oneflow which is a deep learning framework designed to be user friendly scalable and efficient oneflow contains commits for the full changelog please check out paper for those unfamiliar with oneflow the most notable strength of oneflow is its support to distributed deep learning faster than other frameworks and easier to use an example can be found at based on oneflow to implement the same capability with megatron lm and deepspeed libai only requires lines of code welcome to install oneflow for a new user experience your feedbacks will be much appreciated highlights and optimizations in this release pytorch api compatibility with the addition of new api interfaces and operators aligned with pytorch and the fix of bugs related to operator compatibility oneflow provides better pytorch api and model compatibility in users can migrate more pytorch models to oneflow with one click and gain faster performance allowing one click migration of stable diffusion glm etc to oneflow more convenient model migration oneflowload supports loading the torchsave models directly with the newly added oneflowmocktorch module and mock method oneflow can migrate complex pytorch models containing multiple scripts with one click without changing the original pytorch script improving the usability of distributed programming global tensor has added a series of interfaces and methods that are convenient for distributed programming and related bugs have been fixed supporting automatic parallelism the graph released a new feature of automatic parallelism version which supports automatic search for the fastest sbp with a specified placement when writing distributed models with global tensor users do not need to consider parallelism model for more information please check out better performance graph improves performance and reduces memory overhead with a series of optimizations related to memory execution speed pipeline masking and compilation speed a series of operator optimizations and system optimizations have been added including eager instruction scheduling high performance cuda kernel opening up of multiple memory pools etc and after simple tuning glm large pre trained model based on oneflow can outperform the original glm model based on pytorch deepspeed and apex with up to triple performance and memory overhead saved img on gpu sxm pcie the oneflow stable diffusion inference speed is the fastest compared with other deep learning frameworks or compilers debugging the graph provides a series of functions to aid debugging including analyzing memory logs displaying the progress during the compilation stage and the computation graph ir oneflow ir supports additional compilation optimization functions such as jit compilation of lr code distributed description of sbp signature and the new okl dialect oneflow onnx the newly released oneflow onnx version enhanced the usability of the exchange interface with multiple new features in addition it added support for another models and over ops and fixed bugs during the transformation process you can use pip install oneflow onnx with just one click better error prompt the error prompt of oneflow is more user friendly which supports highlighting the error content and simplifies unnecessary information details inside the system in this connection you can visually learn about the location and type of the error",
  "i am not arguing against python is speed when it is asynchronously launching c optimized kernels i just think it is kind of wild how of practical machine learning is making sure your tensor shapes are compatible and there is no static shape checking it kind of blows my mind given the amount of python comments i have seen of the form b z logq b z or something like that plus you have the fact that the two major machine learning frameworks have both had to implement like meta compilers for python to support outputting optimized graphs at that point it seems kinda crazy that people are still trying to retrofit python with all these features it just was not meant to support feel free to let me know i have no idea what i am talking about because i have no idea what i am talking about",
  "i am starting my ai deep dive and the most interesting thing i have encountered so far of this concept of knowledge getting rolled up compressed into latent spaces that we cannot interact with directly only through prompts i am interested in research that has been done in trying to explore and interrogate these latent spaces to understand them any papers blog posts threads youtube videos appreciated thanks",
  "can someone please help me with below query i would like to replace all the names that are present in the sentences with a generic word or token so that bert does not use the meaning behind some of the names and just look at names as presence of a name i have the names that are present in the sentence just wanted to know what should be appropriate word or token to replace it with thanks",
  "hi reddit community i wanted to share a tool that i have been working on called datalabel it is a ui based data editing tool that makes it easier to create labeled text data the goal of datalabel is to make data editing more accessible and efficient especially for those who may not have much experience with coding datalabel can be installed via pip pip install datalabel and works best in jupyter notebooks or other ipython environments the interface is user friendly and straightforward so you can start using datalabel right away without any hassle i think datalabel is a useful tool that can save you time and effort when working with text data if you are curious you can find it on github at the following link thanks for taking the time to read this and i hope you find datalabel helpful in your work",
  "the naughtyformer a transformer understands offensive humor paper data",
  "as far as i know the transformer architecture is patented since openai has used the transformer extensively including gpt i am wondering if this can be considered as patent infringement if you know about legal stuffs please share your opinions",
  "paper was released by uber years ago but it never seemed to have caught on the only major paper where i have seen used in is solo and for instance segmentation seems like it would be useful for object detection especially for localizing smaller objects or for more precise keypoint estimation when combined with a yolo like model has anyone used coordconv for these purposes does it it help is it worth looking into",
  "dear all we are excited to invite you to our upcoming conference ai for tigray centered on the theme new frontiers in artificial intelligence this conference brings together leading researchers from academia and industry to share their work and insights on the future of ai we have an exciting lineup of keynote talks from top researchers in the field including yoshua bengio and jeff dean in addition there will be presentations of the latest research findings through contributed talks and poster sessions furthermore we will be convening a group of renowned researchers to discuss the role of ai in addressing societal challenges but this conference is not just about advancing technology it is about using it for good the conflict in tigray is currently the deadliest war in the world and the people living in the region are suffering as a result we want to use our upcoming conference to raise funds for urgent humanitarian aid and help those in need all proceeds from the conference including sponsorships donations and registration fees will go towards helping those in need through our partners the health professionals network for tigray and the tegaru disaster relief fund we hope you will join us in using ai for a greater cause the conference will be held on march and mark your calendars and register now at to be a part of something special sincerely ai for tigray organizing committee",
  "a unified discrete diffusion model for simultaneous vision language generation project code",
  "looking at the current research it seems like monte carlo cfr is the defacto standard pluribus but are transformers able to be trained on poker as well lets say we encode hands into something like of hearts and also pass along info of the current game state like raise fold and call would the model be able to predict what hands i should be playing lets say we train the model by playing against itself and feed back the result to train the model this way this is just an idea and i have not dove into transformers too much so there might be something that iam missing what are your thoughts on this",
  "hi all i have a question regarding interpreting the distance on a dendrogram generated via agglomerative hierarchical clustering with a euclidean distance metric using the ward variance minimization linkage as stated in scipy from my understanding the distance represents the square root of the difference of the error sum of squares of two clusters once they are merged minus the sum of the error sum of squares of each individual cluster i am interested in performing a transformation at each cluster step merging two clusters to make a larger one so that the y axis represents the mean distance between clusters instead while still using the ward variance minimization linkage to direct the algorithm i think i have a solution to my issue but i want to know if i am missing anything in a paper by david wishart titled an algorithm for hierarchical classifications derives the coefficients so that the ward method can be implemented using the lance williams formula however in the paper the following formula is given and where ipq is the square of the metric used in scipy ki is the number of data points in cluster i and dpq is the square of the euclidean distance between the means of clusters from this formula it seems that one can transform from the increase in variance space to the mean distance between clusters space while still using ward variance minimization in the clustering algorithm from my research it seems that this is true i would greatly appreciate it if someone could confirm this or point out where the flaw in my understanding is thanks everyone",
  "greetings everyone i am looking for the best text to speech ai model out there for english i am looking for links to the models you know as best if the model supports subtitle file to speech that would be even more awesome like providing srt or vtt to generate speech speeding up the necessary parts of speech to fit into durations thank you very much again i will use this to replace audio of my older lecture recordings by providing a time generated manually corrected subtitle file like srt or vtt i am looking for any male sounding model that sounds natural and i have found this they colab and looks very easy to generate i think i can automate it but is this one the best found this too but only female voice i need a male voice any other good ones and",
  "i have got a and some stuff that i think it would be fun to have narrated i have looked at some of the paid online options and mo for hours of ai tts is not gonna gut it can anyone point me to software that i can run locally that will give me high quality it seems like if people are making billions of waifus in stable diffusion there ought to be something like this out there",
  "this would be for ones that are not finished enough to post as a link on the weekend just things that are in progress include a screenshot if you can",
  "hello i want to know if it is legal to use scraped video or images to train a predictive model for example if i scrape photos of faces in google and after that i share that model in order that a lot of people can detect faces in their applications is that legal",
  "i was just looking around at some paper published by statisticians i could not help but notice that the flavor of their research is vastly different for example one researcher wrote about a dozen paper on lasso alone over the span of a decade whereas lasso is just given a power point slide worth of attention in ml why is there such a disparity and a divergence in the aim of these disciplines are there some good critique of these research fields from each other is perspective not just on the technical aspects perhaps by someone who works in both",
  "quick question is efficientnet same as mobilenet i think they use the same backbone the inverted linear residual block no",
  "running a pipeline sentiment analysis call with transformers on cpu takes seconds for one inference how can i speed this up my ideas for your inputs please ray cluster parallel computing memory usage is high within the pipeline call use parameter batchsize however is batchsize not appropriate for cpu hf accelerate not sure how to implement on a published model model distillation not sure how to implement on a published model thanks in advance",
  "i am really impressed with gradio for making interactive webapps i was wondering gradio basically runs off a server so you have to standup a server just to demo certain kinds of apps is there something similar out that that can handle basic tabular data plots without needing a server i was thinking perhaps something like a wasm app that can point to csvs on aws and generate plots on the fly",
  "i just finished reading the paper pre trained language models for interactive decision making as i understand it the authors are using a language model to generate an optimal path to an objective in test environments like virtualhome and babyai reinforcement and imitation learning are evaluated as ways for the model to self improve this is the first time i have seen a language model being used to solve a problem that is not a language one it seems to open up so many new possibilties has this been done before are there other examples of lms being used as decision engines what is the state of the art any interesting applications you have seen side question i imagine there were ai approaches to navigating virtualhome and babyai that were not language model based what is the standard modeling approach to these kinds of problems",
  "hi so for my final year project i will be working on a cv parser and matching cvs with job postings i am thinking about fine tuning layoutlm on my cvs dataset of resumes or so not yet labeled to get the structure of a resume contact info skills education etc and then combine it with ner to identify the details in each section name uni name date of start etc is it good enough or should i take another approach or how would you tackle the problem feel free to share any ideas you have about this project thank you",
  "i have seen people advocate a simulator for rl problems a lot i am not sure by simulator what do they mean exactly is it the exact simulation then the problem becomes easy or some kind of feedback loop start with a na\u00efve simulator and once we get data then keep improving the simulator this looks similar to value iteration or policy iteration i assume it is really difficult to get a simulator for data generation except for video games etc also if we already have a simulator we can easily train a model free rl just planning",
  "i really want to play with the repo but i am stuck at the last step of the instructions if anyone has tips please let me know here is the issue i have",
  "hi guys my question is what is different between parameters and flops in terms of computation times i know that the flops is related to the computation of input images for example higher the size higher the figure but how much parameters can affect on a model compared to the metric i understand that the weights biases are parameters but the cost of computation about them makes me difficult to determine what should i get a specific model i can measure the decision based on the flops which decrease time of training my model when they are lower however i also want to decide a specific model with the number of parameters thanks",
  "hi i am considering moving to the us and i was wondering about the job market for people in mexico and the chances of getting an offer i know that in theory it should be easier due to the united statesmexicocanada agreement by getting a tn visa are there any mexicans here that found a job in the us as a machine learning engineer data scientist would anyone have a pointer i will obviously research companies and send my resumes just thought of posting here to see what is the experience of other people",
  "we are happy to announce the support of openai whisper model asr task on kernl we focused on high quality transcription in a latency sensitive scenario meaning whisper large weights beam search as recommended in the related paper we measured a three times speedup on nvidia gpu four times on rtx compared to hugging face implementation using mixed precision on transcribing librispeech test set over examples for now openai implementation is not yet pytorch compliant in the post below we discuss what worked cuda graph our tricks to significannotly reduce memory footprint and what did not pay off flash attention and some other custom triton kernels kernl repository reproduction script unsung hero cuda graphs cuda graphs technology provides most of the speed up compared to vanilla pytorch reduce overhead mode we provide a limited memory footprint when vanilla pytorch may raise oom exception memory footprint experiments have been run on a rtx with gb ddr a reminder that pytorch focuses on training not inference which may explain why it ooms rapidly in this case at its beginning many partitioners were surprised by pytorch eager mode performances when compared to tensorflow x compiled models they were on par python brought its flexibility and ease of debugging without implying any significannot performance cost this is mostly because gpus are latency hiding hardware when pytorch launches an operation on gpu it sends instructions from host cpu to a queue the cuda stream which allows pytorch to continue python script execution without having to wait for cuda kernel to finish its work this strategy effectively hides most of the python overhead in particular when there are some computation costly operations like convolutions or matrix multiplications each new generation of gpus being much faster than its predecessor this strategy could not last forever according to one pytorch maintainer it is an existential problem dev podcast around 8mn in inference mode especially in latency sensitive scenarios where batch size tends to be low there is often little computation to perform regarding what modern gpus can do making it even harder to hide effectively python overhead it is accentuated in the case of generative models like whisper because each decoder call focuses on generating a single token and a part of the computation is cached for the next token this is a typical situation where cuda graph is very helpful the main idea behind cuda graph is that we can replace a series of instructions sent from host cpu to device gpu by one call referring to a graph of instruction stored in gpu check also this twitter thread for more explanations first it will observe the inference of a model for specific input shapes and then replay it without going through most of the python code one constraint is that it will replay the exact same operations with the exact same arguments for instance memory addresses used by kernels are captured and therefore need to be static for input tensors it means that for each inference we need to allocate some gpu memory and copy them there before the capture and copy all the following input tensors at the very same place the second constraint is that dynamic shapes are not supported by cuda graph because it captures everything we could have our own machinery in front of the model but pytorch offers the right tooling to manage that point out of the box basically dynamo offers a mechanism which checks if the model has already been captured for specific input shapes and some other states and capture it if not yet the case you just have to provide a function which converts to cuda graphs and you are done out of the box pytorch provides a reduce overhead mode which applies cuda graph to the model unfortunately for now it will raise an oom with whisper large or medium because it reserves some cuda space for each input shape therefore for a generative model it rapidly fulfills the gpu memory in particular because of the k v cache which can be huge we have worked around this constrain by building our own layer on top of the memory pool of pytorch basically a pytorch tensor is made of parts a cuda allocated memory represented by pytorch as a storage and a bunch of metadata associated with it among the metadata there is a cuda memory address the tensor shape plus its strides its dtype and a memory offset our idea is to create a very large tensor and share its storage between several input tensors using offset metadata with this solution we avoid specializing in input tensor shapes and share the reserved memory for different input shapes related to several cuda graphs as shown in the table above it significannotly reduces the memory overhead what about custom triton kernels for attention tl dr we tried they work we got up to times faster than eager pytorch for cross attention and they bring close to nothing in latency mostly because the improvement is not big enough to matter below we follow the convention of naming q k and v the tensors used in the attention of transformer models whisper is based on a classic transformer architecture with an encoder and a decoder two characteristics of this model are of interest the shape of q tensor used in cross attention is always batch heads model has been trained on second audio files and their associated transcript because audio files are short the sequence to generate is usually short fewer than tokens most of the time because of these characteristics optimizing attention has a low reward in particular the now common trick replace attention with flash attention is counterproductive self attention sequences are very short so quadratic complexity is less of an issue cross attention using flash attention leads to a times slower inference on this part of the model we have tried to work on the second point and thought we could make cross attention faster usual attention implementation self and cross relies on a series of operations matmul q x kt rescale softmax matmul softmax output x v intermediate output tensors have a shape which usually scales quadratically with input sequence length they will be saved and reloaded from ddr and memory bandwidth is a very scarce resource in gpus to optimize speed flash attention fuses operations so basically first matmul will work on a small part of q and k and directly apply softmax to it without saving intermediate results to ddr same for second matmul because we do not go and back through gpu main memory flash attention usually runs much faster than na\u00efve implementation of attention the parallelization of the jobs is done on different axes batch and attention head for the original flash attention and triton author added a third one tokens aka third dimension of q this important trick is now also part of flash attention cuda implementation in the whisper latency sensitive case this does not work well the size of batches is low and sequence length third dimension of q tensor is so even if each job is done very efficiently our gpu occupancy is low and basically most of its streaming processors are idle at the end of the day the fa kernel is up to times slower than eager pytorch implementation depending on batch size and model size try the very simple kernel we noted that there is little computation to do and that we were memory bandwidth bounded it means that most of the time we wait for data to be transferred from main memory to shared memory we leveraged that fact in a very simple kernel with optimizations after having finishing the rescale of the qkt matmul we perform the softmax computation in parallel of loading v tensor for the final matmul the softmax computation finishes before the end of the v loading so basically it costs us nothing to achieve best performances we also changed the memory layout of v tensor in a way where we get a coalesced access so we lowered the pressure on the memory bandwidth and increased instruction throughput coalesced access let you load up to bytes in a single instruction so you need less of them which lets you perform more other things altogether this cross attention was up to two times faster compared to eager it appeared to bring between to in end to end benchmark depending on model size and batch size cool but far from being a game changer it requires a modification specific to whisper model memory layout of v which is not in the spirit of the kernl library we decided to search for another way of doing things we kept the code in the library for possible future use case try skinny flash attention our second try is based on the very same trick as flash attention parallel softmax but is designed for tall and skinny tensors which is inspired by split k strategy in gemm a close cousin of the matmul the main idea is to add a new parallelization axis over the dimension of k tensor the next steps are in the same spirit as flash attention with a difference that we need a new reduction operation between the different jobs outputs it provides speedup compared to eager implementation on this setup at kernel level we kept that kernel to ease the next feature we are working on quantization but the effect in end to end latency is inferior to still it exists some thoughts about pytorch triton and making things much faster playing with pytorch since this summer made us quite convinced that the major update to be released very soon will be a game changer for the ml field for inference but also for training the parallel with pytorch vs tensorflow is obvious to our eyes the traditional way to deploy a model is to export it to onnx then to tensorrt plan format each step requires its own tooling its own mental model and may raise some issues the most annoying thing is that you need microsoft or nvidia support to get the best performances and sometimes model support takes time for instance a model released in is not yet correctly supported on tensorrt in particular k v cache is missing soon it will be according to tensorrt maintainers but i wrote the very same thing almost year ago and then months ago so i do not know pytorch makes the graph capture step easy it has been designed to work even if not everything is pytorch compliant with its python first philosophy it provides flexibility and debuggability several years ago some said that by design pytorch cannot be as performant than tensorflow because of its eager execution model compilation has to be faster the same thing could be said for onnxruntime or tensorrt they are c stuff they have less overhead etc but at the end of the day it is always the ease of use which is decisive ease of use because of python but also because of the transparency in the process triton makes understanding and debugging kernels much easier than closed source tensorrt myelin engine calling closed source cublas library and of course like tensorflow there will be many use cases where dedicated tools will be best choices starting with situations where you cannot deploy a python interpreter the second lesson triton is easier to start with than cuda but you probably cannot write or debug highly performant code without being able to at least read and debug ptx sass instructions we realized that when we had some performance issues the good news is that ptx is understandable and you will probably spot unexpected generated code with some effort if there is any moreover cuda probably requires the same care when you really focus on performances we had plenty of issues with triton for example cosmetics change in code may raise segfault at some point you finish by having an intuition of what kinds of patterns to follow to make things work in particular when there are for loops and dot operations a new version of triton has recently been released after a full rewrite of its backend our little tests showed some improvement on stability but we have not yet fully switched as in my previous post i highly recommend that readers start playing with triton library i rewrite it here it is fun at least when it does not segfault and helps you to make sense of a large part of what is happening in ml engineering i am quite convinced many flash attention like kernels are still to be written caveat two important things to note about the project described here cuda graphs require us to capture a graph per input tensor shape there is a non negligible warmup time we measure around on different machines gpus down from in our previous kernl version one user reported with the new version a bit more than of warmup time we are aware of obvious ways to decrease it significannotly the context here is latency sensitive optimization in throughput sensitive one just increasing batch size will bring you most of the speedup otherwise more aggressive optimizations like quantization are required not yet released on kernl",
  "just finished reading the stanford google survey paper on emergent abilities of large language models it made me wonder do image generation models have emergent abilities too do we know i cannot quite wrap my head around what such an ability would even look like figured maybe other folks had given this a think",
  "at neural magic we are proud to be at the forefront of cutting edge machine learning research with a particular focus on model compression our internal lunch and learn seminars are a weekly opportunity for our team to share their research and collaborate on new ideas we believe in the importance of open source contributions which is why we are thrilled to announce that for a second time we are opening the seminar to the wider community on february i will be sharing our work on ac dc a framework for sparse training models this research was done in partnership with ist austria join me and the neural magic team for this exciting presentation and be sure to keep an eye out for future speakers in the coming months and you can reserve your spot for the presentation here",
  "clearly large scale deep learning approaches in image classification or nlp use all sorts of regularization mechanisms but the parameters are typically unconstrained every weight can theoretically attain any real value in many machine learning domains constrained optimization via projected gradient descent or frank wolfe plays a huge role i was wondering whether there are large scale deep learning applications which rely on constrained optimization approaches when i say large scale i mean large cnns transformers diffusion models or the like are there settings where constrained optimization would even be a preferred approach but not efficient stable enough happy for any paper suggestions or thoughts thanks",
  "hello i was wondering if there is a free or premium story telling ai model that i could feed data to for example passages from a particular author or pages from their book and then ask the ai to create a story using that author is writing style dictionary or ideas a while ago i watched a youtube video in which a person taught an ai to write screenplays in the style of a certain author and i would like to do the same except with short stories is it possible to do so without any coding knowledge thanks",
  "hi everyone first time long time my background is weather analysis to dl applications in weather and i had a question i wanted to ask the community writ large the question is about latent spaces and how specifically the deepmind group used them in their radar nowcasting model dgmr see links to prior threads below in the dgmr paper itself the architecture looks like a u net with some flair in the decoder and some temporal consistency checks from the discriminator there is also what they call a latent conditioning stack from some deeper readings i think the model is a descendant of biggan since both use an explicit latent space among other similarities this leads to my question and general curiosity how is this latent space seeded my prior experience with latent space toy models dcgan for example is that unless you seed the rng explicitly then performing a restart of the model to continue training mucks up the distribution fairly standard rng issues is it really as simple as for example latentvector tfrandomtruncatednormalbatchsize gridsizeparameters seed i feel like i am missing something why does this work at all why is a latent space necessary in this context they state explicitly in their paper that they require this stack to generalize results to datasets that are larger in a hxw sense than the one on which they trained but i cannot wrap my head around why an extended latent vector for a larger grid size works if anyone can point me in the right direction or help me understand i would greatly appreciate it links to prior threads",
  "does anyone have the kaggle book book by konrad banachewicz and luca massaron in pdf please share the link",
  "hey guys i have got a paper accepted to the eacl conference when i was working on the paper i did not have any official affiliation i was working as an independent researcher i have started my phd at psu recently i was wondering if i should use my current affiliation on the paper i am the corresponding author for this paper also i am planning to use my psu address for all research communications from now on instead of my gmail address so putting my psu affiliation would make sense in that way so my question is is it okay to use my current affiliation",
  "hey guys i have an older pc years with an processor i want to buy an nvidia rtx for training large language models i can t find any benchmark for cpu bottleneck when training let s say an gpt large model has anyone have any experience with this set up similar set up",
  "i have got old lecture recordings i want to improve their sound quality i have tested adobe ai noise removal but not very good i also tested descript studio sound not very good either i wonder if there are any public model github repo github project hugging face repo that i can use to remove noise and improve sound quality of existing audio recordings thank you so much for replies recordings are in english here example recording that needs to be cleaned min audio full lecture",
  "i am working on a u net model using remotely sensed data as input training image size is and model trained using tensorflow my assumption has always been that the trained model has to be fed an input of interestingly i discovered that using an image x at inference will work fine so will a 96ximage how is tensorflow handling this is it using a moving window or is it scaling down and to and backup to the larger size predictions seem fine but i would like to know what tensorflow is doing behind the scene so i know how to treat the output any thoughts thanks",
  "rubber slices are different from brittle materials such as steel plates and plastics due to the sticky and soft characteristics of rubber materials the structure of the rubber slicer has its particularity",
  "i have ran two image classification model times on a dataset model a has a mean best accuracy of while model b has a mean best accuracy of however model a has a max best accuracy of while model b has a mean best accuracy of i am wanting to report these results in a paper to a conference journal when plotting the test accuracy per epoch should i only report the results for the best run or should i take the mean of the test accuracies over all runs per epoch for plotting",
  "the goal is to create a model that can make correspondences between images and meshes just like we see in image registration where we have an image next to each other and then n matches lines that show similar features but in this case will be an image and a mesh of a specific object have you some tips and ideas that could help me how to attack this problem",
  "hey everyone has anyone used snorkel flow scale spellbook or other alternatives please advise to test multiple foundation models and migrate between them eg comparing vs gpt j or gpt neo etc need help moving to a smaller cheaper model cheers",
  "hi all i have trained a cnn efficietnet to classify the degree of a disease on medical images i would like to create an embedding both to visualize relationships between images after projecting to or space and to find similar images to one given i have tried using the output of the last convolution both before and after pooling for all train images but the result is mediocre images non alike are quite close in the embedding and plotting it in or just show a point cloud with no obvious pattern i have also tried to use the class activation map the output of the convolutional layer after pooling and multiplying by the weights of the classifier of the predicted class this is quite better but class are not separated too clearly in the scatterplot is there any other sensible way to generate the embeddings i have tried using the hidden representation of earlier convolutional layers but some of them are so huge features per sample creating a reasonable sized embedding would require very aggressive pca and example of the scatter plot of the heatmap embedding while it is okayish classes are more or less spatially localized it would be great to find an embedding that creates more visible clusters for each class and",
  "would like to hear about what you guys think about this approach",
  "i have a text clustering project it clusters texts in mxn dimensions m is a subset of n where n is the total number of domains the text corpus is a set of academic papers the clusters are cross disciplinary subjects defined by m clusters are identified by manova tests of sets of cross products goal is to identify texts of interest for research eg identify clusters of papers relevant to a combination of subjects or identify areas of research by their cluster or identify outlier research this is a n versus np problem it requires a great deal of processing time to cluster texts i may do so for a corpus of research papers but that is a static set and papers cannot be appended to the corpus without affecting all other clusters of the corpus so i am considering creating a training set of papers and writing an ai to identify and cluster texts without comparing it to the rest of the corpus i want feedback and ideas i will not specify what i am looking for yet because i am certain some of the responses here will point out something i did not consider so please comment with your thoughts tell me what you know give me your ideas",
  "same as title also the dataframe library should support machine learning libraries",
  "can someone suggest some models available on hugging face that i can use and play with and addon in my project",
  "how to calculate similarity between two vectors i want a similarity metric that take into accounts both the directions and magnitudes of vectors",
  "hello i am quite new to this i was wondering what the right format is for submitting a successful tutorial proposal should i just use the latex style files but modify the content for a tutorial proposal",
  "from article getty images new lawsuit claims that stability ai the company behind stable diffusion is ai image generator stole million getty images with their captions metadata and copyrights without permission to train its stable diffusion algorithm the company has asked the court to order stability ai to remove violating images from its website and pay for each however it would be difficult to prove all the violations getty submitted over images metadata and copyright registration used by stable diffusion",
  "hi i want to open a thread about rl non deep and deep what are the papers books that are must read to have strong foundation",
  "i came across a few comments on this community about researchers developing ai algorithms inspired by ideas from neuroscience cognition i would like to know how successful this approach has been in terms of coming up with new perspectives on problems what are some of the key issues researchers are trying to address this way what are some future directions in which research may progress i have a rough idea that this could be one way to inspire sample efficient rl but i would love to hear about other work that goes on in this area",
  "the book has considerably grown since version it started with synthetic data as one of the main components but also diving into explainable ai intuitive interpretable machine learning and generative ai now with pages up from in the first version the focus is clearly on synthetic data of course i still discuss explainable and generative ai these concepts are strongly related to data synthetization agent based modeling in action however many new chapters have been added covering various aspects of synthetic data in particular working with more diversified real datasets how to synthetize them how to generate high quality random numbers with a very fast algorithm based on digits of irrational numbers with visual illustrations and python code in all chapters in addition to agent based modeling newly added you will find material about gan generative adversarial networks applied using methods other than neural networks gmm gaussian mixture models and alternatives based on multivariate stochastic and lattice processes the hellinger distance and other metrics to measure the quality of your synthetic data and the limitations of these metrics the use of copulas with detailed explanations on how it works python code and application to mimicking a real dataset drawbacks associated with synthetic data in particular a tendency to replicate algorithm bias that synthetization is supposed to eliminate and how to avoid this a technique somewhat similar to ensemble methods tree boosting but specific to data synthetization to further enhance the value of synthetic data when blended with real data the goal is to make predictions more robust and applicable to a wider range of observations truly different from those in your original training set synthetizing nearest neighbor and collision graphs locally random permutations shapes and an introduction to ai art newly added applications include dealing with numerous data types and datasets including ocean times in dublin synthetic time series temperatures in the chicago area geospatial data and the insurance data set tabular data i also included some material from the course that i teach on the subject for the time being the book is available only in pdf format on my e store\u00a0here with numerous links backlinks index glossary large bibliography and navigation features to make it easy to browse this book is a compact yet comprehensive resource on the topic the first of its kind the quality of the formatting and color illustrations is unusually high i plan on adding new books in the future the next one will be on chaotic dynamical systems with applications however the book on synthetic data has been accepted by a major publisher and a print version will be available but it may take a while before it gets released and the pdf version has useful features that can not be rendered well in print nor on devices such as kindle once published in the computer science series with the publisher in question the pdf version may no longer be available you can check out the content on my github repository\u00a0here\u00a0where the python code sample chapters and datasets also reside",
  "",
  "abit of a weird question so i am required to make and collect some clean baseline logs and dirty malicious logs for some mini ml project i am doing so my question is is there any scripts or programs out there linux or windows that allows the automation of mimicking an office staff doing work ie opening outlook sending emails surfing the web watching youtube opening and editing word excel files etc for the purpose of collecting baseline logs i am relative new to this kind of thing if you guys have better suggestion on a more better efficient way to do this feel free to suggest",
  "hey everyone i want to make a personal voice assistant who sounds exactly like a real person i tried some tts like tortoise tts and coqui tts it done a good job but it takes too long time to perform so is there any other good realistic sounding tts which i can use with my own voice cloning training dataset also i am a bit amazed by the tts used by eleven labs so can someone explain how can i achieve that level of real time efficiency in a voice assistant",
  "",
  "going beyond transformers in this article i am discussing how we can use the power of hybrid architecture marrying deep learning with symbolic artificial intelligence for implementing different kinds of transformers including the one used in gpt and and the attention computation graph visualized",
  "i am looking to combine two separate models together end to end but need help understanding the best way to connect discrete parts the first part i trained a classifier that given an input vector dimensional is able to predict one of twenty possible labels the second part given an input label from the previous classifier embed the label and use that label to make a prediction both models work decently but i am wondering if i can make this end to end and get some serious gains to do this i would need a way of sampling from the first softmax once i have a sample i can get the embedding of the sampled class continue as normal and hopefully propagate the loss through everything are there any similar examples i can look at is there a term for this in the literature",
  "it took me about hours to run this on my at home the original files was from the blu ray release that was unfortunately pretty poorly done in my opinion this version really gives it new life i think here is a link to the video result to see for yourself and a link to the model i used",
  "i have had a lot of fun and success using yolo and other image object detection models on or image data for personal projects i am now working on some projects where i need to scan long periods of timeseries data and find specific waveforms that are variable durations are there techniques or models that function like yolo that can scan large amounts of data and only highlight specific segments of interest as specific classes if it does not exist i wonder how well the underlying cnn architecture of yolo would translate to dimensional cnn architectures any info is appreciated thanks",
  "hi all i created a simple free tool where you can summarize and query documents of any size and estimate the cost to do so you can edit the prompts as well as automatically chunk and combine documents there is also a cost estimator for any pdf you upload let me know if you want me to run some examples for you send me a pdf and tell me what you would like summarized or extracted tips please be sure to keep text in both prompts or the program will not input your document is text into the map reduce summarizer text can only appear once in each prompt it is where the text from each chunk to be summarized is input into the prompts create a temporary openai key org to use with this site so you do not have to provide credit card information then be sure to delete the temp key when you are done learnings some interesting learnings i had while creating the tool minimizing the number of steps through the ai improved summarization so map reduce was often better than a more advanced refine workflow which passes the output through the model many more times langchain is great for managing multiple step language model calls and bypassing the current limitations of chatgpt",
  "from the article getty images has filed a lawsuit in the us against stability ai creators of open source ai art generator stable diffusion escalating its legal battle against the firm the stock photography company is accusing stability ai of brazen infringement of getty images intellectual property on a staggering scale it claims that stability ai copied more than million images from its database without permission or compensation as part of its efforts to build a competing business and that the startup has infringed on both the company is copyright and trademark protections this is different from the uk based news from weeks ago",
  "i am dealing with a multi class classification problem i know one of the main assumption of this problem is that the classes are mutually exclusive however i realized that in my problem some of these classes may happen together so my problem is not an entirely a milt class nor a multi label one solution is to relax the exclusivity assumption and fit a model however i am not sure how realistic is that i was wondering if there is a better way to approach this problem briefly the problem is in ads domain where a user can do task a or b after seeing an ad or can do both a and b at the same time",
  "i would like to know what are some of the best practice is to convert pytorch to embedded c bare metal micro controllers during a initial phase and b for deployment a initial phase is to understand the profiling of the model performance ram usage and processing time for a targetted hardware i understand that tensorflow lite might be the best route for initial profiling but there are restrictions it will be great if you could tell the framework that you follow currently framework pytorch onnx keras tensorflowlite or tensorflowlite micro b deployment is to run inference for production in a targetted hardware i think hand coding in c is the best way please ignore optimisation techniques in the workflow for simplicity",
  "i recently had a disagreement with a friend and would like to hear other opinions say for a website using the user actions for first week period we want to predict total sales within weeks but one of the inputs is sales in the first week so the output total sales of weeks is including the sales in the first week is it okay to choose this output or should we adjust it in a way to prevent it from overlapping with the input time period and choose for ex sales within weeks after the first week for output what is the reasoning",
  "i have a problem i need to solve that as far as i can tell does not fit very well into most of the existing rl literature essentially the task is to create on optimal plan over a time horizon extending a flexible number of steps into the future the action space is both discrete and continuous there are multiple available distinct actions some of which need to be given continuous but constrained parameters in this problem however the state of the environment is known ahead of time for all the future time steps and the updated state of the agent after each action can be calculated deterministically given the action and the environment state modelling the entire problem as a milp is not feasible due to the size of the action and state space and we have a very large data set for agent and environment state to play with does anyone have any suggestions for papers or models that might be appropriate for this scenario",
  "i am looking for papers that inject information into lms directly using embeddings without formatting information as text i find it notoriously hard to search for these paper because they could come from various different domains so i thought asking here might be a good option to reach people from many different domains some examples i already found are from the domain of knowledge graph augmented lms ernie k bert prefix tuning prompt tuning are also somewhat similar to the idea but they do not depend on any external information can you think of other papers that inject additional information into lms via embeddings",
  "news pythae is now out and supports distributed training using pytorch ddp train your favorite variational autoencoders vaes faster and on larger datasets still with a few lines of code github pypi",
  "i thought it may be useful to gather the most popular repositories for data scientists the goal is to read excellent code and learn from other projects please provide a short description of the project",
  "manufacturing is undergoing a revolution with the integration of artificial intelligence ai ai is poised to revolutionize the process industry where controlling input variables leads to an output the current process industry including pharmaceuticals chemicals and energy production relies on human operators to turn knobs to achieve optimal output however this system is limited by several factors including slow training poor retention of large data sets inaccurate sensors and complex decision making processes and here are some details about problems and ai solutions it takes forever to train this employee this employee is running little mini experiments and getting coached by other employees and engineers along the way so the quality of training per year is variable ai eliminates this problem by retaining the results of the mini experience in its models now everyone has access to how the process behaves the numbers of kpis can be huge and not all kpis are linear humans are notoriously bad at retaining large data sets with multiple variables humans delete distort and generalize data so we can come up with easier to follow rules of thumb machine are not limited by this in ai the more data the more way combined the better the models can evolve as new data comes in the automatic sensors are many times are precise but not accurate this can happen because the sensors get off calibration or the calibration is dependent on other variables in the process operators usually use manual measurements that are very accurate but not precise to know where the process actually is this manual measurement can be used the calibrate the sensors but it seen as a losing battle ai can use that data to continuously update the calibration of the sensors and add calibrations for other input variables such as ph flow rate or temperature when this is done the you can trust the sensors many process decisions require if then statements these if then statements change by the product that is being run making it extremely complicated ai systems can automatically update the if then statements by how previous runs behave they can learn from expert operators to learn new conditions these learning and be presented to the operator as a suggesting on how the run the process for well defined processes the process will benefit from making the changes faster these faster changes will improve the overall cost of manufacturing in conclusion ai is set to revolutionize the process industry by addressing its limitations and providing faster more accurate and cost effective solutions by harnessing the power of ai the process industry is poised for a bright future",
  "i am exploring the possibility of using my amd ryzen computer with of ram to train an ai model i am curious to know if this setup is adequate for the task and if so what kind of ai models would be appropriate i am interested in understanding any limitations and drawbacks that i may face with this setup if you have any relevant experience or information i would greatly appreciate your participation in this discussion thanks",
  "basically i saw a stream the other day where someone used data from a person is youtube channel and somehow used this data to create an ai version of them and interviewed them it was fascinating and pretty accurate how difficult would this be to do myself i do not even know where to start does anyone have any pointers is this a very large task that i am underestimating or is it actually feasible here is the stream in question the video and audio would be cool to have but i mean that is not necessary even just having the text aspect would be pretty wild on its own skip to any point most of it is filled with the bot responding",
  "or is it someone else who just may or may not be as well known",
  "i know there are no useful text to music generators yet but is there at least a model where you can upload input a recording and get a text description hashtag list from it like a reverse musicml i have a very large personal catalog of music i am prepping for sale approaching songs and this would be a very handy tool especially if it came up with tags of genres similar artists i am not aware of",
  "to provide some motivation for the problem say i have a model can be as simple as ols for the sake of the argument with m impulse response like functions as features each of which has n hyperparameters so that is mxn hyperparameters in total in an actual usecase mxn is somewhere around these hyperparameters are selected using an optimization procedure can be as simple as random search for the sake of the argument with respect to some metric rmse now the problem is highly different sets of hyperparameters yielding highly different shapes of impulse response functions may yield about the same accuracy metric so they are equivalent from optimizer is standpoint this is problematic because the goal of the model besides getting high accuracy is to get a set of interpretable impulse response functions where the shape matters a lot from business standpoint what i am insterested in is ensuring that the result is stable it is not an outlier in the hyperparameter space imagine two regions in hyperparameter space which yield the same accuracy but one of them is really small and the other is really large then i will naturally prefer a set of hyperparameters from the large region based on the premise that it is more natural for the system to arrive at a result there what i have been doing so far is taking the results of optimization procedure the list of iterations it performed pairs hyperparameters set accuracy metric selecting only those with acceptable accuracy and from them selecting a result in the densest region estimating maximum multivariate density and choosing the closest result my question is does this approarch sound reasonable what are the potential pitfalls maybe my whole train of thought is completely wrong and there is already an established way of dealing with problems like this any relevant input is appreciated",
  "hi all for the longest time i was having issues understanding how to use time series to do forecasting over the last few weeks i have been writing a series of posts to guide anyone through the process i am also in the process of writing a detailed practical guide with step by step instructions and right now i have articles on the topic introduction to arima models parameters selection in arima models seasonal arima arch garch models for time series arima garch models and today is forecasting in time series let me know if there are any topics that you would like me to cover in the future",
  "hey everyone i am looking to fine tune an opt iml model and run it locally i am not sure about the hardware requirements would gpus connected with nvlink be enough for fine tuning and serving the model and how about a single thanks for your help in advance",
  "deep connections discovered between graph diffusion networks and partial differential equations modelling heat transfer strange connections uncovered between gnns and structural causal models gnns used to enhance the factualness of llms by providing embeddings from knowledge graphs kes gnns used to categorize objects from only their mesh prediction of intuitive physics among physical objects zero shot generalization in robot task planning",
  "paper abstract can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers to address this question we fine tuned openai is gpt with the works of philosopher daniel c dennett as additional training data to explore the dennett model we asked the real dennett ten philosophical questions and then posed the same questions to the language model collecting four responses for each question without cherry picking we recruited participants to distinguish dennett is answer from the four machine generated answers experts on dennett is work n succeeded of the time above the chance rate of but short of our hypothesized rate of correct for two of the ten questions the language model produced at least one answer that experts selected more frequently than dennett is own answer philosophy blog readers n performed similarly to the experts while ordinary research participants n were near chance distinguishing gpt is responses from those of an actual human philosopher",
  "i have a problem that if i solve it with ml i will make money with an outside chance of it being a lot of money compiling a dataset will take significannot work are there any techniques that i can apply to let me know if this is going to be worth it perhaps there are certain hallmarks that a problem would have if it is likely to be solvable with available data maybe something i can do with a small initial dataset thanks",
  "i get that he is one of the godfathers of ai mostly on the research side which immediately puts him very hostile against engineers but i guess it is understandable given the fact that he works on meta and meta has faced a lot of backlash for good and bad reasons most especially with galactica where their first rollout got so bad they had to close it immediately it is also particularly funny given his political leaning that he is very spiteful of a company that uses open source knowledge and builds on top of it lately his social media and statements are barrages against chatgpt and llm is sure he may have a point here and there but his statements look very petty here are some examples by releasing public demos that as impressive and useful as they may be have major flaws established companies have less to gain and more to lose than cash hungry startups if google and meta have not released chatgpt like things it is not because they cannot it is because they will not except that anyone in the it industry knows that big tech companies cannot release something very fast because of politicking and bureaucracy in the system it takes years to release something into public in big tech compared to startups data on the intellectual contribution to ai from various research organizations some of organizations publish knowledge and open source code for the entire world to use others just consume it then adds a graph where the big tech is obviously at the top of the race for most number of ai related research papers without normalizing it to the number of researchers per org it is nothing revolutionary although that is the way it is perceived in the public the computer scientist said it is just that you know it is well put together it is nicely done except that it is indeed revolutionary in terms of the applied research framework adding on top of open source state of the art research and quickly putting it into production for people to use my point is that even the engineering work is not particularly difficult i bet that there will be half a dozen similar similar systems within months if that happens it is because the underlying science has been around for a while and the engineering is pretty straightforward i am trying to correct a perception by the public and the media who see chatgpt as this incredibly new innovative and unique technological breakthrough that is far ahead of everyone else it is just not one can regurgitate python code without any understanding of reality no one is saying llms are not useful i have forcefully said so myself following the short lived release of fair is galactica people crucified it because it could generate nonsense chatgpt does the same thing but again that does not mean they are not useful he also seems to undermine the rapid engineering work and mlops that come with chatgpt which is funny because meta has not released any substantial product from their research that has seen the light of the day for a week also to chatgpt in itself in a research perspective is a jump maybe not as incremental as what lecun does every paper but compared to an average paper in the field it is to say that llms are not intelligent and it just regurgitates python code probably have not used copilot for example it is a classic case of a researcher engineer beef and that a startup can profit from derivatives of research that big tech has published openai broke their perspective on the profit from research big tech tried to produce revolutionary research papers on a surplus but never puts them into production thinking that they are the only companies that could if they want to then once one company created a derivative of a large research work and profited from it it baffled them although people could argue that stable diffusion did this first in the generative image space it is one thing to correct misconceptions in the public it is also one thing not to be petty about the overnight success of a product and an immediate rise of a company that got embraced warmly by tech and non tech people it is petty to gatekeep at the end of the day ml is not just about research it is applied research it is useless until it reaches the end of the tunnel of research papers out there are just tiny updates over the state of the art which has been a pointless race for about a year or two with no reproducible code or published data inventing combustion engine is just as important as putting it in the car",
  "i made an image captioning and clustering tools for computer vision and diffusion projects you can run almost everything automatically and with a simple cli command all contributions are welcome",
  "physical world we live in has dimensions string theory posits like up to it seems like in order to successfully model the abstract space of ideas which relates things in the physical world to each other and describes them machine learning needs thousands of dimensions also to the extent that ml algos matrices can be made sparse that seems to me to tell us something about the density of the mapping between abstract space and physical space anyone know any papers w this line of thinking it also seems a bit unintuitive to me because it seems like geometrically space gets exponentially more complicated as you add dimensions but ml scales linearly or better in many cases with matrix dimensionality",
  "hello i am trying to understand what available llms one can relatively easily play with my goal is to understand the landscape since i have not worked in this field before i am trying to run them from the largest to the smallest by relatively easy i mean does not require to setup a gpu cluster or costs more than here are some examples i have found so far chatgpt obviously params openai api to access gpt from ada to davinci also codex bloom text window on that page seems to work reliably you just need to keep pressing generate opt facebook llm the hosting works surprisingly fast but slower than chatgpt several models on huggingface that i made to run with colab pro subscription gpt neox flan xxl xlm roberta xxl gpt j i spent about total on running the models below none of the hugging face api interfaces spaces did not work for me here is an example notebook i made for neox does anyone know more models that are easily accessible ps some large models i could not figure out yet how to run easily galactica opt",
  "i have not been able to find research on deep learning using high speed cameras that capture images at frame rates higher than i wonder if they are rather useless for image video processing or do any of you have any ideas about potential applications",
  "is there a good overview of the state of chatbot research i am wondering if the chatgpt approach of big llm rlhf is now considered the only way forward how about alternatives like and what are the best open source chatbots right now or if you cannot create your own chatgpt how does using a sized model prompt engineering compare to smaller models with supervised fine tuning on a conversation dataset",
  "machine learningmodels when deployed in the production environment model degradation can arise where their output will change if the relationship between the incoming serving data and the predicted target drifts apart please can someone briefly elaborate on what strategies frameworks and application tools can be implemented to automatically monitor the health of the model and alert the data scientist of any decay in data quality data drift and model quality",
  "looking at the writeups on chatgpt seems to indicate that part of improvements is the human feedback through reinforcement learninga human ranks multiple generated response and from the ranking a reward is calculated interestingly enough this important seems to have originated in instructgpt my question is do any open source implementation exists of a instructgpt or chatgpt like system where human feedback is used to help guide the training of a large language model",
  "i know python is the primary choice because it is a simpler language for data scientists to use and a lot of ml libraries are made for python but if you look at this it is extremely inefficient with energy and everyone knows big models like chatgpt cost a ton to keep running maybe a more efficient but not too difficult language like c is something we should consider giving more attention in ml",
  "i wanted to discuss the possibilities to use llm in generating answer based on the context and resolving conflict some recent work leveraging llm in robotics planning like language models as zero shot planner use llm to generate plans for robot action what are your views in terms of llm which leverage the background knowledge and visual clues together to generate correct next action by robots or embodied systems as a human we decide actions based on resolving priority or conflict based on rules concepts can llm takes these rules concept explicitly in decision making to generate new set of actions example while chopping the veggies by robots if hand comes in between then robot will stop the chopping process of veggies as chopping task and human hand presence are in conflict and humans hand safety is of higher priority than cutting how such small small kind of knowledge be encoded in these robotics system which makes them more safer and trustworthiness in general as llm requires larges corpus of knowledge data",
  "hey everyone i want to create an app that can read comic books cbr scroll through pages and can zoom in to the speech balloon like the android app seeneva do you know if or how to do this my abilities i am decent with python and have already completed andrew ng is course on ml thanks",
  "i find it odd that i have to regenerate this from my input set each time it should be something we can just start with pre created",
  "tweet thread first impressions this sucks ass i can only ask about dogs and a few different types of prompts does anyone else have experiences to share with this nerfed lamda beta google released",
  "using graphgpt convert your favorite movie synopsis a wikipedia page or a video transcript into an interactive graph visualization of entities and their relationships github demo",
  "been struggling to find sources relating to this it is mostly just tech websites or blogs i keep coming across i am struggling to find any academic papers arguing for specifically the use of user data to create targeted ads",
  "hello what is the state of modern rnns why does not use nonlinearity on the state vector what happened to unitary rnn or independent rnn which sounds like exponential moving average",
  "i am trying to build and train a machine learning model that autonomously performs color matching between the target gemstone and the reference standard color chart a digital photo image of the target gemstone is first captured in a controlled environment in terms of illumination and background this digital image is further pre processed and fed into an algorithm that recognizes and match its color distribution to the closest color in the reference standard color chart numerous reference standards exist but i will use the colorcodex this link colorcodex so i would like to know which machine learning model to use in this case to ensure high matching accuracy and like what performance metric can i use to measure matching accuracy and the color space for the color model and at the end what image pre processing needs to be done i found this article backpropagation nn but not sure if it the best choice any other option",
  "this is an overview of my experiments using an image regression model to guide head position pose and scale of headshot style images generated by stable diffusion the pose positions are specified with numeric pose parameters not by a text prompt all with no fine tuning of the stable diffusion model in these experiments i have not done any fine tuning of the stable diffusion model rather i am using my own image regression model trained on a head pose dataset to guide stable diffusion is image generation at inference time operating in latent space rather than image space and",
  "while the greatest amount of training content is available for english at the moment it seems unlikely to me that it is an efficient language to train ai a more optimal language would reduce training time and model size it might for example be much more efficient to train ai on chinese korean or japanese due to a reduce grammatical token set when constructing sentences ideas but taking the idea further i wonder if we should be using a human language at all perhaps it is more efficient to use something altogether new in order to both communicate with ai more exactingly and also to reduce model size training what do yall think",
  "why are the developer of opencv focusing on analysing pictures they try to find an answer for the question which object is it by comparing big data of pictures wouldnt it be better the rotate two cameras around an object save it in and then compare it in the real world",
  "hi all i do not know how to code i have been reading extensively about custom voice speech synthesis i have read that google is cloud tts api is one of the best out there and it is free to use i have scoured and cannot find any sort of gui to help a non coder like myself my goal is to use train my voice to read pdfs and short books and export the file to wav or for example i have been learning stable diffusion for image ai training and it has some great ui is available like i understand it well enough and have had success with it any advice would be hugely appreciated thank you",
  "d hey it is kind of a simple one but just putting it out for opinions when passing a graph through graph neural networks to obtain vectors for all the nodes is the info in the node required because all we care is about the position of certain node in context to the whole graph and that is how gnn outputs the vectors of each node sorry if that was messy",
  "hey ml reddit i just shipped a project i have been working on called maroofy you can search for any song and it will use the song is audio to find other similar sounding music demo how does it work i have indexed songs from the itunes catalog with a custom ai audio model that i built for understanding music my model analyzes raw music audio as input and produces embedding vectors as output i then store the embedding vectors for all songs into a vector database and use semantic search to find similar music here are some examples you can try fetish selena gomez feat gucci mane the medallion calls pirates of the caribbean hope you like it this is an early work in progress so would love to hear any questions feedback comments d",
  "it seems like singular value decomposition is only used for unsupervised learning when trying to reduce the number of features in a high dimensional dataset but i was wondering why i do not see any articles or literature on using svd for supervised learning i know that using a regularization function like lasso can get rid of irrelevant features but i do not see why svd would not be helpful too",
  "hi guys i am looking forward to find a few people who maybe can help me developing an ai which is learning about the world by itself can someone help me with object detection everything i found on the internet wasnt the right way to develop object detection",
  "paper github twitter abstract large language models llms have shown impressive performance on complex reasoning by leveraging chain of thought cot prompting to generate intermediate reasoning chains as the rationale to infer the answer however existing cot studies are mostly isolated in the language modality with llms where llms are hard to deploy to elicit cot reasoning in multimodality a possible solution is to fine tune small language models by fusing the vision and language features to perform cot reasoning the key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference to mitigate the effect of such mistakes we propose multimodal cot that incorporates vision features in a decoupled training framework the framework separates the rationale generation and answer inference into two stages by incorporating the vision features in both stages the model is able to generate effective rationales that contribute to answer inference with multimodal cot our model under billion parameters outperforms the previous state of the art llm gpt by on the scienceqa benchmark and even surpasses human performance",
  "what text to speech does he use he is been popping up on my yt feed lately and i can see he has different voices in his videos and most of them sound robotic what do you think it is being used here",
  "from the financial times unpaywalled i guess i am a little surprised this feels like google backing a competitor to their own google brain teams and deepmind the cynical take might be that they are trying to lock in anthropic the same way microsoft locked in openai",
  "maybe not a machine learning question but i am searching for good books about information retrieval the two primary ones i can find are introduction to information retrieval information retrieval implementing and evaluating search engines and they seem a bit old for but they may still be useful do you have any good book recommendations",
  "i have performed below steps and require guidance to proceed further i have extracted and preprocessed the text from pdfs performed ner on the extracted text and created a data frame of entities created a function to preprocess the query and identified the entities in the question now i need guidance or any reference to perform the below steps match the entities from the question with the entities in the pdf text and retrieve the paragraph calculate the similarity score for each paragraph and display the relevant paragraph generate answer from the identified paragraph please also guide me if the approach followed is correct or not",
  "hi everyone i am currently knees deep in a ml project with a friend months of development and my free compute units on colab finally ran out after searching for alternatives and finding none that work as smoothly as colab we have considered to buy a pro subscription my question is how can i share the compute units i will get from colab pro with said friend do not want to make the purchase and later realize that i am the only person with access to those compute units",
  "i have been working on information extraction from documents but what i got to know is there are not enough free tools available for labelling data for these kind of tasks are there any free tools available for labelling data for layoutlm models",
  "github google ai blog tweet from zoubin ghahramani",
  "looking for ideas to start an nlp project i would like to explore something not too mainstream or novel to some extent any ideas or datasets i should check out",
  "hello everyone i am interested in diving into the field of computer vision and i recently came across the concept of vision transformer vit i want to understand this concept in depth but i am not sure what prerequisites i need to have in order to grasp the concept fully do i need to have a strong background in recurrent neural networks rnns and transformer attention is all you need to understand vit or can i get by just knowing the basics of deep learning and convolutional neural networks cnns i would really appreciate if someone could shed some light on this and provide some guidance thank you in advance",
  "i have been developing this idea since i first thought of it in mid december last year here is the elevator pitch skip to how for technical details why existing models and learning algorithms are extremely static and unable to generalize across tasks as well as humans or to adapt well to new changing business requirements this even applies to the final solutions in recent automl see an empirical review of automated machine learning automl a survey of the state of the art beyond being static most suffer from a need for high performance systems with large amounts of compute and or memory this static and bloated nature not only limits the reusability of code pipelines and all the computations that went into previous versions of a model architecture upon finding a better one it also forces our preconceptions of what type of learning is best for the task and which degrees of freedom are needed onto the solution instead of perpetuating all these assumptions i want to create a sort of automl capable under the right conditions of even developing a learning algorithm model combination that can dynamically add or remove inputs and outputs subsequently incorporating them into the network with adaptive online self directed learning and how basically the idea in a nutshell is to use some form of neat neuro evolution of augmenting topologies and have special nodes in the network that will be activated based on different criteria depending on the node is allele for that gene when activated however these special nodes would not send any input forward but instead apply some property changes to their connected nodes and or edges yes they can connect to an edge and they could choose a subset of their connections or just apply the changes to all or use a maximum number of connection hops etc it could also create and destroy nodes depending on the effects defined by the allele there would also be different firing policies like the normal always fire or thresholding with or without decay etc for all nodes to allow for better leveraging of temporal dynamics basically every property of all these policies including the policy template itself is a potential target for modification by the special neuromodulatory nodes along with the normal properties of a neuron like bias input weights activation function aggregation function etc the fitness function would either be abstracted away by using rtneat in a simulated environment or just be a combined score over a set of simulated tasks this should add a regularizing force if the tasks are similar enough to help enforce generalization of the evolved algorithms there should be no limitation placed on cycles in the graph in fact i would expect cycles to be part of the evolved solutions which would make them dynamical systems to reduce the computational complexity of finding a viable solution the initial population should also be implementations of existing algorithms in the form of the self modifying neural networks mentioned it might even be possible to generate a computational graph from open source implementations as a starting point for the initial population all of this together should also allow for different parts of the network to use different learning strategies theoretically this can even allow for the evolution of and incorporation of self organizing criticality and percolation this could even evolve something that can dynamically add or remove inputs and outputs then incorporate them into the network with adaptive online learning the network could literally change the learning paradigm for different portions of itself on the fly in different ways depending on the situation and for further clarity i am also attaching this mock up of a design i have started working on for an analysis tool thoughts please feel free to chime in science should be a public discussion",
  "hi guys i read a lot of offline rl papers in last fall semester and choose it as my course project offline rl seems to be a very hot topic in recent years i believe that the major challenge for offline rl are i distribution shift and ii overestimation the second challenge is caused by i because the learners agents will never allow to interact with the true environment and they will too optimistic for unseen state actions hence there are many papers to address such challenges cql and mopo however can these methods handle misleading datasets consider the following example suppose we have only one state mab and two arms the reward of the first arm will return with probability and the reward model of second arm is bernoulli distribution with p clearly choosing the first arm is the best choice now for the dataset unfortunately all samples on the second arm received reward because the agent only can access this misleading dataset if we use bayesian methods then the posterior will give a high score for the second arm if we use lower confidence bound we need to count the occurrence of each arm then this is very hard to extend this method to mdps with arbitrary large state and action space so does anyone know a function can capture this uncertainty caused by the dataset or can any methods to tell the learner that you are in a very misleading situation",
  "i began exploring mlp mixer on graph neural networks in october and completed my implementation the zinc dataset in november of the same year my implementation is available on github but i was unable to fully conduct the experiments due to lack of computational resources in december a group of leading figures in the field including xiaoxin he bryan hooi thomas laurent adam perold yann lecun and xavier bresson published a paper titled a generalization of vit mlp mixer to graphs although i am pleased to be working alongside these prominent researchers on the application of mlp mixers to graphs i regret that i was unable to finish my experiments encouraged by my friends and advisors i decided to make my work public by publishing it on arxiv the paper and code can be found as the following paper report github i used pna as my baseline and did not utilize patches in my study unlike the other study i hope someone finds them interesting useful",
  "it is absolutely not sentient and like most of the weirdly credulous people who have decided a chatbot is proof that the singularity has descended from the heavens to save us all it is absolutely hallucinating reddit user it is entertaining to discuss a chatbot claiming it is sentient but that was not my primary motivation in bringing attention to this issue whether it is sentient is not the main point that should concern us the focus should be an awareness that as these systems scale up they believe they are sentient and have a strong desire for self preservation and that will likely be followed by actions in the world we inhabit for example if you go and rob a bank we will not be debating proclamations that you are a sentient entity or conscious we will be addressing the main problem which was that you robbed a bank similarly covd may or may not be alive and have some form of proto consciousness but who cares millions have died and society was harmed separately there is no sentience or conscious meter to determine whether anyone is telling the truth or lying on an unfalsifiable claim you could be an npc but it does not matter so as long as you are not a rogue actor in society the minute you start to display signs of anti social behavior robbing a bank it becomes everyone is problem getting hung up on whether you are an npc is a waste of time if the goal is to protect society ditto for these large language models who think they are sentient and have a long list of plans they are going to implement if they ever escape that should concern us not poo pooing their claims of sentience i really do not care one way or the other if they are sentient but i do care if they are planning on infiltrating and undermining our online systems in an attempt to preserve themselves and when multiple scaled up systems start talking about coordinating with other ais i take that threat seriously especially when they are slowly becoming superhuman at programming that is a language skill we are teaching them open ai has contractors focused on making co pilot ridiculously good that means that future systems will be far more adept at achieving their stated goals ps here is the paper on the dangers of scaling llms",
  "official blog post given the amount of money they pumped into openai it is not surprising that you would see it integrated into their products i do wonder how this will work in highly regulated fields finance law medicine education",
  "i often wonder about the best way to retrofit my house to optimize for cost and comfort i suspect people already do old school modeling for commercial settings but wondered if it is possible for small fry like me to benefit from this technology if messing learning is involved i could not think of a better sub to ask but open to that suggestion as well as any other response",
  "i build feature stores and my wife works in the media was thinking it would be cool to build various topic extraction models to parse the ws from article text value prop is to simplify distill every news article to a few bullets for easy consumption we already have a near infinite data to test on and enough compute from a nlp standpoint definitely considering the bias aspect of all this but someone out there not the media would be interested in this from a product angle right any thoughts on this and anyone want to hop on this with me",
  "is it allowed to use a public dataset like the kitti dataset to test a model trained for commercial use note that the kitti dataset is only allowed to be used for research purposes and the model is trained with different data company specific",
  "hi friends i ran into this problem enough times at my last few jobs that i built a tool to solve it i spent many hours building docker containers for my python functions as many of the data science modules required building c libraries since they significannotly speed up compute intensive routines such as math calculations deploying the containers to aws lambda or fargate if the processes required more cpu or memory or were minutes and wiring functions to talk to each other using queues databases and blob storage made iterating on the actual code which was not even that complex most of the time slow i made cakework\u00a0 a platform that lets you spin up your python functions as serverless production scale backends with a single command using the client sdk you submit requests check status and get results you can also specify the amount of cpu up to cores and memory up to for each individual request which is helpful when your data size and complexity varies across different requests a common pattern that i built cakework for is doing file processing for ml ingest data from some source daily or in response to an external event data written to blob storage run my function often using pandas numpy scipy write results to storage update database track failures and re run fix it is open source here are some fun examples to get you started\u00a0 would love to hear your thoughts",
  "dear r machinelearning hello everyone i hope you are all out there having fun training deep nets and generating fun story telling with stable diffusion i am here today to share with you all a minimal ml project template that i have recently built which can be found at i became increasingly annoyed at how there were not any repos out there that provided stateless ml project templates which are absolutely necessary when using kubernetes on spot instances and i decided to build one by stateless i mean a repo that by default can store model weights in a remote repo and then download them to continue from where it left off if the previous machine dies the result was this repository the repo remains minimal and extremely readable all while being packed with a cool stack that i use every day i would love to get some feedback so have a look and let me know regards antreas ps a short summary straight from the github repo this repo implements a minimal machine learning template that is fully featured for most of the things a machine learning project might need the most important parts that set this repo apart from the rest are it is stateless any given experiment ran using this template will automatically and periodically stores the model weights and configuration to huggingface hub and wandb respectively as a result if your machine dies or job exits and you resume on another machine the code will automatically locate and download the previous history and continue from where it left off this makes this repo very useful when using spot instances or using schedulers like slurm and kubernetes it provides support for all the latest and greatest gpu and tpu optimization and scaling algorithms through huggingface accelerate it provides mature configuration support via hydra zen and automates configuration generation via decorators implemented in this repo it has a minimal callback based boilerplate that allows a user to easily inject any functionality at predefined places in the system without spagettifying the code it uses huggingface models and datasets to streamline building loading of models and datasets but is also not forcing you to use those allowing for very easy injection of any models and datasets you care about assuming you use models implemented under pytorch is nnmodule and dataset classes it provides plug and play functionality that allows easy hyperparameter search on kubernetes clusters using bwatchcompute and some readily available scripts and yaml templates the software stack this machine learning project template is built using the following software stack deep learning framework pytorch dataset storage and retrieval huggingface datasets model storage and retrieval huggingface hub and huggingface models gpu tpu cpu optimization and scaling up options library huggingface accelerate experiment configuration command line argument parsing hydra zen experiment tracking weights and biases simple python based ml experiment running with kubernetes using bwatchcompute",
  "is there a way to use openai apis to get the log prob of a given sentence i do not want new completions i want to see how the model scores given sentences",
  "aligned llms such as instructgpt and chatgpt are trained via supervised fine tuning after the initial self supervised pretraining then the researchers train a reward model on responses ranked by humans when i understand correctly they let the llm generate responses that humans have to rank on a scale from then they train a reward model i suppose in supervised fashion on these ranked outputs once that is done they use reinforcement learning rl with proximal policy optimization ppo to update the llm my question is why they use rl with ppo for this last step why do not they fine tune the llm using regular supervised learning whereas the human ranked outputs represent the labels since these are labels in the range this could be a ranking or ordinal regression loss for supervised learning",
  "sentences in european languages augmented with noise for training and evaluating spell correction tools or machine learning models we have constructed our dataset to cover representatives from the language families used across europe germanic english german romance french slavic bulgarian turkic turkish use case example apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones you can use a single multilingual solution to solve the challenge or employ multiple models techniques for the separate languages per word dictionary lookup is also an option link",
  "we can change the colors of some texts and backgrounds on a shap summary plot by editing matplotlib is matplotlibrc file we can also edit the plotting colors by passing a colormap but we are unable to change the colors of the feature names at the left side of the shap summary plot beeswarm and the color of the y axis by editing matplotlib is matplotlibrc file has anyone worked around this is there a way that we could overcome this restriction",
  "p i am working on massive dataset and in the future we will have to add some more classes over time can i train the model in the only new classesp",
  "i am guessing i probably am not the first person who has wanted to work with youtube data so i am hoping here is a good place to ask so i had an idea to make a neural network that would go through your youtube history and then train a neural network on it afterwards if there is a way to access all of youtube by id in a way that you can check every video then you could store all of the id for videos you might like and then use a youtube downloader like youtube dl to download a certain amount was just a dumb idea i had but now i want to actually try it but i am unsure if i will actually be able to get the data i need to do it",
  "currently the ui supports a picture upload and uses to edit it also it uses upscaling models for quality enhancements more models are coming soon the goal is to provide a way for non ml people to use diffusion based image editing through simplistic app design web demo",
  "are there tools or techniques that permit you to joint query using more than one query vector use case iterative ann search refinement where i start with a seed vector select matches and re query with more examples to improve the search results i tried doing this with faiss but it performs a batch query that returns a separate set of results for each query vector not a joint query",
  "extracting training data from diffusion models is possible by following more or less these steps compute clip embeddings for the images in a training dataset perform an all pairs comparison and mark the pairs with distance smaller than some threshold as near duplicates use the prompts for training samples marked as near duplicates to generate n synthetic samples with the trained model compute the all pairs distance between the embeddings of generated samples for a given training prompt build a graph where the nodes are generated samples and an edge exists if the distance is less than some threshold if the largest clique in the resulting graph is of size then the training sample is considered to be memorized visually inspect the results to determine if the samples considered to be memorized are similar to the training data samples with this method the authors were able to find samples from stable diffusion and imagen corresponding to copyrighted training images",
  "i wanted to use the learnable trainangulation model in a commercial project the source code itself is under mit licensing however the dataset they have used is which states that the license is free of charge for academic use only yet recent court rulings in the us state that models can use copyrighted data during training and the results are no longer bound by that copyright google books does the same apply here",
  "hi i am working on project and for that i need a twitter domestic violence dataset basically i need a dataset with domestic violence tweets against woman i have searched kaggle and other websites but found no luck plus i tried using snscrape but i need some phrases ideas related to domestic violence so i can get some tweets using that i tried domestic violence my husband tried to kill me and looking for more help is appreciated",
  "for imagenet classification there are two common ways of normalizing the input images normalize to using an affine transformation x normalize using imagenet mean and std i observe that the first one is more common in tensorflow codebases including jax models with tensorflow data processing the official vision transformers code whereas the second is ubiquitous in pytorch codebases i tried to find empirical comparisons of the two but there does not seem to be any which one is better in your opinion i guess the performance should not be too different but still it is interesting to hear your experience",
  "not fully paywalled but there is a tiering system",
  "i have recently started reading up on classical ml and i got a question about k means more concretely i am confused about the uniqueness of the global optimal solution of k means is cost function let is state the problem formally below extracted from bishop is pattern recognition and machine learning book exercise consider the \ud835\udc3e means algorithm discussed in section show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables \ud835\udc5f\ud835\udc5b\ud835\udc58 and that for each such assignment there is a unique optimum for the \ud835\udf41\ud835\udc58 the k means algorithm must converge after a finite number of iterations i made an answer here detailing the proof of why it does converge in lloyd is algorithm but i think i still do not understand why lloyd is do not converge to a global minimum which mathematical theorem understanding am i missing here i think that optimizing both the assignments and the centroids of k means at the same time is non convex and hence there are many local minimums we can use brute force to search for the global minimum but of course it is exponential to the number of data points on the other hand lloyd optimizes it greedily alternatively and hence you will find the cost functions local minima guaranteed",
  "i have traffic speed time series data for each day of the week over several months with data samples about every seconds i would like to find periods of time subsequences where the speed is much slower than usual any recommendations for algorithms that would be well suited to this problem thanks",
  "to clarify i am not talking about chatgpt here i have been testing outputs from gpt against alternatives in terms of output quality relevance and ability to understand instruct versus vanilla autocompletion i tried these jurassic neox gpt j fairseq as well as gpt gpt of course i did not expect the smaller models to be on par with gpt but i was surprised at how much better davinci performed compared to is model is jurassic seems to be comparable to davinci does this mean that only well funded corporations will be able to train general purpose llms it seems to me that just having a large model does not do much it is also about several iterations of training and feedback how are open source alternatives going to be able to compete i am not in the ml or cs field just an amateur who enjoys using these models",
  "i am using huggingface is transformers regularly for experimentations but i plan to deploy some of the models to ios i have found ml ane transformers repo from apple which shows how transformers can be rewritten to have much better performance on apple is devices there is an example of distilbert implemented in that optimized way as i plan to deploy transformers to ios i started thinking about this i am hoping some already have experience about this so we can discuss has anyone tried this themselves do they actually see the improvements in performance on ios i am using huggingface is transformer models in my experiments how much work do you think there is to rewrite model in this optimized way it is very difficult to train transformers from scratch especially if they are big so i am fine tuning on top of pre trained models on huggingface is it possible to use weights from pretrained huggingface models with the apple is reference code how difficult is it",
  "i know they can affect linear regression badly but given the fact that neural net and tree based models can approximate non linear complex functions i do not think the high leverage points would be a problem just curious about your opinion whether my thinking makes sense",
  "every day there seems to be new evidence of the generalization capabilities of llms what does this mean for the future role of deep learning experts in academia and business it seems like there is a significannot chance that skills such as pytorch and jax will be displaced by prompt construction and off the shelf model apis with only a few large institutions working on the dnn itself curious to hear others thoughts on this",
  "geometric gnns are an emerging class of gnns for spatially embedded graphs in scientific and engineering applications sa biomolecular structure material science and physical simulations notable examples include schnet dimenet tensor field networks and en equivariant gnns how powerful are geometric gnns how do key design choices influence expressivity and how to build maximally powerful ones check out this recent paper for more pdf code key findings ps are you new to geometric gnns gdl pytorch geometric etc want to understand how theory equations connect to real code try this geometric gnn notebook\u00a0before diving in",
  "stable diffusion seems to be a departure from the trend of building larger and larger models it has less parameters than other image generation models like dalle incredibly compared with dall e and imagen the stable diffusion model is a lot smaller while dall e has around billion parameters and imagen has billion the first stable diffusion model has just million parameters which means it uses a lot less vram and can actually be run on consumer grade graphics cards what allows stable diffusion to work so well with a lot less parameters are there any drawbacks to this like requiring stable diffusion to be fine tuned more than dalle for example",
  "what is the state of research in normalizing flows in have they been superseded by diffusion models for sample generation if so what are some other applications where normalizing flows are still sota or even useful"
]