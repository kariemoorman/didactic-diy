{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dc5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de8f88",
   "metadata": {},
   "source": [
    "## Contextual Bandit \n",
    "\n",
    "Contextual bandit models are a class of machine learning models used in online decision-making scenarios, where an agent/policy needs to make a sequence of decisions or actions over time while interacting with an environment. The rewards or outcomes of actions are influenced by additional contextual information.\n",
    "\n",
    "### Key Components of Contextual Bandit Models: \n",
    "- <b>Actions</b>: A set of choices that an agent can make. Each action has an associated reward or outcome.\n",
    "- <b>Context</b>: Features that describe the current state or environment. Contextual features help inform the agent's decision on which action to take.\n",
    "- <b>Reward Function</b>: Estimation or prediction of the expected reward for each action given the current context. The reward function maps actions and contexts to reward values, and is typically learned from the data.\n",
    "- <b>Learning Algorithm</b>: Mathematical representation of the decision space, used to estimate the value of actions in different contexts. The algorithm updates the value estimates based on observed rewards and contexts.\n",
    "- <b>Policy</b>: Definition of the strategy for selecting actions based on the current context. The goal is to find an optimal policy that maximizes cumulative rewards over time.\n",
    "- <b>Exploration vs. Exploitation</b>: Effectively balancing the trade-off between exploration (trying new actions to learn their values) and exploitation (choosing actions that are believed to have high rewards based on current knowledge). Exploration is essential for learning the true values of actions, especially when there is uncertainty or limited knowledge about the rewards associated with different actions in various contexts. Exploitation aims to maximize the immediate rewards by choosing actions that are likely to perform well. The choice of policy (exploration strategy) determines how the agent balances these two objectives.\n",
    "\n",
    "### Prototypical Use Cases: \n",
    "Contextual bandit models are prototypically used in situations where sequential decision-making is required based on contextual information.\n",
    "- <b>Online Advertising</b>: Displaying targeted ads to users on websites or mobile apps. Choosing which ad to show to a user based on their browsing behavior, demographics, and context.\n",
    "- <b>Recommender Systems</b>: Recommending products, movies, music, or content to users. Selecting the next item to recommend in a sequence based on user preferences, historical interactions, and real-time context.\n",
    "- <b>Content Personalization</b>: Customizing the content shown to users on websites or news platforms, e.g., tailoring news articles or videos to individual preferences and reading patterns.\n",
    "- <b>Dynamic Pricing</b>: Setting prices for products or services in real-time based on market conditions, user behavior, and competitor pricing. Offering personalized discounts or promotions to maximize revenue and customer satisfaction.\n",
    "- <b>Supply Chain Management</b>:Optimizing inventory management and order fulfillment based on real-time demand, supplier conditions, and inventory levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ed4ae",
   "metadata": {},
   "source": [
    "### Contextual Bandit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbf902",
   "metadata": {},
   "source": [
    "#### - Linear Model \n",
    "Linear bandit models consist of a single linear layer that takes the context as input and produces action scores or logits. The assumption is that the relationship between the context and action values is linear. Linear models are easily scalable to handle large datasets and high-dimensional contexts, however they may not capture complex non-linear relationships present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e964386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBanditModel(nn.Module):\n",
    "    def __init__(self, n_features, n_actions):\n",
    "        super(LinearBanditModel, self).__init__()\n",
    "        self.fc = nn.Linear(n_features, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae7d2e",
   "metadata": {},
   "source": [
    "#### - Feedforward Neural Network (DNN) Model\n",
    "DNN bandit models are complex neural network architecture comprised of multiple layers, including one or more hidden layers, with non-linear activation functions. Useful when making decisions based on contextual information that require capturing complex, non-linear relationships between the context and the expected rewards for each action. DNN models can be computationally intensive, require larger datasets, and are prone to overfitting if not regularized correctly (i.e., hyperparameter tuning is crucial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551a2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardBanditModel(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, hidden_dim=64):\n",
    "        super(FeedforwardBanditModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce434b9d",
   "metadata": {},
   "source": [
    "#### - Wide & Deep Model\n",
    "Wide and deep bandit models combine the strengths of linear models (wide) to capture broad, abstract patterns and the deep neural networks (deep) to capture low-level feature interactions in the contextual data. By combining both components, the model can potentially perform well across a wide range of contextual bandit problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4b94209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideComponent(nn.Module):\n",
    "    def __init__(self, n_features, n_actions):\n",
    "        super(WideComponent, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class DeepComponent(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, hidden_dim=64):\n",
    "        super(DeepComponent, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class WideAndDeepModel(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, hidden_dim=64):\n",
    "        super(WideAndDeepModel, self).__init__()\n",
    "        self.wide_component = WideComponent(n_features, n_actions)\n",
    "        self.deep_component = DeepComponent(n_features, n_actions, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Combine the outputs of both components (wide and deep).\n",
    "        wide_output = self.wide_component(x)\n",
    "        deep_output = self.deep_component(x)\n",
    "        return wide_output + deep_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b24e9",
   "metadata": {},
   "source": [
    "#### - Convolutional Neural Network (CNN) Model\n",
    "CNN bandit models are employed when contextual information is structured as images or grid-like data, making them well-suited for capturing spatial patterns and features in the context. The model consists of convolutional layers, followed by one or more fully connected layers. Convolutional layers perform spatial feature extraction, while fully connected layers process extracted features to make action predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0ba8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBanditModel(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(ConvolutionalBanditModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(32 * 32 * 32, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output.\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773879f",
   "metadata": {},
   "source": [
    "#### - Recurrent Neural Network (RNN) Model\n",
    "RNN bandit models are able to handle temporal dependencies in the data, making them a suitable choice when the contextual information is sequential or time-dependent. Typically comprised of one or more recurrent layers, followed by one or more fully connected layers. The recurrent layers process sequential context data, while fully connected layers make action predictions. Dependencies can be short-range (e.g., RNN) or long-range (e.g., LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29f2c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBanditModel(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, hidden_dim=64, num_layers=1):\n",
    "        super(RNNBanditModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(n_features, hidden_dim, num_layers, batch_first=True) # Define RNN layer.\n",
    "        self.fc = nn.Linear(hidden_dim, n_actions) # Define output layer.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state with zeros.\n",
    "        out, _ = self.rnn(x, h0) # Forward pass through the RNN layer.\n",
    "        out = self.fc(out[:, -1, :]) # Take the output from the last time step and pass it through the output layer.\n",
    "        return out\n",
    "    \n",
    "\n",
    "class LSTMContextualBandit(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, hidden_dim=64, num_layers=1):\n",
    "        super(LSTMContextualBandit, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(n_features, hidden_dim, num_layers, batch_first=True) #Define LSTM layer.\n",
    "        self.fc = nn.Linear(hidden_dim, n_actions) # Define output layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden states with zeros.\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0)) # Forward pass through the LSTM layer.\n",
    "        out = self.fc(out[:, -1, :]) # Take the output from the last time step and pass it through the output layer.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ef1cd",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1f38d",
   "metadata": {},
   "source": [
    "### Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "805890da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bandit(model, optimizer, criterion, num_epochs):\n",
    "    num_epochs = num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass.\n",
    "        action_logits = model(context)\n",
    "        # Convert chosen_actions to a PyTorch tensor\n",
    "        chosen_actions_tensor = torch.tensor(chosen_actions, dtype=torch.long)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(action_logits, chosen_actions_tensor)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090978b7",
   "metadata": {},
   "source": [
    "### Model Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "35e6ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bandit(k):\n",
    "    for i in range(1,k+1):\n",
    "        new_context = torch.tensor(np.random.randn(1, n_features), dtype=torch.float32)\n",
    "        action_logits = model(new_context)\n",
    "        chosen_action = torch.argmax(action_logits).item()\n",
    "        reward = rewards[chosen_action].item()\n",
    "        print(f'Turn: {i}, Action: {chosen_action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59e0b1",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae39d4",
   "metadata": {},
   "source": [
    "## Model Specifications\n",
    "\n",
    "- <b>Learning Rate</b>\n",
    "- <b>Optimizer</b>\n",
    "- <b>Criterion (Loss Function)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5317b",
   "metadata": {},
   "source": [
    "### <u>Learning Rate</u>\n",
    "\n",
    "Learning rate is a hyperparameter that determines the step size at which the model's parameters are updated during the training process. It controls the magnitude of adjustments made to the model's weights or coefficients in response to the computed gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ea05e",
   "metadata": {},
   "source": [
    "#### - Fixed Learning Rate\n",
    "Learning rate is held constant throughout training. This approach is simple and can work well when the data distribution and model architecture are relatively stable (e.g., `lr=0.01`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb953e9",
   "metadata": {},
   "source": [
    "#### - Learning Rate Schedules\n",
    "Learning rate is changed during training according to a predefined schedule. Helps balance between fast convergence in the early stages and fine-tuning in the later stages of model training. E.g., \n",
    "- <b>Step Decay</b>: Reduce the learning rate by a fixed factor after a fixed number of epochs or steps. \n",
    " - `scheduler = StepLR(optimizer, step_size=10, gamma=0.95)`, where gamma decay factor of 0.95 signals a 5% reduced learning rate at each step_size (10 epochs).\n",
    "- <b>Exponential Decay</b>: Exponentially decrease the learning rate over time.  \n",
    " - `scheduler = ExponentialLR(optimizer, gamma=0.95)`\n",
    "- <b>Cosine Annealing</b>: Use a cosine function to decrease the learning rate in a cyclical manner.\n",
    " - `scheduler = CosineAnnealingLR(optimizer, T_max=50)`, where T_max represents the number of epochs that make up a full cycle of cosine annealing (50 epochs).\n",
    "- <b>Learning Rate Decay on Plateau</b>: Monitor a validation metric (e.g., validation loss or accuracy) during training, and if it plateaus or worsens, reduce the learning rate. This helps the model fine-tune as it gets closer to convergence.  \n",
    " - `scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)`\n",
    " \n",
    "The `scheduler.step()` action is included in the model training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c0515",
   "metadata": {},
   "source": [
    "#### - Learning Rate Warmup \n",
    "Model training starts with a small learning rate that gradually increases over a few epochs. Helps the model avoid getting stuck in local minima early in training. E.g., \n",
    "\n",
    "```\n",
    "# Define learning rate warmup parameters\n",
    "initial_learning_rate = 0.1\n",
    "warmup_epochs = 10  # Number of epochs for warmup\n",
    "warmup_factor = 0.1  # Warmup factor for initial learning rate\n",
    "\n",
    "# Training Loop Learning Rate\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Learning rate warmup phase\n",
    "        lr = initial_learning_rate * (warmup_factor + (1.0 - warmup_factor) * (epoch / warmup_epochs))\n",
    "    else:\n",
    "        # Regular training phase\n",
    "        lr = initial_learning_rate\n",
    "        \n",
    "    # Update the optimizer's learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3b343",
   "metadata": {},
   "source": [
    "#### - Adaptive Learning Rate\n",
    "Adjust the learning rate based on the gradient information. Popular algorithms include Adam, RMSprop, and Adagrad. Adaptive learning rate algorithms adapt the learning rate on a per-parameter basis, which can be beneficial in more complex models. E.g., \n",
    "\n",
    "`optimizer = optim.Adam(model.parameters(), lr=0.001)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80067707",
   "metadata": {},
   "source": [
    "### <u>Optimizer</u>\n",
    "Optimizers are algorithms or methods used to adjust the parameters of a model in order to minimize the error or loss function during the training process. The primary goal of an optimizer is to find the optimal set of model parameters that result in the best possible performance on a given task. To do this, the optimizer iteratively updates the model's parameters based on the computed gradients of the loss function with respect to those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1942c68",
   "metadata": {},
   "source": [
    "#### - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD is the most fundamental optimizer. It updates model parameters based on the gradient of the loss with respect to the parameters. While it can be slower to converge than more advanced optimizers like Adam, it is often used as a baseline and can work well with appropriate learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4d6ef20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7084c5f",
   "metadata": {},
   "source": [
    "#### - RMSprop\n",
    "RMSprop is an adaptive learning rate optimizer that maintains a moving average of squared gradients for each parameter. It scales the learning rates differently for each parameter, which can help in training deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8341fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee572a6",
   "metadata": {},
   "source": [
    "#### - Adagrad\n",
    "Adagrad adapts the learning rate for each parameter based on the historical gradient information. It performs well on sparse data but may decrease the learning rate too aggressively over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173efef",
   "metadata": {},
   "source": [
    "#### - Adadelta\n",
    "Adadelta is an extension of Adagrad that addresses its aggressive learning rate decay by using a moving average of past gradients rather than accumulating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a211bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_optimizer = optim.Adadelta(model.parameters(), rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1449c1",
   "metadata": {},
   "source": [
    "#### - Nesterov Accelerated Gradient (NAG)\n",
    "NAG is an improved version of SGD that takes into account the future gradient information when updating parameters. It often converges faster than plain SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1b909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nag_optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dfeb9",
   "metadata": {},
   "source": [
    "#### - L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n",
    "L-BFGS is a quasi-Newton optimization algorithm that is often used for small to medium-sized datasets. It can be a good choice for optimizing shallow networks with a limited number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db3368ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optimizer = optim.LBFGS(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d8d74",
   "metadata": {},
   "source": [
    "#### - Proximal Gradient Descent (PGD)\n",
    "PGD is an optimization algorithm used for problems with sparse parameter updates. It's suitable for models with L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d99ce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4aca2",
   "metadata": {},
   "source": [
    "#### - Adam (Adaptive Moment Estimation)\n",
    "Adam is an optimization algorithm used for training machine learning models. It combines the advantages of AdaGrad and RMSprop, offering robust and efficient convergence. Adam maintains moving averages of gradients (first moment) and squared gradients (second moment) for each parameter, adaptively scales learning rates, and applies bias correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5007c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8b3f0",
   "metadata": {},
   "source": [
    "### <u>Criterion (Loss Function)</u>\n",
    "Criterion refers to the loss function or objective function used to quantify the error or discrepancy between the model's predictions and the true target values (ground truth) during the training process. The purpose of a loss function is to provide a single scalar value that the optimization algorithm (optimizer) seeks to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677b058",
   "metadata": {},
   "source": [
    "#### - Mean Squared Error (MSE) Loss\n",
    "Used for regression problems where the goal is to predict a continuous target variable. Measures the average squared difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba82eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7132a2",
   "metadata": {},
   "source": [
    "#### - L1 Loss (Absolute Error)\n",
    "Similar to MSE loss but measures the average absolute difference between predicted and actual values. Often used when outliers in the data should be handled with less sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36be403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee6640",
   "metadata": {},
   "source": [
    "#### - Smooth L1 Loss (Huber Loss)\n",
    "Combines properties of MSE and L1 loss. Smooth transition from L1 loss for small errors to L2 loss (MSE) for large errors. Less sensitive to outliers compared to MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "243104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_l1_criterion = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233e8e5",
   "metadata": {},
   "source": [
    "#### - Binary Cross-Entropy Loss (BCE Loss)\n",
    "Used for binary classification problems where the target variable has two classes (0 and 1). Measures the negative log-likelihood of the predicted class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "971acbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3fce5",
   "metadata": {},
   "source": [
    "#### - Binary Cross-Entropy Loss with Logits (BCEWithLogits Loss)\n",
    "Similar to BCE Loss but applied to the logits (before the sigmoid activation) rather than probabilities. Often used when applying a sigmoid activation function to the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3fdedb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_logit_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59d2e3",
   "metadata": {},
   "source": [
    "#### - Multi-Class Hinge Loss (MultiMargin Loss)\n",
    "Used for multi-class classification problems. Encourages correct class scores to be higher than incorrect class scores by a margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4bdaffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimargin_criterion = nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8c073",
   "metadata": {},
   "source": [
    "#### - Cross Entropy Loss (Log Loss or Negative Log-Likelihood Loss)\n",
    "Primarily used for classification problems. Measures the dissimilarity between the predicted class probabilities and the true class labels, where the goal is to minimize this dissimilarity, encouraging the model to assign higher probabilities to the correct classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3dda5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85456200",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8984ce",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model Training & Prediction Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184b859",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0325e",
   "metadata": {},
   "source": [
    "#### - Generate Random Data Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7cba1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4932)\n",
    "n_samples = 10000\n",
    "n_features = 5\n",
    "n_actions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d49f0",
   "metadata": {},
   "source": [
    "#### - Generate Random Contextual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "46a24e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.tensor(np.random.randn(n_samples, n_features), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e12c02",
   "metadata": {},
   "source": [
    "#### - Generate Random Action Probabilities for Each Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "87dfb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_theta = torch.tensor(np.random.randn(n_features, n_actions), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd59f9c",
   "metadata": {},
   "source": [
    "#### - Choose Actions Based on Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cfe37904",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities = torch.exp(torch.matmul(context, true_theta))\n",
    "action_probabilities /= action_probabilities.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe771b",
   "metadata": {},
   "source": [
    "#### - Sample Actions Based on the Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0d2645c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_actions = torch.multinomial(action_probabilities, 1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60fb82",
   "metadata": {},
   "source": [
    "#### - Calculate Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2388d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.matmul(context, true_theta)  # Shape: [n_samples, n_actions]\n",
    "chosen_action_indices = torch.arange(n_samples, dtype=torch.long), chosen_actions\n",
    "rewards = rewards[chosen_action_indices]  # Select rewards based on chosen actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91d27a",
   "metadata": {},
   "source": [
    "### 2. Select Bandit Model & Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767dc27",
   "metadata": {},
   "source": [
    "#### - Build Model\n",
    "Load features and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "541745df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearBanditModel(n_features, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413d9be",
   "metadata": {},
   "source": [
    "#### - Initialize Model Parameters \n",
    "Select learning rate, optimization method, criterion, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d77f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = adam_optimizer\n",
    "criterion = cross_entropy_criterion\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f060f80",
   "metadata": {},
   "source": [
    "### 3. Train Bandit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6abfa169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Loss: 0.7356\n",
      "Epoch [1/50] Loss: 0.7324\n",
      "Epoch [2/50] Loss: 0.7293\n",
      "Epoch [3/50] Loss: 0.7263\n",
      "Epoch [4/50] Loss: 0.7234\n",
      "Epoch [5/50] Loss: 0.7206\n",
      "Epoch [6/50] Loss: 0.7179\n",
      "Epoch [7/50] Loss: 0.7153\n",
      "Epoch [8/50] Loss: 0.7127\n",
      "Epoch [9/50] Loss: 0.7103\n",
      "Epoch [10/50] Loss: 0.7079\n",
      "Epoch [11/50] Loss: 0.7056\n",
      "Epoch [12/50] Loss: 0.7033\n",
      "Epoch [13/50] Loss: 0.7012\n",
      "Epoch [14/50] Loss: 0.6991\n",
      "Epoch [15/50] Loss: 0.6970\n",
      "Epoch [16/50] Loss: 0.6951\n",
      "Epoch [17/50] Loss: 0.6931\n",
      "Epoch [18/50] Loss: 0.6913\n",
      "Epoch [19/50] Loss: 0.6895\n",
      "Epoch [20/50] Loss: 0.6877\n",
      "Epoch [21/50] Loss: 0.6860\n",
      "Epoch [22/50] Loss: 0.6843\n",
      "Epoch [23/50] Loss: 0.6827\n",
      "Epoch [24/50] Loss: 0.6811\n",
      "Epoch [25/50] Loss: 0.6796\n",
      "Epoch [26/50] Loss: 0.6781\n",
      "Epoch [27/50] Loss: 0.6767\n",
      "Epoch [28/50] Loss: 0.6752\n",
      "Epoch [29/50] Loss: 0.6739\n",
      "Epoch [30/50] Loss: 0.6725\n",
      "Epoch [31/50] Loss: 0.6712\n",
      "Epoch [32/50] Loss: 0.6699\n",
      "Epoch [33/50] Loss: 0.6687\n",
      "Epoch [34/50] Loss: 0.6674\n",
      "Epoch [35/50] Loss: 0.6663\n",
      "Epoch [36/50] Loss: 0.6651\n",
      "Epoch [37/50] Loss: 0.6640\n",
      "Epoch [38/50] Loss: 0.6629\n",
      "Epoch [39/50] Loss: 0.6618\n",
      "Epoch [40/50] Loss: 0.6607\n",
      "Epoch [41/50] Loss: 0.6597\n",
      "Epoch [42/50] Loss: 0.6587\n",
      "Epoch [43/50] Loss: 0.6577\n",
      "Epoch [44/50] Loss: 0.6567\n",
      "Epoch [45/50] Loss: 0.6558\n",
      "Epoch [46/50] Loss: 0.6549\n",
      "Epoch [47/50] Loss: 0.6540\n",
      "Epoch [48/50] Loss: 0.6531\n",
      "Epoch [49/50] Loss: 0.6522\n"
     ]
    }
   ],
   "source": [
    "train_bandit(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6913",
   "metadata": {},
   "source": [
    "### 4. Model Predictions\n",
    "Updates context, chooses an action based on that context, and collects rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d069ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn: 1, Action: 0, Reward: 5.472799777984619\n",
      "Turn: 2, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 3, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 4, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 5, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 6, Action: 0, Reward: 5.472799777984619\n",
      "Turn: 7, Action: 0, Reward: 5.472799777984619\n",
      "Turn: 8, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 9, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 10, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 11, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 12, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 13, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 14, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 15, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 16, Action: 0, Reward: 5.472799777984619\n",
      "Turn: 17, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 18, Action: 1, Reward: 1.050110101699829\n",
      "Turn: 19, Action: 2, Reward: 0.19325566291809082\n",
      "Turn: 20, Action: 0, Reward: 5.472799777984619\n"
     ]
    }
   ],
   "source": [
    "predict_bandit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc141a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb248ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
