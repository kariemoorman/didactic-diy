created_unix_utc,created_pst,commenter_name,subreddit,comment,reddit_permalink
1676726330.0,18-Feb-2023 05:18:50,dojoteef,MachineLearning,"See previous discussion:
https://old.reddit.com/r/MachineLearning/comments/110swn2/d_quality_of_posts_in_this_sub_going_down",/r/MachineLearning/comments/115ez2r/d_please_stop/j916kx3/
1676588409.0,16-Feb-2023 15:00:09,dojoteef,MachineLearning,Post was removed due to breaking Rule 1: Beginner or career related question,/r/MachineLearning/comments/1143fmu/d_recommended_process_for_building_an_ml_model/j8u0nhb/
1676571153.0,16-Feb-2023 10:12:33,dojoteef,MachineLearning,"I commend what Huggingface is trying to do (be the source for the latest models that is consistent and easy to use), but every time I've used the library I've had to tackle bugs that were very time consuming to pinpoint, which is exacerbated by the structure of the code. The worst bugs have been subtle heisenbugs: the code seemed to work most of the time, but failed at other times. The heisenbugs are what made me stop using Huggingface altogether, unless it's my only option.

For example, I ran into a bug that only manifested when downloading a specific pretrained model for a task, which in turn downloads a config file that had a bug in the config. As a user it was super difficult to know where the source of the bug was without extensive spelunking. I've had many similarly difficult to diagnose issues each time I've used the Huggingface ecosystem.

I understand that what you're tasked with as a company is a *huge* undertaking for such a small team. Maybe splitting the package into a ""stable"" package and a ""nightly"" package could help (with stable being extensively bug tested more like an Ubuntu LTS release). My guess is that your team is likely too small to support that approach while adding new features at the same speed.",/r/MachineLearning/comments/113m1ly/d_huggingface_considered_harmful_to_the_community/j8sqm4i/
1676307764.0,13-Feb-2023 09:02:44,dojoteef,MachineLearning,"Tbh, it's because I took a step back and haven't been moderating the sub the past week and a half. I've been the one mod doing the majority of the filtering of these posts over the past couple of years and the noise has just been going up exponentially over that time. It's very time consuming and I'm pretty burned out doing it, so I've taken some time away. I brought this up with the other mods before stepping back a bit.

It's probably good to try to get more mods, but I think the majority of the current mods are afraid to hire on new mods that might have a different philosophy of moderating, thus changing the feel of the sub.",/r/MachineLearning/comments/110swn2/d_quality_of_posts_in_this_sub_going_down/j8e2m8g/
1675133045.0,30-Jan-2023 18:44:05,dojoteef,MachineLearning,Removed due to rule #6.,/r/MachineLearning/comments/10plwt2/p_ai_containing_an_outbreak_regression_with_a/j6l7f5x/
1674869344.0,27-Jan-2023 17:29:04,dojoteef,MachineLearning,Removed due to rule #4. You might try posting in r/CSCareerQuestions or r/GradAdmissions  instead. Good luck!,/r/MachineLearning/comments/10n1v0z/d_feeling_anxious_about_results_of_graduate/j66ho77/
1674766202.0,26-Jan-2023 12:50:02,dojoteef,MachineLearning,"I'd guess that it's an easier optimization problem. GANs are known to have stability issues during training, likely due to the adversarial formulation.

I think a more interesting question is why it also performs better than VAEs, since diffusion models also fall under the category of variational inference. Again I'd assume it's an easier optimization problem due to having a large number of denoising steps. Perhaps a technique like [DRAW](https://arxiv.org/abs/1502.04623) could match diffusion models if used with more steps? Not sure.",/r/MachineLearning/comments/10m1sdm/d_why_are_gans_worse_than_latent_diffusion_models/j60evd7/
1674757437.0,26-Jan-2023 10:23:57,dojoteef,MachineLearning,Great timing considering [this discussion of watermarking LLMs](https://reddit.com/r/MachineLearning/comments/10ijzi2/d_couldnt_devs_of_major_gpts_have_added_an/) from a few days ago.,/r/MachineLearning/comments/10lyb7r/a_watermark_for_large_language_models/j5zrabs/
1674752364.0,26-Jan-2023 08:59:24,dojoteef,MachineLearning,"You stated I removed comments simply because they do not align with ""my politics"". This isn't true. I've removed comments by people who both support this initiative and disagree with it, so your statement is clearly untrue and misrepresents the facts. Look at the link you posted again. This is the last response you will get on this subject from me.",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5zdmgn/
1674740258.0,26-Jan-2023 05:37:38,dojoteef,MachineLearning,"Removed for brigading and misinformation. A 15yr old account that literally posted in r/ml for the first time to push their political agenda. Their comment word cloud literally has ""left"", ""media"", ""fucking"", and ""fuck"" in their top 5 words. Check for yourself:

https://redditmetis.com/user/jvardrake",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5yk0n1/
1674607841.0,24-Jan-2023 16:50:41,dojoteef,MachineLearning,Not all PhD students are under 30. I'd estimate 5-10% of PhD students in my CS program are over 30.,/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5rddbj/
1674575635.0,24-Jan-2023 07:53:55,dojoteef,MachineLearning,"I'm going to be much more aggressive in removing comments that are intentionally obtuse. If you have something constructive to say, please be civil (no insults, snark, etc) in your comments otherwise your comment will likely be removed.",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5p06il/
1674575470.0,24-Jan-2023 07:51:10,dojoteef,MachineLearning,"Because that person made a single comment and hasn't commented since. On the other hand you're going back and forth with the quips. If you notice I didn't respond to your initial quip in this thread.

And as a mod, I really don't want this post to turn into a flamewar with people insulting each other, because that's even more time consuming to moderate and I have my research in addition to moderating. I do it as a community service.",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5ozqmq/
1674573914.0,24-Jan-2023 07:25:14,dojoteef,MachineLearning,Sure. So why continue the trend? Downvote and ignore it. You're not going to change their mind with another quip.,/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5ovrgl/
1674573258.0,24-Jan-2023 07:14:18,dojoteef,MachineLearning,">	And that has the side effect that you can infer a surprisingly large set of criteria about someone who believes it is messed up.

Please don't be flippant and smug about it. You actually cannot tell; there is definitely a signal, but it could simply be because someone knows a person rather than being a representative themselves.

Regardless, this type of comment where you assume you know a person from a short interaction typically causes these discussions to derail further into quips back and forth with little substance. Let's try to avoid that.",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5ou3zf/
1674567186.0,24-Jan-2023 05:33:06,dojoteef,MachineLearning,"A small comment on the age-based criteria. I'm not sure what the reason for the category is, but it's likely harder for students in that age group (30-50) to publish than it is for those who went directly to grad school from undergrad. They are more likely to have kids to care for, have a much larger adjustment back to academic requirements, etc. I'd definitely reconsider why the committee included this as criteria.

That said, as someone who headed a DEI initiative at my university, I know how difficult the role can be. Good luck with the initiative!",/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/j5ogecj/
1674503218.0,23-Jan-2023 11:46:58,dojoteef,MachineLearning,"This has been studied quite a bit. You can just follow the citation graph of the fastText paper:
[Enriching Word Vectors with Subword Information](https://www.semanticscholar.org/paper/Enriching-Word-Vectors-with-Subword-Information-Bojanowski-Grave/e2dba792360873aef125572812f3673b1a85d850)

For example, people have investigated sampling different subword tokenizations during training ([Stochastic Tokenization with a Language Model for Neural Text Classification](https://aclanthology.org/P19-1158/)) and character-aware embeddings ([CharBERT: Character-aware Pre-trained Language Model](https://aclanthology.org/2020.coling-main.4/)).",/r/MachineLearning/comments/10jka1r/d_embedding_bags_for_llms/j5l399n/
1674246685.0,20-Jan-2023 12:31:25,dojoteef,MachineLearning,"Reddit automatically removed it, likely due to editing the post. Don't know why, but that occasionally triggers their spam filter. I've approved the post again.",/r/MachineLearning/comments/10gxs5i/d_did_youtube_just_add_upscaling/j56v9rx/
1674222862.0,20-Jan-2023 05:54:22,dojoteef,MachineLearning,"It's not about which country it's in either. If we were talking about coal miners in the US, I'd say the same thing. Workers often don't consider the toll on their mind and body until much later. Frequently they'll push their children to make a different job decision later based on their lived experience. Clearly the companies know the harmful nature of the work and so should put more effort in ensuring adequate safety for their workers. And if you cannot ensure safety, but the work is critical then you need to be able to care for the workers long term even after they have done their duty.",/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/j555gwc/
1674221522.0,20-Jan-2023 05:32:02,dojoteef,MachineLearning,"I think the title of the post really buries the lead. It makes people focus on the price of labor, but not the human toll of annotating graphic depictions of disturbing human behavior without adequate safety. From the article:

>	Much of that text appeared to have been pulled from the darkest recesses of the internet. Some of it described situations in graphic detail like child sexual abuse, bestiality, murder, suicide, torture, self harm, and incest.

>	One Sama worker tasked with reading and labeling text for OpenAI told TIME he suffered from recurring visions after reading a graphic description of a man having sex with a dog in the presence of a young child. “That was torture,” he said. “You will read a number of statements like that all through the week. By the time it gets to Friday, you are disturbed from thinking through that picture.” The work’s traumatic nature eventually led Sama to cancel all its work for OpenAI in February 2022, eight months earlier than planned.

It's not surprising that companies want cheap labor, or that workers in these countries are willing and even at times eager to have a job like this. Rather the troublesome issue is knowing the harmful nature of the work and essentially exploiting unwitting workers who don't have the experience to know better. It sounds like Sama at least tried a bit (have a therapist available), but ultimately didn't do enough (not enough therapists needed for the demand).

History and the present are full of explotation of workers and the environment (Nestle anyone?), but that doesn't preclude us from asking questions and demanding better when.",/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/j552n0g/
1674174769.0,19-Jan-2023 16:32:49,dojoteef,MachineLearning,"Stating the number of parameters in a model, without stating the architecture is like stating the number of lines of code in a program without describing the algorithm.

As stated, your speed measurement is incomparable.",/r/MachineLearning/comments/10gffem/discussion_im_getting_50fps_with_4_billion/j52v16e/
1674058180.0,18-Jan-2023 08:09:40,dojoteef,MachineLearning,"Note that the authors have an earlier paper introducing discrete latents for NLP and there are a number of follow up papers to this one as well. So if your interested in a deep dive, you should investigate the citation graph of this paper. Good luck!",/r/MachineLearning/comments/109yuvi/d_has_any_work_been_done_on_vqvae_language_models/j4vnho4/
1673701126.0,14-Jan-2023 04:58:46,dojoteef,MachineLearning,Be civil in your comments. Hurling insults will result in bans.,/r/MachineLearning/comments/10bkjdk/n_classaction_lawsuit_filed_against_stability_ai/j4b294h/
1673537692.0,12-Jan-2023 07:34:52,dojoteef,MachineLearning,See [Fast Decoding in Sequence Models using Discrete Latent Variables](https://arxiv.org/abs/1803.03382),/r/MachineLearning/comments/109yuvi/d_has_any_work_been_done_on_vqvae_language_models/j41m2hd/
1673283051.0,09-Jan-2023 08:50:51,dojoteef,MachineLearning,"It really depends on what you're trying to do with the data. For example, in NLP you could look directly at sequences of words, but for some problems parts of speech or parse trees alone could be enough for a particular type of analysis.

That said, there is no clear cut guidance on determining which problems can empirically be solved with a lower dimensional representation. It's possible in a pure math domain, but as soon as you begin looking at real world problems it's often not possible to prove there exists a lower dimensional manifold capable of representing the problem space.

My suggestion is to use any domain knowledge you have to try various representations and see which performs best. Good luck!",/r/MachineLearning/comments/107h7eq/d_am_i_reducing_the_dimensionality_of_the_problem/j3mlflc/
1673182262.0,08-Jan-2023 04:51:02,dojoteef,MachineLearning,"We already moderate these kinds of posts, but sometimes they are up for a few hours before we see them.",/r/MachineLearning/comments/106ee16/d_the_limitless_potential_of_opensource_ai_in/j3gnisd/
1672974413.0,05-Jan-2023 19:06:53,dojoteef,MachineLearning,"They also have a demo website:

https://valle-demo.github.io/",/r/MachineLearning/comments/104ixvi/r_neural_codec_language_models_are_zeroshot_text/j359hps/
1672850667.0,04-Jan-2023 08:44:27,dojoteef,MachineLearning,Beginner projects go elsewhere. Try r/learnmachinelearning instead.,/r/MachineLearning/comments/102zbk0/project_building_a_stateful_multicontext_aware/j2xcxbg/
1672838952.0,04-Jan-2023 05:29:12,dojoteef,MachineLearning,Please do not post this again. See rule #6.,/r/MachineLearning/comments/102zbk0/project_building_a_stateful_multicontext_aware/j2wlgsa/
1672701434.0,02-Jan-2023 15:17:14,dojoteef,MachineLearning,"Better late than never. Started my PhD in my mid thirties and I'm glad I did.

That said, I knew exactly what I wanted to work on (it's relatively niche) and have been fortunate enough to find an advisor willing to let me work in that area. If you're unsure, then it might make sense to work in industry for a while and later decide if you want to come back for a PhD.",/r/MachineLearning/comments/101qbfl/d_life_advice_to_relatively_late_bloomer_ml/j2p0jrg/
1672243389.0,28-Dec-2022 08:03:09,dojoteef,MachineLearning,"It really depends on your reasons for wanting a publication associated with your software package. Some PhD programs require journal pubs, some only care about conferences. Same can be said about grant funding institutions and even corporate research labs.

Once you know what type of venue you want to publish in (if you even need a publication for professional development), then consider what venue will best highlight your work to the audience you care about.

If you purely want to focus on the engineering aspect, then maybe a venue like JOSS might work. If instead you want to highlight the science aspect (e.g. what types of research your code enables, how it improves methodology, etc) then JMLR might be better. That said, the major conferences in the field also typically have software/implementation focused tracks you can submit to (demo track, industry track, etc). 

Good luck!",/r/MachineLearning/comments/zxcp8h/d_where_to_publish_ml_software/j1zkfrt/
1672160910.0,27-Dec-2022 09:08:30,dojoteef,MachineLearning,"You don't need to tell them one is AI or model generated. Could be two model generated texts or two human written texts. Merely having another text for comparison allows people to better frame the task since otherwise they essentially need to imagine a baseline for comparison, which people rarely do.",/r/MachineLearning/comments/zwht9g/p_can_you_distinguish_aigenerated_content_from/j1v4j4r/
1672158260.0,27-Dec-2022 08:24:20,dojoteef,MachineLearning,"Very interesting idea. It could easily be applied to images since digital watermarks already exist. Not sure how feasible it is for AI generated text.

Tbh, I imagine it behooves companies to do this so they are less likely to train on media (text, images, audio, etc) produced from a model. The more ubiquitous the use of AI generation becomes, the more of an issue this poses. Currently that problem is likely quite minimal and probably acts to inject a small bit of noise into training (and the knowledge distillation effect could make slightly improve training efficiency).

Though I guess a new data cleaning step could be running a classification model to classify if the media trained on is likely AI generated, though that would likely be less efficient than a hash produced at the time of generation.",/r/MachineLearning/comments/zwi4jx/discussion_2_discrimination_mechanisms_that/j1uy04f/
1672157778.0,27-Dec-2022 08:16:18,dojoteef,MachineLearning,"Nice job!

Though, to produce a better comparison it's best to show two examples side-by-side (one by a human, the other by the model, in a randomized order of course). The reason is that most people are not trained to analyze short snippets of text out of context. People trained to do that, e.g. English teachers, can better distinguish generated text without a baseline to compare against, but most people (crowd sourced evaluation) will likely produce a very biased analysis not reflective of the real ability for humans to distinguish between the two.

For a more thorough investigation of this phenomenon you can check out our research:

[The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation](https://aclanthology.org/2021.emnlp-main.97/)",/r/MachineLearning/comments/zwht9g/p_can_you_distinguish_aigenerated_content_from/j1uwubj/
1672051951.0,26-Dec-2022 02:52:31,dojoteef,MachineLearning,Please see rule #8 regarding self-promotion. Contests and tutorials designed to advertise your platform are prime examples of self-promotion that will be targeted for removal by the mods.,/r/MachineLearning/comments/zvkjoh/n_personalise_stable_diffusion_models_in/j1plusn/
1671096873.0,15-Dec-2022 01:34:33,dojoteef,MachineLearning,See the ACL 2020 paper: [Automatic Detection of Generated Text is Easiest when Humans are Fooled](https://aclanthology.org/2020.acl-main.164/),/r/MachineLearning/comments/zl1gyn/d_has_anyone_experimented_with_detection_of_nlp/j0ayzuq/
1671096647.0,15-Dec-2022 01:30:47,dojoteef,MachineLearning,"See the graphs in the paper that introduced nucleus sampling: [The Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=rygGQyrFvH). They visualize how human authored text has different statistical properties from machine generated text. That's mainly a tradeoff between fluency and coherence. Sampling procedures like top-*k* or nucleus sampling restrict the tokens that can be emitted and thus introduce statistical bias in the generated text, but produce more fluent text. Rather, sampling from the full distribution gets closer to the distribution of human-authored text, but often degenerates into incoherence (hence the title of the paper).",/r/MachineLearning/comments/zmd6l8/d_is_natural_text_always_maximally_likely/j0ayqqq/
1670951066.0,13-Dec-2022 09:04:26,dojoteef,MachineLearning,"This is great! Is it realistically possible to train LLMs ala BLOOM from scratch using these, or just do finetuning? I guess I'm wondering how the training speed scales with more compute nodes.

Even if we assume high end GPUs/TPUs, a frequent bottleneck is throughput due to network latency. How big of an issue is that? For example, I had previously tried scaling to multi-node training on my University's cluster and it turned out that it was faster to do gradient accumulation on a single node than to do multi-node training because the network switches were not purchased with high-throughput in mind.",/r/MachineLearning/comments/zky7ly/d_are_there_any_distributed_model_training/j02kku4/
1670945431.0,13-Dec-2022 07:30:31,dojoteef,MachineLearning,"While there is a field of research investigating federated learning which might one day allow for an ML@Home type project, as it stands the current algorithms require too much memory, computation, and bandwidth for training the very large models like GPT3.

I'm hopeful that an improved approach will be devised that mitigates these issue (in fact I have some ideas I'm considering for my next research project), but as it stands these issues render a real ML@Home type project currently infeasible.",/r/MachineLearning/comments/zky7ly/d_are_there_any_distributed_model_training/j0275on/
1670173146.0,04-Dec-2022 08:59:06,dojoteef,MachineLearning,You already posted this last week. Please do not re-post.,/r/MachineLearning/comments/zcevjn/p_metric_learning_theory_practice_code_examples/iyw2lcx/
1670172963.0,04-Dec-2022 08:56:03,dojoteef,MachineLearning,"Mistakes happen. In this case the authors report the issue publicly and should be commended for that.

The NeurIPS organizers can choose to address the issue in whatever way they deem appropriate, especially as the authors are not hiding the fact that their results were changed.

Of course you're free to assume it's malicious if you want (at least that seems to be the stance your taking, but if it's not then I might have misinterpreted your response).",/r/MachineLearning/comments/zcdw0k/d_neurips_2022_outstanding_paper_modified_results/iyw254f/
1670171587.0,04-Dec-2022 08:33:07,dojoteef,MachineLearning,"Linking to the dataset, paper, or blog post would be helpful:

https://www.deepmind.com/blog/benchmarking-the-next-generation-of-never-ending-learners",/r/MachineLearning/comments/zc7l9x/d_eli5_what_is_exactly_nevis22/iyvysm6/
1670171251.0,04-Dec-2022 08:27:31,dojoteef,MachineLearning,"See the [author's explanation](https://openreview.net/forum?id=fSfcEYQP_qc&noteId=KM3S-BRy4D) on OpenReview:

>	We update the result tables in the camera-ready version. The revision is due to a different data version of query augmentation. Previously, the data is cooked by one of our co-authors while using a different train-test split to train the query generator, causing some data leakage issue. All experiments in the previous submission are based on this query augmentation version, so the performance is relatively higher. When preparing the camera-ready version, we review and reproduce the code end-to-end for official release. At that time, we realize the data leakage problem. So, we re-cook the query augmentation data and reproduce all the experiments again in the new table. After solving the data leakage problem, NCI still shows more than 15% improvement over the current best SOTA. We have released the complete open-source code at GitHub:
>
>	https://github.com/solidsea98/Neural-Corpus-Indexer-NCI
>
>	Welcome to follow and reproduce our work. Looking forward to further discussions and collaborations.",/r/MachineLearning/comments/zcdw0k/d_neurips_2022_outstanding_paper_modified_results/iyvxzsz/
1670073947.0,03-Dec-2022 05:25:47,dojoteef,MachineLearning,"Check this paper out:
https://reddit.com/r/MachineLearning/comments/mnoka6/r_infinitygan_towards_infiniteresolution_image/",/r/MachineLearning/comments/zbg2yz/d_what_methods_would_you_recommend_for_building/iyqulau/
1668484734.0,14-Nov-2022 19:58:54,dojoteef,MachineLearning,"It depends. Most conferences specifically state their policy in relation to preprints. For example [NeurIPS states](https://neurips.cc/Conferences/2022/PaperInformation/NeurIPS-FAQ):

>	**What is the policy on comparisons to recent work?** Papers appearing less than two months before the submission deadline
are generally considered concurrent to NeurIPS submissions.  Authors are not expected to compare to work that appeared only a month or two before the deadline. 

>	**Are arxiv papers also subject to the policy above?** Yes, we do not distinguish arxiv papers and other published (conference & journal) papers, and the two-month rule applies in the same way. More nuanced judgements, including how to determine the date of publication, should be made by the area chair handling the submission.

and ICLR has a [similar policy](https://iclr.cc/Conferences/2023/ReviewerGuide):

>	**Q**: Are authors expected to cite and **compare with very recent work?** What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)

>	**A**: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.

You should check the policy for the conference you are reviewing for/submitting to for more relevant instructions.",/r/MachineLearning/comments/yvkvgo/d_is_it_legitimate_for_reviewers_to_ask_you/iwf3rdk/
1668269511.0,12-Nov-2022 08:11:51,dojoteef,MachineLearning,"It's interesting that you use GPT-3 instead of Codex. Recently, I've been working on some research that combines natural language (in the form of dialogues) with Lua scripts. Turns out that using Codex on a few-shot setup performs better than a finetuned GPT-3 Curie model (haven't tried Davinci, but since it was $100 to finetune Curie, I'm a little gunshy to try Davinci since it's 10x more expensive to finetune).

Considering Codex is currently free, you might consider giving it a shot!",/r/MachineLearning/comments/ytaywi/deleted_by_user/iw32us1/
1668180427.0,11-Nov-2022 07:27:07,dojoteef,MachineLearning,"There is a lot of uncertainty out there for sure. I know people who've done multiple internships with the same company over the course of their PhD and are worried they might not receive a full-time offer when they graduate. I'm not sure if their concerns are founded or not, but that's their current outlook.",/r/MachineLearning/comments/ysc7gs/d_current_job_market_in_ml/ivyftve/
1668006416.0,09-Nov-2022 07:06:56,dojoteef,MachineLearning,"No one can answer that question, since not all possible output images are equally probable (some are even impossible given trained network weights). You might be able to make an empirical estimate, but enumerating the true output space of any sufficiently complex NN is an open problem.",/r/MachineLearning/comments/yqjx2q/discussion_could_someone_explain_the_math_behind/ivou9rr/
1667964494.0,08-Nov-2022 19:28:14,dojoteef,MachineLearning,This sub is not the place for witch hunts. This post has been removed and comments locked.,/r/MachineLearning/comments/ypzbp5/d_academia_the_highest_funded_plagiarist_is_also/ivmzzjs/
1667939842.0,08-Nov-2022 12:37:22,dojoteef,MachineLearning,"That's a bit of an older work. You might have better luck with newer papers (especially since the frameworks used at the time have changed a lot).

That said, if you really want to something aimilar to PixelRNN, you might consider basing your work on PixelCNN++ from OpenAI (it uses a pre-1.0 version of Tensorflow):
https://github.com/openai/pixel-cnn

It's an extension of PixelCNN, which was a more computationally efficient follow up to PixelRNN.",/r/MachineLearning/comments/ypx4aw/project_pixelrnn/ivleig5/
1667652841.0,05-Nov-2022 05:54:01,dojoteef,MachineLearning,"You might appreciate these blog posts relating deep learning models to physics:
https://mcbal.github.io/",/r/MachineLearning/comments/ymn4xn/d_physicsinspired_deep_learning_models/iv5c9sh/
1667561140.0,04-Nov-2022 04:25:40,dojoteef,MachineLearning,"Slightly off-topic: I'm a huge John Carmack fan, but he isn't the author of that code. It's just part of engine code that his company released for the game Quake 3 Arena. For details, check out:

https://www.beyond3d.com/content/articles/8/",/r/MachineLearning/comments/ylizjt/n_classaction_lawsuit_filed_against_github/iv0hfoe/
1667358149.0,01-Nov-2022 20:02:29,dojoteef,MachineLearning,"KataGo supports Tromp-Taylor rules:
https://lightvector.github.io/KataGo/rules.html",/r/MachineLearning/comments/yjryrd/n_adversarial_policies_beat_professionallevel_go/iuprp1z/
1667357782.0,01-Nov-2022 19:56:22,dojoteef,MachineLearning,"It seems most commenters are pointing out reasoning why the proposed setup seems deficient in one way or the other.

But the point of the research is to highlight potential blind spots even in seemingly ""superhuman"" models, even if the failure modes are weird edge cases that are not broadly applicable.

By first identifying the gaps, mitigation strategies can be devised that make training more robust. In that sense, the research is quite useful even if a knowledgable GO player might not be impressed by the demonstrations highlighted in the paper.",/r/MachineLearning/comments/yjryrd/n_adversarial_policies_beat_professionallevel_go/iupqxxr/
1667350416.0,01-Nov-2022 17:53:36,dojoteef,MachineLearning,"You might want to check out OpenAI's paper:
[Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)",/r/MachineLearning/comments/yjqkig/d_p_bidirectional_conditional_text_generation/iupacu9/
1667256893.0,31-Oct-2022 15:54:53,dojoteef,MachineLearning,"I've been using Typescript recently for some of my research and the speed is so much faster compared to Python. Additionally, the type system is much nicer than Python's type annotations.

I'm glad to see some diversification in the ML space. And while I know it's not the focus of the project, it might be nice to have a subset that runs in the browser. It can help in making client-side apps that require ML.",/r/MachineLearning/comments/yin6ho/d_using_javascript_for_ml_trainingresearch_not_in/iuk3vq6/
1667231567.0,31-Oct-2022 08:52:47,dojoteef,MachineLearning,Please do not post this again. You've posted the repo each week the past few weeks and multiple times this week.,/r/MachineLearning/comments/yido0n/r_awesome_solution_for_portrait_segmentation_with/iuieoud/
1667102381.0,29-Oct-2022 20:59:41,dojoteef,MachineLearning,Why in the world did this post devolve into an attack against non-English/non-native English speakers? Have some respect. Such comments will be removed and bans will be handed out for any further idiocy.,/r/MachineLearning/comments/ygj11f/r_ernievilg_20_improving_texttoimage_diffusion/iubyq0z/
1666799256.0,26-Oct-2022 08:47:36,dojoteef,MachineLearning,Email the chairs: aaai23chairs@aaai.org,/r/MachineLearning/comments/ydyzlk/deleted_by_user/itv9kbp/
1666798263.0,26-Oct-2022 08:31:03,dojoteef,MachineLearning,"Since being able to see the names of other reviewers doesn't imply authors can guess your identity, the more troubling possibility is that a reviewer is collaborating with the authors of that paper.

Did you reach out to the meta-reviewer and the chairs? That should be the first thing you do when you run into such a situation.",/r/MachineLearning/comments/ydyzlk/deleted_by_user/itv72az/
1666695958.0,25-Oct-2022 04:05:58,dojoteef,MachineLearning,"Stealth marketing isn't appreciated on the sub, i.e. starting a conversation so you can compare dstack against existing providers. Please don't do that.",/r/MachineLearning/comments/yd1mur/d_what_cloud_gpu_platforms_do_you_use/itpi0v1/
1666348386.0,21-Oct-2022 03:33:06,dojoteef,MachineLearning,"Please use the existing thread:
https://reddit.com/r/MachineLearning/comments/y9d6f6/d_aaai_2023_reviews/",/r/MachineLearning/comments/y9nns2/deleted_by_user/it6s1ir/
1666272096.0,20-Oct-2022 06:21:36,dojoteef,MachineLearning,"Hmm, if using Cohen's kappa, then discretizing the annotations such that you can compute matches will be important (this seems difficult). If you go for discretization, you could also consider Randolph's kappa:
http://www.justusrandolph.net/kappa/

It uses an empirical prior, rather than assuming an equal probability for each category.

Alternatively you could define a metric (in the mathematical sense) that measures distance between two annotations. That should allow computing Krippendorf's alpha. There are a bunch of common metrics people use, see:
https://en.wikipedia.org/wiki/Krippendorff%27s_alpha#Difference_functions

Ultimately, the question of interpretability comes up when you uses any of these correlation measures, as there is no generally agreed upon understanding of what the coefficient ranges imply. For example, having more categories typically decreases the correlation and having fewer categories increases it.",/r/MachineLearning/comments/y8ocbw/d_how_yo_measure_inter_rater_reliability_for/it2eodb/
1665769933.0,14-Oct-2022 10:52:13,dojoteef,MachineLearning,https://csrankings.org,/r/MachineLearning/comments/y40hpi/d_is_there_a_way_to_see_the_trend_of_universitys/isbe7je/
1665615217.0,12-Oct-2022 15:53:37,dojoteef,MachineLearning,Post was removed due to breaking Rule 1: Beginner or career related question,/r/MachineLearning/comments/y2i1ek/d_summer_2023_internship_referral_data_scientist/is3160e/
1665450083.0,10-Oct-2022 18:01:23,dojoteef,MachineLearning,Post was removed due to breaking Rule 1: Beginner or career related question,/r/MachineLearning/comments/y0vc9j/discussion_best_way_to_transition_from_frontend/iru0o1t/
1665417217.0,10-Oct-2022 08:53:37,dojoteef,MachineLearning,Post was removed due to breaking Rule 1: Beginner or career related question,/r/MachineLearning/comments/y0i45b/d_is_machine_learning_a_good_fit_for_me/irrvl09/
1665414971.0,10-Oct-2022 08:16:11,dojoteef,MachineLearning,Post was removed due to breaking Rule 1: Beginner or career related question,/r/MachineLearning/comments/y0eqj5/d_which_is_better_career_path_ml_or_sde/irrq0ay/
1665229477.0,08-Oct-2022 04:44:37,dojoteef,MachineLearning,"I got my start doing research exactly like this. At the time I was looking to apply to grad schools, but had been working in industry for years without any formal ml training. I found a professor who posted about needing collaborators their first year and so I applied.

Turns out the letter of recommendation I received from him was pivotal to my acceptance by my current advisor, since it demonstrated recent research experience, despite having great industry letters of recommendation talking about my abilities as a lead engineer.

I'm sure anyone you collaborate with will be very appreciative of your efforts.",/r/MachineLearning/comments/xyl78y/deleted_by_user/iri92a7/
1665191815.0,07-Oct-2022 18:16:55,dojoteef,MachineLearning,"I presume you worked with a faculty member on this paper. If so, you should discuss these questions with them. Most will pay for your conference expenses (travel, accommodation, and daily expenses like food). Their grants usually include funding for conference expenses.",/r/MachineLearning/comments/xydgc0/d_attending_emnlp_2022_in_person/irgxlt4/
1665156175.0,07-Oct-2022 08:22:55,dojoteef,MachineLearning,You could probably run the equivalent of a grammatical error correction (GEC) model on it — one trained on programming languages rather than a spoken language.,/r/MachineLearning/comments/xy1vao/d_handling_syntax_errors_in_code_generation/irew5z2/
1658525073.0,22-Jul-2022 14:24:33,dojoteef,MachineLearning,"I generally agree with you, except for the fact that you should be selective regarding who you place your trust in.

I'm a fifth year PhD student and you can easily verify who I am and what I've done with a quick search of my username online. I've been part of the faculty hiring process multiple times (elected PhD students who have achieved candidacy can take part in interviewing faculty candidates and even have a couple of votes during faculty hiring decisions — see [section 8.4.8 of our college bylaws](https://www.umass.edu/provost/sites/default/files/2020-03/CICSBylaws%20V2%2010-22-2019.pdf) for proof). I haven't seen anything akin to the stated claim — not in talking with faculty from our school or other top schools, and have never heard someone be so flippant about published work like this. If they don't have such onerous requirements for tenure track faculty, I can't imagine they would for PhD applicants.

Instead you are relying on the word of someone with little to no history on reddit (an account that is merely a few days old) that you have to trust on faith actually has any experience in the area. If they are willing to provide some proof of who they are, I'd love to know what school completely discounts joint first-authorship.",/r/MachineLearning/comments/w58mbf/d_can_you_reorder_equalcontribution_author_names/ih8qx4x/
1658507172.0,22-Jul-2022 09:26:12,dojoteef,MachineLearning,"Despite your flippant tone I'll assume you're ultimately after a good-faith discussion, so I'll engage for now.

Denigrating people (e.g. discounting them outright) and casting aspersions (e.g. callously stating there's no way the authors had equal contribution) is indeed toxic, whether you cast a wide net (e.g. referring to all equal contribution co-authors) or single individuals out.

Additionally, stating that a class of individuals are ignored simply due to superficial attributes leads to increased mental distress, contributing to issues like imposter syndrome. If a paper states equal contribution, then ignoring this fact is superficial and seems negligent at best, especially for an admissions committee (speaking of which, a small fraction of PhD applicants have (co-)first author papers, so discounting them outright seems quite suspect even for a top program).

If there are two central theorems to a theoretical paper and one co-author formulated and proved the first one and another co-author did the other (and both double-checked each other's work), it's pretty clear that there is a fairly equal contribution. Arbitrarily deciding who did more in such a case based on the order of names seems capricious.",/r/MachineLearning/comments/w58mbf/d_can_you_reorder_equalcontribution_author_names/ih7gz7w/
1658502893.0,22-Jul-2022 08:14:53,dojoteef,MachineLearning,"Ad hominem attacks (""What is this fragility"") aren't welcome in this sub. If you want to engage is such behavior, please move your discussion to twitter instead.",/r/MachineLearning/comments/w58mbf/d_can_you_reorder_equalcontribution_author_names/ih75vjg/
1658501835.0,22-Jul-2022 07:57:15,dojoteef,MachineLearning,Please don't spout toxic remarks like this — academia is toxic enough as it is.,/r/MachineLearning/comments/w58mbf/d_can_you_reorder_equalcontribution_author_names/ih7376u/
1658420600.0,21-Jul-2022 09:23:20,dojoteef,MachineLearning,"Good question! As someone born in Egypt, but who grew up in the US, I'm really only familiar with the Egyptian dialect of Arabic, which is quite different, yet lots of the Arab world can understand me due to the impact of Egyptian cinema (sadly, the reverse is not the case — I find it quite difficult to understand other dialects). In the past I've seen little emphasis on separating Arabic into distinct dialects, but rather focusing on classical Arabic. What approach have you all taken to this challenge?",/r/MachineLearning/comments/w4jg7q/d_hey_reddit_were_a_bunch_of_research_scientists/ih2h432/
1658418439.0,21-Jul-2022 08:47:19,dojoteef,MachineLearning,"Translating literary works is often more difficult than translating news or Wikipedia articles. Increased use of idioms, story-specific terms, and colorful phrasing often results in translationese. Interestingly, earlier human translations of some literary works contained more translationese, though that appears to have changed over time (this is according to some folks in my lab who are working on translating novels). Given that this is a hard problem, and even some of the parallel training data might have these artifacts, are there any specific approaches you all are taking to overcome these challenges with your work on translation of children's stories?",/r/MachineLearning/comments/w4jg7q/d_hey_reddit_were_a_bunch_of_research_scientists/ih2bb0m/
1658406726.0,21-Jul-2022 05:32:06,dojoteef,MachineLearning,"No they don't replace all parameters with low rank layers. Read section 4.2. The speed benefit only exists when finetuning on a downstream task, because you can freeze layers (thus not calculate gradients) and still get the similar performance. The only convert attention weights in a Transformer, but freeze the MLP weights.

I'd recommend reading through the paper more carefully.",/r/MachineLearning/comments/w4eq0z/d_in_an_mlp_model_if_i_disable_the_gradient_in/ih1j9gt/
1658405758.0,21-Jul-2022 05:15:58,dojoteef,MachineLearning,"You should give enough context for people to know what exactly you're referring to. I presume you're using [the code](https://github.com/microsoft/LoRA) from the paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685). If so, then the speed and memory benefits depend on the hyperparameters you choose for the low rank layers. Additionally, the paper states they get a 25% speed increase training a version of GPT-3 (175B parameters) using their approach (with `r=4`). It could be that you don't see much benefit unless you have extremely large models.",/r/MachineLearning/comments/w4eq0z/d_in_an_mlp_model_if_i_disable_the_gradient_in/ih1hets/
1658322057.0,20-Jul-2022 06:00:57,dojoteef,MachineLearning,"You can either train BERT+model2 in an end-to-end fashion (backprop through both models), which will require enough compute and memory, or you can keep the BERT embeddings fixed and only backprop through model2 (and live with any error accumulation due to mismatched BERT). That said, if you have very little data, then keeping BERT fixed might actually be better. Though if you have lots of diverse data, then the end-to-end approach might produce better results.",/r/MachineLearning/comments/w3lmn0/p_technique_to_stack_models_on_top_of_each_other/igwtpfn/
1657913098.0,15-Jul-2022 12:24:58,dojoteef,MachineLearning,Please do not have shill accounts upvote and comment on your post.,/r/MachineLearning/comments/vzwdh6/p_nbsnapshot_automated_jupyter_notebook_testing/igaw75n/
1657886505.0,15-Jul-2022 05:01:45,dojoteef,MachineLearning,"See https://pytorch.org/docs/stable/notes/randomness.html for more info on why you are not guaranteed the same results.

Also, floating point calculations *are* order dependent (not all operations are distributive, associative, or commutative). So reordering calculations can result in different outputs. See https://en.wikipedia.org/wiki/Floating-point_arithmetic#Accuracy_problems",/r/MachineLearning/comments/vziqnu/deleted_by_user/ig94zan/
1657807442.0,14-Jul-2022 07:04:02,dojoteef,MachineLearning,[Transformer-XL](https://openreview.net/forum?id=HJePno0cYm) was originally rejected. It has [lots of citations](https://www.semanticscholar.org/paper/Transformer-XL%3A-Attentive-Language-Models-beyond-a-Dai-Yang/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6).,/r/MachineLearning/comments/vywfx3/d_are_there_any_rejected_papers_that_ended_up/ig4nb95/
1657750153.0,13-Jul-2022 15:09:13,dojoteef,MachineLearning,Certainly an interesting marketing approach. Expose flawed datasets by using your platform to relabel the data and investigate the discrepancies. Might be useful to consolidate your learnings and screening methodology into a conference paper (if you haven’t already).,/r/MachineLearning/comments/vye69k/30_of_googles_reddit_emotions_dataset_is/ig1t6c8/
1657645323.0,12-Jul-2022 10:02:03,dojoteef,MachineLearning,"That’s a pretty common technique. For example, you might use a re-ranker on some generated text to choose the text that best fits the purpose (e.g. a summary that more closely matches the source article, a translation that more closely matches the source sentence).",/r/MachineLearning/comments/vxfy50/d_does_it_make_sense_to_generate_text_sequences/ifvnaf0/
1657130356.0,06-Jul-2022 10:59:16,dojoteef,MachineLearning,"Casual machine learning is a very active field. For example, check out this [recent survey paper](https://arxiv.org/abs/2206.15475).

The paper is 124 pages long (excluding the bibliography), with a bibliography that is 41 pages in length. Admittedly, this is a very crude metric and not all of the citations are directly causal ML related, but nonetheless it demonstrates that more than a handful of researchers are interested in the area.",/r/MachineLearning/comments/vsvgoz/d_why_arent_there_much_people_working_on_causal/if3popn/
1656970075.0,04-Jul-2022 14:27:55,dojoteef,MachineLearning,Check out [A Continuous Relaxation of Beam Search for End-to-End Training of Neural Sequence Models](https://www.semanticscholar.org/paper/A-Continuous-Relaxation-of-Beam-Search-for-Training-Goyal-Neubig/fee62123e1d2ac56065675983475b079e1e9106f) and the work that cites it. Similar approaches have been used to guide generation for style and other attributes (via classifiers like you’re trying to do).,/r/MachineLearning/comments/vrhlfa/deleted_by_user/iev83o7/
1656947500.0,04-Jul-2022 08:11:40,dojoteef,MachineLearning,Stop reposting the same research under different accounts.,/r/MachineLearning/comments/vr9eic/r_can_interpretability_improve_model_accuracy/ietr2t8/
1656417024.0,28-Jun-2022 04:50:24,dojoteef,MachineLearning,"Transformers with relative positional encodings can, e.g. see [Transformer XL](https://paperswithcode.com/method/transformer-xl).",/r/MachineLearning/comments/vml9we/dcan_a_transformer_neural_network_learn_to/ie1ldc3/
1656379496.0,27-Jun-2022 18:24:56,dojoteef,MachineLearning,"Since it unifies named tensors with einops, does it share some of the same performance degradation issues that einops have?",/r/MachineLearning/comments/vmabau/p_firstclass_dims_a_generalization_of_einops_and/ie01emc/
1656333407.0,27-Jun-2022 05:36:47,dojoteef,MachineLearning,"Please don’t ask questions that can easily be found somewhere else. Quite literally the first result when doing a search online for “biggan”:

https://paperswithcode.com/method/biggan",/r/MachineLearning/comments/vlu13v/research_what_were_some_of_the_best_practices/idx7qri/
1656263696.0,26-Jun-2022 10:14:56,dojoteef,MachineLearning,"In the future, please provide links to [the project](https://vizcom.ai) and any background information on what ML is used.

After a quick search, I found info about the tech stack here: https://read.cv/teams/vizcom

>	* Pytorch
>	* Generative Models (Image to Image Translation and 3D Object Reconstruction)
>	* Inference: FastAPI, EC2
>	* Train locally
>	* Graph Neural Networks",/r/MachineLearning/comments/vl7iut/p_a_drawing_application_called_vizcom_that_uses/idtry3i/
1656191640.0,25-Jun-2022 14:14:00,dojoteef,MachineLearning,"This is a very naive take. Are there papers that get published solely based on name recognition — sure. Just like some clearly plagiarized papers get published. Is it a rampant phenomenon? That’s highly doubtful and such a strong claim would require clear evidence.

I’m not aware of anyone who thinks reviewing is an unbiased endeavor that’s based purely on merit; rather it’s perceived to be a noisy process that has many flaws. That fact has been often repeated on this sub and on academic Twitter. So most ML venues understand there are problems and are continuing to tweak the reviewing processes to improve the situation (how successful that has been is certainly up for debate).

That said, I would venture that problems like reviewer mismatch or apathy play a larger role in general than bias due to name recognition (whether by a senior AC, or by posting a preprint before submission).",/r/MachineLearning/comments/vkc7fo/research_not_all_our_papers_get_published/idqcrwe/
1656004268.0,23-Jun-2022 10:11:08,dojoteef,MachineLearning,Please do not post this again.,/r/MachineLearning/comments/vizj2f/deleted_by_user/idgbckq/
1655897357.0,22-Jun-2022 04:29:17,dojoteef,MachineLearning,This really isn’t the place to ask. You should email the conference chairs and ask.,/r/MachineLearning/comments/vi2z2y/d_can_the_deadline_of_aaai_2023_be_extended/idajkkq/
1655897274.0,22-Jun-2022 04:27:54,dojoteef,MachineLearning,"I’m not sure what kind of feedback you’re looking for, but why not simply use a masked language model like BERT? You can randomly mask 15% of the tokens (like BERT’s training), then predict those tokens. Repeat until you only have predicted tokens. In this way, it can be thought of like a form of MCMC sampling.

See [BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model](https://arxiv.org/abs/1902.04094) for more info.",/r/MachineLearning/comments/vhzk9e/d_iterative_imputation_hmm_sequence_generation/idajg40/
1655811562.0,21-Jun-2022 04:39:22,dojoteef,MachineLearning,Try r/learnmachinelearning,/r/MachineLearning/comments/vhaj8c/deleted_by_user/id65svc/
1655594407.0,18-Jun-2022 16:20:07,dojoteef,MachineLearning,"It’s not a bug. There was no tag in the title of your link post. That said, the post is not a good fit for the sub, so would probably be manually removed.

In the future, send a modmail rather than making a post like this.",/r/MachineLearning/comments/vfhcwp/d_meta_reposted_link_limitation/icvwllf/
1655061502.0,12-Jun-2022 12:18:22,dojoteef,MachineLearning,"The sub is not purely research focused and hasn’t ever been. That’s why [tags were introduced](https://reddit.com/r/MachineLearning/comments/56hdqi/no_shirt_no_tags_no_service_posts_without/) for projects, news, and discussions in addition to research. You can always filter by tag.

And there has long been a push to reduce beginner content (e.g. [2 yrs ago](https://reddit.com/r/MachineLearning/comments/co37ut/regarding_beginners_guides/), [5 yrs ago](https://reddit.com/r/MachineLearning/comments/4s175l/growing_pains_of_rmachinelearning_more_active/), [8 yrs ago](https://reddit.com/r/MachineLearning/comments/1zje5g/meta_questions_get_downvoted_in_this_sub_so_lets/)), which we try our best to enforce. For example in the last 24 hours the mods collectively removed 44 posts out of the 56 that were posted (most by the AutoModerator, but I alone removed 9 beginner posts).

Of course, if you see content that you don’t think belongs, feel free to report it.",/r/MachineLearning/comments/v8smlm/d_request_for_moderators/ic4b8he/
1654264080.0,03-Jun-2022 06:48:00,dojoteef,MachineLearning,A better place to post would be r/learnmachinelearning. This subreddit does not focus on beginner content like this. Good luck!,/r/MachineLearning/comments/v3yz4j/d_machine_learning_and_autism_looking_for_an_open/ib1bkdg/
1654214651.0,02-Jun-2022 17:04:11,dojoteef,MachineLearning,"Also, slurm allows interactive debugging by using `srun` instead of `sbatch`.",/r/MachineLearning/comments/v3iskb/d_building_a_ai_training_cluster/iaz5nh9/
1653499509.0,25-May-2022 10:25:09,dojoteef,MachineLearning,Here’s a link to the [author’s response](https://reddit.com/r/MachineLearning/comments/u7ouxh/_/i9wz5h7/?context=1) in the comments.,/r/MachineLearning/comments/u7ouxh/r_authors_claim_to_have_solved_mnist_and_cifar/i9yk602/
1652902147.0,18-May-2022 12:29:07,dojoteef,MachineLearning,Consider [Phrase-BERT](https://arxiv.org/abs/2109.06304),/r/MachineLearning/comments/usisgt/discussion_are_there_any_better_topic_modelling/i940os7/
1652654503.0,15-May-2022 15:41:43,dojoteef,MachineLearning,"Note, the name of the conference is [NeurIPS](https://neurips.cc/).",/r/MachineLearning/comments/uqfx7f/d_is_it_possible_to_submit_a_pure_math_paper_to/i8r1gz0/
1652013318.0,08-May-2022 05:35:18,dojoteef,MachineLearning,"Just do a quick search for it online. Pretty much every major conference has guidance on how to be a good reviewer, e.g. [CVPR](https://cvpr2022.thecvf.com/sites/default/files/2021-11/How%20to%20be%20a%20good%20reviewer-tutorials%20for%20cvpr2022%20reviewers.pptx.pdf) and [ACL](https://aclrollingreview.org/reviewertutorial).",/r/MachineLearning/comments/ukttgx/d_tips_for_reviewing_at_top_a_conferences/i7sgx0m/
1651061079.0,27-Apr-2022 05:04:39,dojoteef,MachineLearning,"Typically, this is what a cache (e.g. Redis) is for. Also, you should only worry about consistency if that’s a requirement. If so, checking a model version before returning the prediction seems reasonable.

EDIT: btw, I’d recommend r/mlops for these kind of questions.",/r/MachineLearning/comments/ud1sel/d_how_to_storesurface_predictions_along_with/i6e2azs/
1651060466.0,27-Apr-2022 04:54:26,dojoteef,MachineLearning,Questions like these should either go to stackoverflow or r/learnmachinelearning,/r/MachineLearning/comments/uczty8/deleted_by_user/i6e18kf/
1650479756.0,20-Apr-2022 11:35:56,dojoteef,MachineLearning,Great intro! It was very clear and well placed for people already familiar with variational inference.,/r/MachineLearning/comments/u81xcu/d_diffusion_models_video_tutorial/i5ij9tr/
1650467439.0,20-Apr-2022 08:10:39,dojoteef,MachineLearning,Please don’t imply that we are the ones downvoting. I haven’t downvoted. In fact I’ve just seen this post. Feel free to ask the community their thoughts.,/r/MachineLearning/comments/u7wwzb/d_whats_your_opinion_on_project_promoting_posts/i5hncth/
1650454087.0,20-Apr-2022 04:28:07,dojoteef,MachineLearning,Ok. But please be respectful regarding self-promotion in the future. We catch enough flak from people complaining about the frequency of self-promotion on the sub.,/r/MachineLearning/comments/u7633f/d_nlp_has_huggingface_what_does_computer_vision/i5gurmp/
1650453650.0,20-Apr-2022 04:20:50,dojoteef,MachineLearning,"You have repeatedly posted self-promotion this week. These are still up:

https://reddit.com/r/MachineLearning/comments/u3xjr7/d_kubernetes_for_ml_how_are_yall_doing_it/

https://reddit.com/r/MachineLearning/comments/u2pi8b/p_how_and_where_do_you_serve_your_model_using/

https://reddit.com/r/MachineLearning/comments/u40qlq/d_do_you_train_and_deploy_models_using_just_one/

but I had to take others down because you posted about your project a bunch this week. You sure certainly not following the reddiquette advice you are quoting here. Additionally, you edited the post after it got popular to promote your work further.",/r/MachineLearning/comments/u7633f/d_nlp_has_huggingface_what_does_computer_vision/i5gu3h5/
1650452530.0,20-Apr-2022 04:02:10,dojoteef,MachineLearning,Removed as a shameless bait and switch to promote your work.,/r/MachineLearning/comments/u7633f/d_nlp_has_huggingface_what_does_computer_vision/i5gsetb/
1650407878.0,19-Apr-2022 15:37:58,dojoteef,MachineLearning,Please do not repost,/r/MachineLearning/comments/u7ac80/deleted_by_user/i5emsfn/
1650407616.0,19-Apr-2022 15:33:36,dojoteef,MachineLearning,Please don’t repost,/r/MachineLearning/comments/u789dt/deleted_by_user/i5em5jq/
1650291974.0,18-Apr-2022 07:26:14,dojoteef,MachineLearning,The term you’re looking for is [zero-shot classification](https://en.m.wikipedia.org/wiki/Zero-shot_learning). A well known recent example is [CLIP](https://openai.com/blog/clip/).,/r/MachineLearning/comments/u6cjwx/d_which_keywords_describe_my_task/i57n2uc/
1650241434.0,17-Apr-2022 17:23:54,dojoteef,MachineLearning,Please see the [author’s response](https://reddit.com/r/MachineLearning/comments/u5pxvh/_/i55785a/?context=1).,/r/MachineLearning/comments/u5pxvh/d_is_it_ok_to_promise_a_dataset_in_your_paper_get/i55dcgo/
1650137014.0,16-Apr-2022 12:23:34,dojoteef,MachineLearning,"I sympathize, but you have to understand that the nearly all post and comment removals are done by just u/programmerChilli and I. The sub is large and we don’t have the time to be on reddit 24/7 so unfortunately some of the posts and comments sit for a while before we have a chance to remove them (like the ones you listed were on reddit for a couple of hours before I had a chance to remove them).

Also, we sometimes make a mistake in a removal, so feel free to send a modmail asking about a removal if you want us to reconsider (you’ve deleted the post now so I cannot see what it contained).

That said, I’ve recently spoken with the mods about adding moderators.",/r/MachineLearning/comments/u53hb3/d_moderation_uniformity_in_subreddit/i4zqlqd/
1650135993.0,16-Apr-2022 12:06:33,dojoteef,MachineLearning,You ahead posted once. Please do not repost.,/r/MachineLearning/comments/u4v7nx/p_new_marketplace_for_ai_labeled_data/i4zoduo/
1648129988.0,24-Mar-2022 06:53:08,dojoteef,MachineLearning,"It takes time to grow a sub and I don’t think the number of subscribers is indicative of the quality of discussion. While this sub is more active in general, I hesitate to say it has more mlops discussion.

A quick search indicates there are [four posts that mention mlops](https://www.reddit.com/r/MachineLearning/search/?q=mlops&restrict_sr=on&t=month) in the past month on r/ml (one of which is this post), while [there are 23](https://www.reddit.com/r/mlops/search/?q=mlops&restrict_sr=on&t=month) on r/mlops. While that’s certainly a crude measure, it lends some credence to the notion that r/mlops isn’t a waste of your time.

\* Sorry if I was a bit curt, I should have likely phrased it as: “You might want r/mlops.”",/r/MachineLearning/comments/tm39o4/d_what_mlops_platform_do_you_use_and_how_helpful/i1xiayj/
1648124714.0,24-Mar-2022 05:25:14,dojoteef,MachineLearning,You want r/mlops.,/r/MachineLearning/comments/tm39o4/d_what_mlops_platform_do_you_use_and_how_helpful/i1x7e16/
1648124016.0,24-Mar-2022 05:13:36,dojoteef,MachineLearning,"Go to [Semantic Scholar](https://www.semanticscholar.org) or [Google Scholar](https://scholar.google.com), search for the paper, then look through its citations.",/r/MachineLearning/comments/tlycan/d_examples_of_nn_initialization_strategy_inspired/i1x64ty/
1648108085.0,24-Mar-2022 00:48:05,dojoteef,MachineLearning,"Do a literature review. Found this paper within two minutes of searching:

[Rare Gems: Finding Lottery Tickets at Initialization](https://arxiv.org/abs/2202.12002)",/r/MachineLearning/comments/tlycan/d_examples_of_nn_initialization_strategy_inspired/i1wl5sv/
1647692179.0,19-Mar-2022 05:16:19,dojoteef,MachineLearning,"In the future, please disclose your affiliations. This includes u/Stormfreek. We often get reports of deceptive advertising due to a lack of disclosure from company representatives and frequently take posts down for that reason.",/r/MachineLearning/comments/tgefsw/discussion_pytorch_lightning_vs_deepspeed_vs_ffcv/i19tgg2/
1647550578.0,17-Mar-2022 13:56:18,dojoteef,MachineLearning,"You could use a regular dialog model (e.g. DialogGPT), then use [unsupervised style transfer](https://arxiv.org/abs/2010.05700) on the generated utterance.",/r/MachineLearning/comments/tghpun/r_domainspecific_pretraining_of_gpt_help/i12hdba/
1647517992.0,17-Mar-2022 04:53:12,dojoteef,MachineLearning,"This isn’t true. With data parallelism you can use different batch sizes per device, so you can indeed use the full memory of both devices.",/r/MachineLearning/comments/tg76ns/gpus_and_memory_d/i10a8sz/
1647370482.0,15-Mar-2022 11:54:42,dojoteef,MachineLearning,Please don’t post this a third time. It was removed for being a beginner question. You might ask in r/learnmachinelearning  instead.,/r/MachineLearning/comments/teucsb/deleted_by_user/i0s8sw1/
1646922863.0,10-Mar-2022 06:34:23,dojoteef,MachineLearning,"Not sure why, but this post is being targeted by some troll accounts — never posted in r/MachineLearning before, but downvoting and commenting about censorship (which fact checking is not) within minutes of the post going live.",/r/MachineLearning/comments/tazl2g/this_is_how_we_battle_fake_news_with_ai_project/i03zcec/
1646229611.0,02-Mar-2022 06:00:11,dojoteef,MachineLearning,"Please try to limit self-promotion to no more than once a month, and then only when significant changes happen.",/r/MachineLearning/comments/t4jwn3/p_python_fastest_auto_arima_extension_exogenous/hz1sf6k/
1646182632.0,01-Mar-2022 16:57:12,dojoteef,MachineLearning,"This was posted a week ago. Please don’t repost, especially self-promotion.",/r/MachineLearning/comments/t4jwn3/p_python_fastest_auto_arima_extension_exogenous/hyzjzwn/
1645888654.0,26-Feb-2022 07:17:34,dojoteef,MachineLearning,"I haven’t followed the CV side of things, but you could check this NLP paper out and see if the approach can be adapted to vision:

https://arxiv.org/abs/1910.12366",/r/MachineLearning/comments/t1tiwu/d_model_extraction_attack/hyisro9/
1645848059.0,25-Feb-2022 20:00:59,dojoteef,MachineLearning,"I sincerely doubt the majority of papers that can’t be reproduced are intentionally being nefarious. If anything, many researchers do not spend the extra effort to ensure their work is easily reproducible, even for them. There are often many manual steps that are left undocumented. It’s really a problem of misaligned incentives — historically reproducibility has not been a focus in conference reviewing. This is definitely changing (at least in NLP), so I’m hopeful this problem will improve in time, though I doubt you will ever get the percentage of papers that cannot be reproduced down to zero.",/r/MachineLearning/comments/t1lzqp/deleted_by_user/hyh2eqh/
1645287329.0,19-Feb-2022 08:15:29,dojoteef,MachineLearning,Please don’t post this tutorial again.,/r/MachineLearning/comments/swbhr1/deleted_by_user/hxlal35/
1645273468.0,19-Feb-2022 04:24:28,dojoteef,MachineLearning,Please do not ask this question a third time.,/r/MachineLearning/comments/sw104n/deleted_by_user/hxkjapl/
1645136458.0,17-Feb-2022 14:20:58,dojoteef,MachineLearning,Yes.,/r/MachineLearning/comments/suxwkp/d_the_science_and_information_organization_sai/hxd9who/
1644326606.0,08-Feb-2022 05:23:26,dojoteef,MachineLearning,Interesting. It’s like a multimodal T5. Essentially everything is sequence to sequence with a unified vocab across modalities.,/r/MachineLearning/comments/snia53/r_paper_unifying_architectures_tasks_and/hw2u92n/
1644249757.0,07-Feb-2022 08:02:37,dojoteef,MachineLearning,"Again, please post these questions to r/learnmachinelearning  instead.",/r/MachineLearning/comments/smtfag/d_classifying_machine_learning_problems_as_p_np/hvyib67/
1644249628.0,07-Feb-2022 08:00:28,dojoteef,MachineLearning,"https://www.levels.fyi/Salaries/Software-Engineer/London/

You can filter by years of experience and ML/AI as a keyword.",/r/MachineLearning/comments/smsr9b/d_senior_phd_salaries_in_london/hvyhzb5/
1644237491.0,07-Feb-2022 04:38:11,dojoteef,MachineLearning,Please ask these types of questions in r/learnmachinelearning  instead.,/r/MachineLearning/comments/smihot/d_comparing_model_solutions_from_saddle_points_vs/hvxrrs6/
1643926034.0,03-Feb-2022 14:07:14,dojoteef,MachineLearning,The model can easily be reverse engineered even if you go with a server-based approach. See [Thieves on Sesame Street! Model Extraction of BERT-based APIs](https://martiansideofthemoon.github.io/2020/04/04/iclr20.html),/r/MachineLearning/comments/sjt557/d_share_your_mldl_model_as_black_box/hvh9xcg/
1643687841.0,31-Jan-2022 19:57:21,dojoteef,MachineLearning,"Great discussion, thanks for sharing!",/r/MachineLearning/comments/shijyk/r_diffusion_models_are_autoencoders/hv388wt/
1643681589.0,31-Jan-2022 18:13:09,dojoteef,MachineLearning,"You want to look at statistical tests that assess the likelihood of a real effect (i.e reject the null hypothesis). Sadly, lots of research in ML ignores this important aspect of scientific inquiry.

See for example: [With Little Power Comes Great Responsibility](https://aclanthology.org/2020.emnlp-main.745/)

I’d great that you’re asking about it though!",/r/MachineLearning/comments/shg9bx/discussion_how_to_obtain_meaningful_conclusions/hv2u9oa/
1643588832.0,30-Jan-2022 16:27:12,dojoteef,MachineLearning,"Ditto. I rarely knock papers for lack of novelty, unless it is clearly well-trodden territory (I hate having to rate novelty on a scale as a criterion in reviews). Yet, I do look favorably on papers that I perceive as novel.

On a side note, I abhor when authors repeatedly pepper their paper with claims of novelty. It seems fine to state you have a novel idea at most 1-3 times (abstract, intro, and conclusion), but beyond that it just feels like a cudgel; it’s as if the authors feel reviewers will simply believe there’s novelty if only they repeat it often enough.",/r/MachineLearning/comments/sgjnor/d_novelty_in_science_a_guide_for_reviewers/huxdrmc/
1643031043.0,24-Jan-2022 05:30:43,dojoteef,MachineLearning,Please include a link to the original paper when you post about research.,/r/MachineLearning/comments/sbl7qf/r_paper_summary_masked_autoencoders_are_scalable/hu0lixp/
1643000214.0,23-Jan-2022 20:56:54,dojoteef,MachineLearning,See: https://openaccess.thecvf.com/content_CVPR_2019/papers/Alcorn_Strike_With_a_Pose_Neural_Networks_Are_Easily_Fooled_by_CVPR_2019_paper.pdf,/r/MachineLearning/comments/sbbsrr/has_there_been_any_work_into_the_effectiveness_of/htzd0fx/
1642885419.0,22-Jan-2022 13:03:39,dojoteef,MachineLearning,"From your website, what does it mean that your data is “Encrypted in transit and at rest”? Does that just mean you use SSL (since you state you don’t store the data)?

Clearly you have to use some form of language model to predict documentation (are you using Codex? if not, what do you use?). I’m guessing at least that model sees user data in clear text, right? If so, is there any logging being done that can inadvertently expose the data (even if it’s through a stack trace due to an exception)?",/r/MachineLearning/comments/sab6tk/p_documentation_generated_using_ai/htshb2o/
1642864251.0,22-Jan-2022 07:10:51,dojoteef,MachineLearning,"In the future, if a paper already has a project website that fully details the research, please do not post a link to your popsci video covering the research. It’s fine to post the link to the project website or a video from the authors though.

Note, this paper has a perfectly good [overview video](https://m.youtube.com/watch?v=4zKliOMilGY) that’s very accessible and light on the technical details.",/r/MachineLearning/comments/sa1m2e/research_animating_pictures_with_eulerian_motion/htr0lki/
1642391836.0,16-Jan-2022 19:57:16,dojoteef,MachineLearning,"One of the important aspects of dialog in video games is that it’s grounded in the game world. If a GPT3 powered NPC agrees to sell an item, how do you realize the result in the game world? Same goes with any dialog that implies a change in the game state. Do you currently have some framework to support this?",/r/MachineLearning/comments/s5ol2u/p_discord_community_for_game_developers_using_ml/hszv0v2/
1641667192.0,08-Jan-2022 10:39:52,dojoteef,MachineLearning,"The first few times GANs were trained on NSFW datasets, there might have been some novelty, but at this point that novelty has been played out. Locking this post. Future posts like this will be removed unless there is more novelty than “look I trained StyleGAN on another NSFW dataset”.",/r/MachineLearning/comments/rz3m7i/p_stylegan_to_generate_pairs_of_boobs/hrt3of2/
1641148467.0,02-Jan-2022 10:34:27,dojoteef,MachineLearning,"Yes, it slows things down, but it’s only for debugging. Almost any debugging (except post-mortem debugging) has a performance penalty.",/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/hqypydd/
1641137081.0,02-Jan-2022 07:24:41,dojoteef,MachineLearning,"Normally for frameworks like PyTorch, the CUDA kernels run asynchronously in the background. That means the program counter in python might have moved on to a new instruction when the CUDA kernel raises an exception. You can make the CUDA kernels run synchronously by setting the environment variable `CUDA_LAUNCH_BLOCKING=1`, thus getting useful callstacks from the error. The frameworks don’t have an incentive to do better than that since the callstack combined with the CUDA error is usually enough to figure things out.

It might be possible to do a better job of reporting error messages in these cases, but the effort likely isn’t worth it compared to other potential improvements.",/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/hqxx7lt/
1640966229.0,31-Dec-2021 07:57:09,dojoteef,MachineLearning,"Nested sampling with MultiNest is pretty fast, so I’m not sure you’re going to significantly improve your efficiency, but it can’t hurt to try other approaches.

I presume you can’t easily find a proposal distribution closer to your true distribution? That’s typically the way to speed MCMC methods up.

You can compare visualizations for a number of MCMC methods (and some non-Markovian approaches) in action on this site:

http://chi-feng.github.io/mcmc-demo/

It is a low dimensional demo, so your results may vary if your distribution is high dimensional. Maybe try something like [Stein Variational Gradient Descent](https://papers.nips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf) if you haven’t already.",/r/MachineLearning/comments/rsv2o4/d_machine_learning_alternative_to_mcmc_or_nested/hqoyx02/
1640026118.0,20-Dec-2021 10:48:38,dojoteef,MachineLearning,This would be best answered on r/learnmachinelearning,/r/MachineLearning/comments/rks2yz/discussion_educative_dataset_for_students/hpbr0w9/
1639832207.0,18-Dec-2021 04:56:47,dojoteef,MachineLearning,This question would be better answered on r/learnmachinelearning.,/r/MachineLearning/comments/rj31e6/d_ml_for_timing_anomalies_detection/hp1kg6x/
1639795315.0,17-Dec-2021 18:41:55,dojoteef,MachineLearning,"This is your third post in two days discussing your work. Please try to follow [reddiquette on self promotion](https://www.reddit.com/wiki/selfpromotion):

>	a general rule of thumb is that 10% or less of your posting and conversation should link to your own content",/r/MachineLearning/comments/riptnt/research_adam_optimizer_from_inception_through/hp04ekn/
1639768013.0,17-Dec-2021 11:06:53,dojoteef,MachineLearning,"Did you look at the PlotNeuralNet examples?

https://github.com/HarisIqbal88/PlotNeuralNet/tree/master/examples/Unet_Ushape",/r/MachineLearning/comments/rin63t/d_best_tool_for_drawing_unet_style_diagrams/hoyd1rc/
1639540300.0,14-Dec-2021 19:51:40,dojoteef,MachineLearning,Removed since it’s a duplicate of a post from this morning,/r/MachineLearning/comments/rgorf6/r_glam_efficient_scaling_of_language_models_with/holoz9t/
1639540260.0,14-Dec-2021 19:51:00,dojoteef,MachineLearning,Removed since it’s a duplicate of a post from this morning,/r/MachineLearning/comments/rgojkz/r_you_only_need_one_model_for_opendomain_question/holowdd/
1639225740.0,11-Dec-2021 04:29:00,dojoteef,MachineLearning,"Thanks for the interesting conversation. How do you propose people address this issue in practice? For example, I’m not sure there is enough detail about the dataset to retrain Megatron to ensure a fair comparison since they perform deduplication — is a raw version of the exact dataset available? The training dataset definitely has an effect on final perplexity.

The only way I can see is ignoring the training dataset and ensure a consistent experimental protocol across the baselines you train, but then it’s not reasonable to compare to certain models like Megatron and GPT-3, where the main improvement is scale, not modeling.

Ultimately, I think you’re right that the ideal approach is to ensure consistency across all baselines when possible, but when it’s not possible, make that explicit in your table (which in some sense they did since it’s separated from their model and baseline transformer they trained).",/r/MachineLearning/comments/rdkwx9/r_improving_language_models_by_retrieving_from/ho4421a/
1639195030.0,10-Dec-2021 19:57:10,dojoteef,MachineLearning,"I understand your concern, which is also discussed in the EMNLP 2021 paper [You should evaluate your language model on marginal likelihood over tokenisations](https://aclanthology.org/2021.emnlp-main.161/). As discussed in the paper, while computing the exact marginal is likely intractable, approximating it is totally reasonable. As Table 1 shows, the effect of marginalization over tokenizations acts to lower the perplexity. Thus the naive approach leads to a higher (worse) perplexity. In 2019 Sabrina Mielke mentions in [this blog post](https://sjmielke.com/comparing-perplexities.htm) a similar argument, and notes that subword (open) vocabularies are likely penalized compared to word-level (closed) vocabularies when it comes to perplexity. These both point to the fact that if anything, the dominant approach to reporting perplexity for subword models actually overestimates the perplexity — if they report a lower perplexity than another model then it likely does represent an improvement.

That said, I agree that ideally people should reproduce results rather than copying from another paper, but that can also be faulty (if they don’t do a good job of reproducing the paper).

tldr; the approach of dividing by a fixed sequence length allows for comparison, and if you want to produce an even better estimate of perplexity then you can certainly marginalize over tokenizations as well.",/r/MachineLearning/comments/rdkwx9/r_improving_language_models_by_retrieving_from/ho2uqvn/
1639186091.0,10-Dec-2021 17:28:11,dojoteef,MachineLearning,"Are you sure these are not comparable? You can indeed compare across tokenizations, see section 4.2 of the [Compressive Transformer](https://arxiv.org/abs/1911.05507) paper which introduces the PG-19 dataset. Since [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) has a fixed number of tokens (it’s pre-tokenized), similar to PG-19, the approach described in the Compressive Transformer works fine.

I don’t see any clear indication from this paper how they computed perplexity, which usually makes me give the authors the benefit of the doubt (or when reviewing, it’s a point I verify with the authors).",/r/MachineLearning/comments/rdkwx9/r_improving_language_models_by_retrieving_from/ho2bgt7/
1638970196.0,08-Dec-2021 05:29:56,dojoteef,MachineLearning,Removed as a repost,/r/MachineLearning/comments/rbn6oe/deepminds_player_of_games/hnpqx3l/
1638763960.0,05-Dec-2021 20:12:40,dojoteef,MachineLearning,This is not the place for debugging code. Please do not post this again.,/r/MachineLearning/comments/r9xkfk/d_attributeerror_list_object_has_no_attribute_to/hnf43qw/
1638659184.0,04-Dec-2021 15:06:24,dojoteef,MachineLearning,"I’m not sure if anyone has explicitly done this, but previous work has used discrete latents to allow modeling video in latent space — [Latent Video Transformer](https://arxiv.org/abs/2006.10704) is an example. Presumably, if subsequent frames are visually similar, then they should have latent representations that are more similar.

Note that explicitly enforcing consecutive frames to be nearby in latent space might not work well if there are abrupt scene changes, which happens frequently in edited videos (e.g. tv shows, movies, etc).",/r/MachineLearning/comments/r90f3v/d_embedding_consecutive_video_frames_close_to/hn94fy4/
1638536824.0,03-Dec-2021 05:07:04,dojoteef,MachineLearning,You posted about this yesterday. Please do not post about this again.,/r/MachineLearning/comments/r7utuo/p_pls_help/hn29r1i/
1638536226.0,03-Dec-2021 04:57:06,dojoteef,MachineLearning,"Pretentious much? Some top programs, like Stanford, have a [first year research rotation](https://cs.stanford.edu/academics/phd/first-year-research-rotation-program) for incoming PhD students. So yes, some programs do work this way.",/r/MachineLearning/comments/qv0jg1/d_does_getting_into_top4stanford_cmu_mit_berkeley/hn28qgc/
1638497485.0,02-Dec-2021 18:11:25,dojoteef,MachineLearning,"You’re certainly entitled to your opinion,  
but as a PhD student who focuses on NLP, I can definitively say my university, along with many other top US universities I know, considers ACL conferences to be the best place for publishing NLP papers. We aren’t settling for ACL conferences, while pining after the likes of NeurIPS.

The main difference is that linguistic analysis is valued and expected at ACL conferences, contrary to venues like NeurIPS, ICLR, and ICML.",/r/MachineLearning/comments/r79oqv/d_aaai_manipulated_reviewer_scores_without/hn0jn6c/
1638456100.0,02-Dec-2021 06:41:40,dojoteef,MachineLearning,"I understand the desire to not be scooped, but flag planting isn’t great. The current preprint (revision #2) states:

>	Results on CIFAR10 task and other datasets will be added to this preprint upon completion.

The paper is sorely lacking an analysis section, with just a few short blurbs like:

>	The current model was found to be an order of magnitude more memory efficient on the BPE text classification task in comparison with the vanilla transformer.

That’s the entirety of the discussion on the memory implications. Similar areas of analysis are given equally short shrift.",/r/MachineLearning/comments/r703rj/r_pureformer_do_we_even_need_attention/hmxpcj8/
1638115345.0,28-Nov-2021 08:02:25,dojoteef,MachineLearning,First we lauded Sesame Street characters like ELMo and BERT for improving language understanding. Now we’re evoking Sesame Street storybooks to address philosophy of science questions about evaluating language understanding. Who knew Sesame Street was so versatile?,/r/MachineLearning/comments/r45wdo/r_ai_and_the_everything_in_the_whole_wide_world/hmeuqbo/
1637696378.0,23-Nov-2021 11:39:38,dojoteef,MachineLearning,"Many of the top CS programs in the US have dropped the GRE for PhD admissions (not necessarily for MS admissions). In fact, [research](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0206570) has shown that higher GRE scores actually have a slight negative correlation with completing a PhD.",/r/MachineLearning/comments/r0428v/d_regarding_phd_admissions_in_ml_how_much_will/hlt1n4f/
1637331806.0,19-Nov-2021 06:23:26,dojoteef,MachineLearning,"Check out the best paper from ACL 2020:

[Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://aclanthology.org/2020.acl-main.442/)",/r/MachineLearning/comments/qxeu1x/d_stresstesting_models_to_ensure_robustness/hl99vrw/
1635948126.0,03-Nov-2021 07:02:06,dojoteef,MachineLearning,Lots of interesting info. Thanks for sharing!,/r/MachineLearning/comments/qlilnf/n_zillows_nnbased_zestimate_leads_to_massive/hj5db2k/
1635945006.0,03-Nov-2021 06:10:06,dojoteef,MachineLearning,That’s an interesting tidbit. Thanks for sharing! Are you able to share any additional differences between the two forecasting models?,/r/MachineLearning/comments/qlilnf/n_zillows_nnbased_zestimate_leads_to_massive/hj56e5z/
1635599125.0,30-Oct-2021 06:05:25,dojoteef,MachineLearning,"Here’s the YouTube link from the authors:
https://youtube.com/watch?v=WJRyu1JUtVw",/r/MachineLearning/comments/qj0yhj/newsresearch_adop_approximate_differentiable/hin04ak/
1635009727.0,23-Oct-2021 10:22:07,dojoteef,MachineLearning,"For NLP, the intent of an utterance (the causal reasoning for the words being uttered) is important for understanding language. Thus, if you only have a stream of symbols (discrete tokens representing words), without a way to distinguish between what a human states vs what the model generates, then the language model is prone to produce “self-delusions” — biasing future generations on its own outputs, even when fallacious. This can also help explain the occurrence of repetition in language model generated text.",/r/MachineLearning/comments/qe0sl3/r_shaking_the_foundations_delusions_in_sequence/hhre2r0/
1634995359.0,23-Oct-2021 06:22:39,dojoteef,MachineLearning,"Thanks for sharing! Some really great insights in that technical report. While I’m aware of casual modeling, the provided perspective is certainly new to me and seems super useful in sequential modeling.",/r/MachineLearning/comments/qe0sl3/r_shaking_the_foundations_delusions_in_sequence/hhqnee1/
1633622641.0,07-Oct-2021 09:04:01,dojoteef,MachineLearning,"That workshop is non-archival:

>	IJCAI2021-WSRL is a non-archival venue and there will be no published proceedings.",/r/MachineLearning/comments/q3bggv/deleted_by_user/hfqqwly/
1633314194.0,03-Oct-2021 19:23:14,dojoteef,MachineLearning,Microsoft [employs the creator of Python and funds his ability to further develop improvements to the Python ecosystem](https://twitter.com/gvanrossum/status/1326932991566700549?s=20).,/r/MachineLearning/comments/q0utbe/deleted_by_user/hfaz4vu/
1633261378.0,03-Oct-2021 04:42:58,dojoteef,MachineLearning,"Check out [BirdCast](https://birdcast.info). Not sure if it’s exactly what your looking for, but they list [publications](https://birdcast.info/about/publications/) and have a section for [news](https://birdcast.info/news/) which sometimes describes new research (typically for a general audience).",/r/MachineLearning/comments/q0f6tg/d_favorite_blogs_about_a_technical_usecase_niche/hf7r27v/
1633260742.0,03-Oct-2021 04:32:22,dojoteef,MachineLearning,"[TABBIE: Pretrained Representations of Tabular Data](https://aclanthology.org/2021.naacl-main.270/) allows infilling row/column headers (see Figure 4). As this is research, there are no cloud services that make use of it (that I know of).",/r/MachineLearning/comments/q0euys/d_how_to_use_ml_to_detect_key_from_a_value/hf7q327/
1632840607.0,28-Sep-2021 07:50:07,dojoteef,MachineLearning,Is this game being used as a way to gather crowd sourced annotations? The process you describe seems quite reminiscent to crowd sourced dataset collection.,/r/MachineLearning/comments/px4p6x/telephone_pictionary_with_image_synthesis/helnq2k/
1632741240.0,27-Sep-2021 04:14:00,dojoteef,MachineLearning,"Asked and answered a lot. Just search this sub. Most recently:

https://reddit.com/r/MachineLearning/comments/l1z8cr/d_best_way_to_draw_neural_network_diagrams/",/r/MachineLearning/comments/pwe783/deleted_by_user/hegilyu/
1632589522.0,25-Sep-2021 10:05:22,dojoteef,MachineLearning,"1.	Conditional generative models in this sense are ones that model the joint distribution of `x`, e.g. conditional VAEs.
2.	They are referring to fact that auto-regressive models are “powerful” enough on their own to model the data without needing to condition on the context (or latent variables). This can lead to a phenomenon called posterior collapse in VAEs where the latent variables are ignored by an adequately strong auto-regressive decoder. A similar phenomenon can happen for conditional models.",/r/MachineLearning/comments/pvaix9/d_motivation_of_contrastive_predictive_coding/he8p03q/
1632536665.0,24-Sep-2021 19:24:25,dojoteef,MachineLearning,"Here’s a link to the blog post:
https://openai.com/blog/summarizing-books/

There is also a summary explorer:
https://openaipublic.blob.core.windows.net/recursive-book-summ/website/index.html",/r/MachineLearning/comments/puxq1v/r_recursively_summarizing_books_with_human/he670sv/
1632441131.0,23-Sep-2021 16:52:11,dojoteef,MachineLearning,Use source control and tag your experiments with the commit sha.,/r/MachineLearning/comments/pu7rxp/d_how_do_you_ensure_reproducibility/he113aw/
1632225395.0,21-Sep-2021 04:56:35,dojoteef,MachineLearning,"Comments like yours, which are designed to make you seem like you’re better than others, are in really poor taste. Sure, there are lots of beginners on this sub, but it’s also frequented by practitioners, researchers, etc. Please don’t disparage people for having a sense of imagination.

And I’m sure you realize that most technologies can have both malicious and beneficial uses. CV is already being used for nefarious purposes (e.g. US drones and surveillance of Uyghurs), yet we still improve the detection abilities of CV models.",/r/MachineLearning/comments/ps0d02/p_a_truck_with_the_text_jcn_clip_is_scarily_good/hdphhtx/
1632162357.0,20-Sep-2021 11:25:57,dojoteef,MachineLearning,"While this is creepy in a big brother way, I also see lots of useful possibilities. Imagine setting up IFTTT using natural language triggers for your home surveillance system, e.g. `if` “package delivered” `then` email me.",/r/MachineLearning/comments/ps0d02/p_a_truck_with_the_text_jcn_clip_is_scarily_good/hdmda3d/
1632139570.0,20-Sep-2021 05:06:10,dojoteef,MachineLearning,Reposts are not allowed,/r/MachineLearning/comments/prt0ru/p_r_i_want_to_introduce_the_c_dataframe/hdkxnw0/
1631199849.0,09-Sep-2021 08:04:09,dojoteef,MachineLearning,Please describe how this is related to ML. Looking over your website it’s not clear. This might not be the correct sub for this post.,/r/MachineLearning/comments/pkz6lk/p_realtime_streaming_data_gemo_gamedemo/hc6szzk/
1631191544.0,09-Sep-2021 05:45:44,dojoteef,MachineLearning,"Locking the thread since the comments are full of low effort jokes. And as [this comment](https://old.reddit.com/r/MachineLearning/comments/pkvt4n/p_opensource_dck_pic_detection_model_to_improve/hc65gom/) mentions, Bumble already includes such a feature.",/r/MachineLearning/comments/pkvt4n/p_opensource_dck_pic_detection_model_to_improve/hc6bisj/
1630848112.0,05-Sep-2021 06:21:52,dojoteef,MachineLearning,Please link to the paper.,/r/MachineLearning/comments/picsx1/deep_natural_language_processing_for_linkedin/hbomqpw/
1630777367.0,04-Sep-2021 10:42:47,dojoteef,MachineLearning,"Of course context matters — in this colloquial usage, “primates” means non-human primates. Please don’t attack a straw man. If indeed the goal of classification was biological taxonomy, then Facebook’s reaction would not be to disable the classification feature.",/r/MachineLearning/comments/phjecd/n_facebook_apologizes_after_ai_puts_primates/hblbn8t/
1630759294.0,04-Sep-2021 05:41:34,dojoteef,MachineLearning,"See section 3.3 from [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://arxiv.org/abs/2106.15195):

>	As pointed out by Figure 3, copying scores (mostly BLEU) from previous work was rarely done before 2015. In 2019 and 2020, nearly 40% of the papers reported on comparisons with scores from other papers. While many papers copied and compared metric scores across papers, it is often unclear whether they are actually comparable.",/r/MachineLearning/comments/phnx8c/d_do_you_reproduce_a_method_for_sota_comparison/hbk9ejq/
1630725183.0,03-Sep-2021 20:13:03,dojoteef,MachineLearning,Try [this article from USA Today](https://www.usatoday.com/story/tech/2021/09/03/facebook-video-black-men-primates-apology/5721948001/).,/r/MachineLearning/comments/phjecd/n_facebook_apologizes_after_ai_puts_primates/hbj00cy/
1630609523.0,02-Sep-2021 12:05:23,dojoteef,MachineLearning,"Go to 7:30 in the linked video:

https://www.reddit.com/r/MachineLearning/comments/7hys85/n_ali_rahimis_talk_at_nipsnips_2017_testoftime/

Btw, the whole talk is pretty great. I’d recommend watching it all if you have the time.",/r/MachineLearning/comments/pfrxpw/r_deep_reinforcement_learning_at_the_edge_of_the/hbco5ul/
1630496291.0,01-Sep-2021 04:38:11,dojoteef,MachineLearning,I for one welcome the return of the NeurIPS rigor police.,/r/MachineLearning/comments/pfrxpw/r_deep_reinforcement_learning_at_the_edge_of_the/hb6blox/
1630034662.0,26-Aug-2021 20:24:22,dojoteef,MachineLearning,Terrible advice from an account that is now deleted. Removed the comment as it clearly came from an anonymous troll.,/r/MachineLearning/comments/pc00wb/d_emnlp_findings_how_should_i_proceed/haifyuo/
1629333354.0,18-Aug-2021 17:35:54,dojoteef,MachineLearning,"The trend you mention is sadly far too common in AI/ML. The problem is that you cannot not generate scientific conclusions this way.

If the goal is only to say, this one time on this one dataset I was able to beat this other model, then sure this approach works. Just make a dataset with a leaderboard and overfit to your heart’s content.

For the goal of proposing new generalizable advancements, this approach does not work. It only causes the community to have to sift through lots of noise.

See [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://arxiv.org/abs/2106.15195) for criticisms of this approach in relation to machine translation. Similar issues exist for other related disciplines, including RL.",/r/MachineLearning/comments/p6zu7w/d_op_in_rreinforcementlearning_claims_that/h9h3a6u/
1628112792.0,04-Aug-2021 14:33:12,dojoteef,MachineLearning,"Despite user privacy being an important issue, this post is tenuously related to ML.",/r/MachineLearning/comments/oxyvnu/d_facebook_disables_accounts_tied_to_nyu_research/h7q85o2/
1627935462.0,02-Aug-2021 13:17:42,dojoteef,MachineLearning,Posts like these are better suited for r/learnmachinelearning.,/r/MachineLearning/comments/owlqwl/d_most_important_theorems_in_optimization/h7gwrdl/
1627161582.0,24-Jul-2021 14:19:42,dojoteef,MachineLearning,"See my recent comment that indicates it might still be possible to extract the data:

https://reddit.com/r/MachineLearning/comments/opzr3t/_/h68i1la/?context=1",/r/MachineLearning/comments/oqxqm0/d_publishing_weights_of_a_model_trained_on_a/h6endl2/
1627137563.0,24-Jul-2021 07:39:23,dojoteef,MachineLearning,"I bet if you finetuned it on a translation dataset, it could easily do translation. You can already do that with any decoder-only Transformer model. During training, if you format your input like:

`<source sentence> <sep> <target sentence>`

Then at inference time you start with a prefix that contains the source and separator, and the model predicts the target.

That said, if you are trying to use the unique aspects for TransformerXL, namely the history of hidden states from the previous chunk of the sequence, then you could treat the history as the encoded source sentence. There are two issues you’d have to contend with:

1.	There is no backpropagation from the current sequence into the history. This might decrease performance slightly.
2.	The current code implementations for TransformerXL assume a fixed length history. You’d likely need to write your own dynamic length history.

There are other minor issues you’d likely have to deal with as well if you want to adapt current code implementations.",/r/MachineLearning/comments/oqmc14/d_can_the_transformerxl_be_used_for_machine/h6daxip/
1627045975.0,23-Jul-2021 06:12:55,dojoteef,MachineLearning,"I hope it’s due to reviewer and AC filtering due to feedback provided by ACs and SACs respectively. Conferences grew dramatically in a few short years, during which it wasn’t possible to limit reviewers and ACs to only high quality folks (not enough senior peole, no data on newer reviewers). Maybe as the exponential growth period is beginning to level off, it’s now possible to institute more filtering. I don’t think it’ll ever be perfect, but maybe it’ll improve in time.",/r/MachineLearning/comments/opxkr2/discussion_rant_we_keep_hearing_about_bad_and/h68tr6m/
1627040541.0,23-Jul-2021 04:42:21,dojoteef,MachineLearning,"I’ve certainly had bad reviews before; I think it comes with the territory, especially given the size of conferences these days. That said, I’m reviewing for EMNLP this year and the reviews have been super high quality. I’m reviewing six papers and only one has a big spread in review scores: 1.5, 2.5, and 3.5. The rest are all within 0.5-1 point of each other.

Not sure if it’s luck this year, or if the conference actively tried to improve review quality by better reviewer selection criteria. As an author, the reviews we received were better than I was expecting as well.

Finally, the ACs have been engaged, and one was really trying to be empathetic to a paper that received a 2, 2.5, and 2.5. They clearly read the reviews and author response and were asking reasonable questions to better understand why the review scores were so low and if the problem was with the claims in the paper, the reported metrics, or something else.

Again, I can’t be certain if it’s just luck this year. It’s only my third year reviewing for EMNLP, but I also review for the other major *ACL conferences and this has by far been the best I’ve seen. Hopefully the trend continues (assuming it is actually a trend and not just some local maxima that I stumbled upon).",/r/MachineLearning/comments/opxkr2/discussion_rant_we_keep_hearing_about_bad_and/h68kpoy/
1627038612.0,23-Jul-2021 04:10:12,dojoteef,MachineLearning,"Direct access to the model is not needed to be able to reverse engineer the model. See [How to steal modern NLP systems with gibberish?](http://www.cleverhans.io/2020/04/06/stealing-bert.html)

That said if you are not concerned about reverse engineering, this question becomes a API security question and you will likely get better responses on a different sub, like r/Webdev.",/r/MachineLearning/comments/opzr3t/d_how_to_protect_ip_when_deploying_a_model_for/h68i1la/
1626524105.0,17-Jul-2021 05:15:05,dojoteef,MachineLearning,"We do have proposals, but since the paper is under review, I should hold off on details. There are quite a few papers you could read instead that discuss human evaluation in general (not AMT specifically), but some of their insights can also apply to AMT. Here are a couple that can lead you in the right direction:

[Best practices for the human evaluation of automatically generated text](https://aclanthology.org/W19-8643/)

[Twenty Years of Confusion in Human Evaluation: NLG Needs Evaluation Sheets and Standardised Definitions](https://aclanthology.org/2020.inlg-1.23/)",/r/MachineLearning/comments/olj1ab/r_baidus_knowledgeenhanced_ernie_30_pretraining/h5ibiye/
1626490898.0,16-Jul-2021 20:01:38,dojoteef,MachineLearning,"We have a paper under submission that shows these AMT qualifications alone are pretty meaningless. We also ran a pilot study (not reported in our paper) that asked turkers to self report values like:

* Age and year of birth
* Native language and English level
* Approval rate (it’s listed on their UI)

We grouped crowd workers into specific buckets based on approval rate (91-100%) to isolate differences. We found that even crowd workers with 95% or greater approval did not report age correctly (which should be at most no more than +/- 1 year off based on year of birth) or English proficiency (e.g. native language is not English, but stating native English proficiency).


Less than 75% of workers with an approval rate of 95% or 96% reported age correctly (given the definition above). Only 80% of workers with an approval rating of 97% answered these correctly.


A similar (but less pronounced) trend happened with English proficiency. It was more pronounced between 91-95% approval rate.


Additionally, this survey is *super* simple and should require less than a minute to complete. The average WorkTimeInSeconds reported by Mturk for these workers was over 10min! Basically, the crowd workers open a large number of HITs in parallel and complete them serially. Makes it look like they spent a considerable time on each task, despite the real time being much smaller, essentially making the time reported meaningless.


Issues with human evaluation (especially on AMT) have been noted in numerous papers, including this recent paper: [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://arxiv.org/abs/2107.00061)",/r/MachineLearning/comments/olj1ab/r_baidus_knowledgeenhanced_ernie_30_pretraining/h5h56wp/
1626181846.0,13-Jul-2021 06:10:46,dojoteef,MachineLearning,This post fits better on r/learnmachinelearning. Please post it there instead.,/r/MachineLearning/comments/oje1t1/d_looking_for_the_best_recommender_system_online/h51738g/
1626177830.0,13-Jul-2021 05:03:50,dojoteef,MachineLearning,"If you’re using pytorch, then consider trying [deepspeed](https://www.deepspeed.ai/) as an alternative to getting a larger GPU. It can do things like offload GPU memory to RAM or even a high speed SSD without too much impact to training speed (probably considerably less than downgrading to a K80).",/r/MachineLearning/comments/oj98br/d_does_it_make_sense_to_mix_a_rtx_3090_with_a/h510j31/
1624921078.0,28-Jun-2021 15:57:58,dojoteef,MachineLearning,"Please state the purpose of this survey in explicit terms, or this post will be removed.",/r/MachineLearning/comments/o9u42a/r_60_question_survey_about_machine_learning_for/h3dkump/
1624453745.0,23-Jun-2021 06:09:05,dojoteef,MachineLearning,"Our department has a similar policy, likely because our IT people are responsible for the machines in use in the department.

I only have experience with Lambda. We’ve had some quality control issues with one of our three lab machines, repeatedly having to reboot it because the GPUs would no longer be recognized. Lambda support worked with our IT folks and they received replacement parts (multiple GPUs and a power supply). Ultimately, I think the underlying issue is likely the motherboard (by process of elimination), but the machine is no longer under warranty, so we simply removed one GPU (1 of 4) from the one faulty machine and it runs fine now.

That said, our other two machines have worked reasonably well so far. The machines we have are: 2 servers w/4 2080Tis, and 1 machine w/2 3090s (we got all machines when the GPU was the latest model). We went for the 3090s so we could run larger language models (we also have access to a shared cluster with some RTX8000s, but it’s nice to have dedicated lab machines).

Hope that helps with decision making process!",/r/MachineLearning/comments/o6bzvd/d_purchasing_a_40k_gpu_server_for_a_new_lab/h2rihnk/
1622805977.0,04-Jun-2021 04:26:17,dojoteef,MachineLearning,"That’s actually what [computational neuroscience](https://en.m.wikipedia.org/wiki/Computational_neuroscience) does. They model neurons, both individually and in networks.",/r/MachineLearning/comments/ns2ne8/d_what_do_we_know_about_brain_neurons_and_why/h0jz9gp/
1622678812.0,02-Jun-2021 17:06:52,dojoteef,MachineLearning,"I think you are conflating a number of different issues without realizing it.

1.	Yes, you can simply use KLDivLoss with log softmax.
2.	The first “workaround” you highlighted is actually implementing a different concept (label smoothing), which requires additional code on top if the simple KLDivLoss to implement.
3.	The reason people use the CrossEntropyLoss is that it is more memory efficient when you have one-hot targets (especially when you have a large number of classes like in NLP vocabularies).
4.	Some of the other examples you highlight are well before the library fully stabilized (from v0.3.0 in 2017).",/r/MachineLearning/comments/nqt9ip/deleted_by_user/h0dnpzh/
1621773093.0,23-May-2021 05:31:33,dojoteef,MachineLearning,"You want r/learnmachinelearning. Additionally, read the Wikipedia article on the [Poisson distribution](https://en.m.wikipedia.org/wiki/Poisson_distribution).

>	…is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.",/r/MachineLearning/comments/nj6giu/discussion_how_is_poisson_distribution_applied_to/gz5kypx/
1621717140.0,22-May-2021 13:59:00,dojoteef,MachineLearning,Removing as this is an old repost.,/r/MachineLearning/comments/niruw0/d_gpt3_has_been_posting_on_reddit_for_some_time/gz3e89h/
1621674508.0,22-May-2021 02:08:28,dojoteef,MachineLearning,Please post questions like this to r/learnmachinelearning  instead.,/r/MachineLearning/comments/nicm5y/d_is_there_any_relationship_between_this_graph/gz1etos/
1621600463.0,21-May-2021 05:34:23,dojoteef,MachineLearning,Looks like you fixed them. Works for me now.,/r/MachineLearning/comments/nhnjr3/r_a_review_of_neural_anisotropy_directions_2020/gyxq46w/
1621599959.0,21-May-2021 05:25:59,dojoteef,MachineLearning,Looks like one of the authors of the paper [thinks this is an outstanding review of their work](https://twitter.com/gortizji/status/1395656619481079808?s=20). Might be worth taking a look.,/r/MachineLearning/comments/nhnjr3/r_a_review_of_neural_anisotropy_directions_2020/gyxp9be/
1621599845.0,21-May-2021 05:24:05,dojoteef,MachineLearning,Your links don’t work due to the backslashes.,/r/MachineLearning/comments/nhnjr3/r_a_review_of_neural_anisotropy_directions_2020/gyxp2ik/
1621572170.0,20-May-2021 21:42:50,dojoteef,MachineLearning,I look forward to chatting with you during your poster session.,/r/MachineLearning/comments/nhbvpa/p_a_model_theoretic_view_of_storytelling_and_what/gywrhuu/
1621519820.0,20-May-2021 07:10:20,dojoteef,MachineLearning,You might want to check out [InfinityGAN](https://hubert0527.github.io/infinityGAN/) whenever the code gets released.,/r/MachineLearning/comments/nh06ym/highest_resolution_gan_available_d/gytomoo/
1621429019.0,19-May-2021 05:56:59,dojoteef,MachineLearning,"When training you need to account for the size of the inputs, the intermediate computations (n^2 in size for self-attention), the gradients, and any additional memory that the optimizer might need (e.g. when using Adam).",/r/MachineLearning/comments/ng4nbw/d_how_to_predict_nlp_transformer_model_sizes/gyow192/
1620962310.0,13-May-2021 20:18:30,dojoteef,MachineLearning,"Looks like you beat me to it! I was holding off on posting this so I could post the video as a link post this weekend. Read about it on gaming news sites first. The temporal stability is certainly impressive, combined with the minimal hallucinations (apparently due to the patch-based discriminator).",/r/MachineLearning/comments/nbyrcj/r_enhancing_photorealism_enhancement/gy24fir/
1620936109.0,13-May-2021 13:01:49,dojoteef,MachineLearning,This is a well-known issue often referred to as hallucination. See [Go Figure! A Meta Evaluation of Factuality in Summarization](https://arxiv.org/abs/2010.12834) for a fairly recent discussion of the topic.,/r/MachineLearning/comments/nbm3b6/d_hugging_face_summarization_adding_wrong/gy0o693/
1620925283.0,13-May-2021 10:01:23,dojoteef,MachineLearning,"There are many reasons for this including:

1.	High quality human annotated datasets for these models often don’t exist (it’s really an issue across ML, but is especially worrisome for sensitive research areas that may reinforce bias)
2.	Where data exists, it may already contain historical biases (e.g. sentencing/recidivism)
3.	The over reliance on crowd sourcing, which is often poorly controlled, and thus introduces issues in both annotated training data (e.g. [How Much Reading Does Reading Comprehension Require](https://aclweb.org/anthology/D18-1546)\) and evaluation (https://ehudreiter.com/2021/05/07/high-quality-human-evaluation)",/r/MachineLearning/comments/nbisqb/d_why_is_the_baseline_for_fairness_and_bias_zero/gxzxteg/
1620830751.0,12-May-2021 07:45:51,dojoteef,MachineLearning,Why not simply train the model right-to-left instead of left-to-right?,/r/MachineLearning/comments/naq75f/d_end_priming_transformers_techniques/gxuxc3d/
1620561027.0,09-May-2021 04:50:27,dojoteef,MachineLearning,Removing considering this is not ML related.,/r/MachineLearning/comments/n82laq/d_casio_ai1000_pocket_lisp_computer_from_1989/gxhluun/
1620479592.0,08-May-2021 06:13:12,dojoteef,MachineLearning,"Please describe what ML techniques you used and code/libraries used.

Edit: removing the post for now until more information is provided.",/r/MachineLearning/comments/n7lv1o/oc_phand_gesture_recognition_play_pause_control/gxducxh/
1620477530.0,08-May-2021 05:38:50,dojoteef,MachineLearning,"Considering the tremendous strain India is currently under due to covid, having the government’s premier research organization introduce a technology that can help rural communities seems like a win, even if you did something similar and posted it on GitHub.

These are extraordinary times. Governments across the globe, including the US where I live, are in favor of suspending patents for the covid vaccine. That’s a lot more impactful to the bottom line of companies that spent considerable time and resources developing a viable vaccine.

Recognition in general is often a combination of luck and marketing. It’s gonna be hard to beat the reach of a government organization as a private citizen.",/r/MachineLearning/comments/n7lqde/d_the_future_of_opensource_ai/gxdr7j0/
1620322905.0,06-May-2021 10:41:45,dojoteef,MachineLearning,"Both. See Table 2. There is the straightforward experiment of simply applying the neural probabilistic language model (NPLM) approach from Bengio et al. 2003 (but with modern changes like the use of ReLU). This performs reasonably well, but still lags behind Transformers. Then they replace the first layer of a Transformer with one from NPLM and demonstrate improved performance over the Transformer in word-level language modeling.

NPLM lags considerably for character-level datasets. My guess is it’s due to the long context length. They show it doesn’t make use of long context well (possibly because it needs to be much deeper to allow adequate mixing across long distances).",/r/MachineLearning/comments/n62qhn/r_do_you_even_need_attention_a_stack_of/gx6b39q/
1620319905.0,06-May-2021 09:51:45,dojoteef,MachineLearning,"I wasn’t trying to imply the approaches are identical. Rather, I was answering whether there is reason to believe replacing attention layers with with feed forward layers could behave similarly for NLP. Specifically, the LM paper has a fully connected layer that takes as input the concatenation of multiple tokens, which is similar to the per-patch MLPs in the image domain (it’s conceptually concatenating multiple pixels).",/r/MachineLearning/comments/n62qhn/r_do_you_even_need_attention_a_stack_of/gx63usf/
1620310381.0,06-May-2021 07:13:01,dojoteef,MachineLearning,"Yes! In fact someone from my lab has an upcoming paper in NAACL 2021 she colloquially refers to as *stupidLM* that shows competitive performance without attention, using an approach that incorporates a mixing feed forward layer over multiple tokens.

See: [Revisiting Simple Neural Probabilistic Language Models](https://arxiv.org/abs/2104.03474)

It’s a modern update of the 2003 Bengio et al paper [A neural probabilistic language model](https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).",/r/MachineLearning/comments/n62qhn/r_do_you_even_need_attention_a_stack_of/gx5his5/
1620060172.0,03-May-2021 09:42:52,dojoteef,MachineLearning,We don’t need yet another thread arguing about Tensorflow and Pytorch. Locking the thread.,/r/MachineLearning/comments/n3y7er/d_half_of_my_team_knows_tensorflow_the_other_half/gwsuk5n/
1619794568.0,30-Apr-2021 07:56:08,dojoteef,MachineLearning,"Slightly off topic, but if you had published the preprint on April 1st you could have applied UnO to estimate off-policy returns for the card game [Uno](https://en.m.wikipedia.org/wiki/Uno_(card_game\)) which would have been pretty meta!

As for the actual research, does the tightness of the bounds change when modeling the cdf vs the mean for example?",/r/MachineLearning/comments/n1rruc/r_universal_offpolicy_evaluation/gwfk755/
1619565646.0,27-Apr-2021 16:20:46,dojoteef,MachineLearning,"Some people report values in the range [0-1] others report [0-100]. If you’re looking for more detail, [the paper that introduces ROUGE](https://www.aclweb.org/anthology/W04-1013) is a good resource for understanding it since there are a bunch of different variations, like ROUGE-L and ROUGE-W as well.",/r/MachineLearning/comments/mzwh2r/d_question_on_rouge_scores_for_evaluating/gw3zo4i/
1619234135.0,23-Apr-2021 20:15:35,dojoteef,MachineLearning,"This demo of an AI story writing assistant from Google researchers was presented during the [HCI+NLP workshop at EACL 2021](https://sites.google.com/view/hciandnlp/home). It combines a dialog model with few shot learning to give authors the ability to request story continuations, elaborate on story details and more. It’s still an early prototype and there are some limitations, but hopefully it can be improved in time. See [the paper](https://www.seas.upenn.edu/~daphnei/EACL_wordcraft.pdf) for more details.

I’m excited to see where research like this goes next. Though I’m biased considering I worked on a (less sophisticated) story writing assistant for [Storium](https://storium.cs.umass.edu).",/r/MachineLearning/comments/mxbjji/r_wordcraft_a_humanai_collaborative_editor_for/gvn69yj/
1619125518.0,22-Apr-2021 14:05:18,dojoteef,MachineLearning,"I also put a large emphasis on code quality and reusability, though I definitely feel in the minority considering most of the research (and even industry) code I see.",/r/MachineLearning/comments/mwbs7t/d_in_a_scale_of_1_to_10_how_much_importance_or/gvhos63/
1618968610.0,20-Apr-2021 18:30:10,dojoteef,MachineLearning,"The mods have actually been discussing this recently, so your post is timely. We were potentially considering making a pinned poll to solicit feedback on options moving forward. Maybe this thread can help brainstorm ideas. We’re happy to hear feedback on how to balance self-promotion and the use of link aggregators/newsletters.

Please note that we do bring the issue up through modmail with those who frequently post such self-promotional content, which unfortunately sometimes rankles folks.",/r/MachineLearning/comments/mv452o/d_new_tag_for_self_promotion_content/gv9wuwv/
1618755001.0,18-Apr-2021 07:10:01,dojoteef,MachineLearning,"In the future make sure to link to the repo in a comment. Here’s a link to the code:

https://github.com/elixir-nx/livebook",/r/MachineLearning/comments/mta1fb/p_livebook_jupyterstyle_environment_for_machine/guyxcnz/
1618698080.0,17-Apr-2021 15:21:20,dojoteef,MachineLearning,"Not a GAN, but I posted about [this paper w/demo](https://reddit.com/r/MachineLearning/comments/jprrah/r_text_style_transfer_wo_parallel_data/) a few months back.",/r/MachineLearning/comments/mszqrv/p_gan_for_text_generation/guwp283/
1618401822.0,14-Apr-2021 05:03:42,dojoteef,MachineLearning,"Dota and starcraft bots use an API, rather than raw pixels. That’s why they need noise and delays. If you use raw pixels and keyboard/mouse for input, I see no reason to add any delay/noise. Rather, to make an “easier” RL agent you could modify the reward (for example give a reward for dying n number of times in a match).",/r/MachineLearning/comments/mqd1ho/rp_counterstrike_from_pixels_with_behavioural/guh849z/
1618063309.0,10-Apr-2021 07:01:49,dojoteef,MachineLearning,None of the links (except your youtube video) work. Removing the post for now.,/r/MachineLearning/comments/mo43jc/n_from_amputee_to_cyborg_with_this_aipowered_hand/gu1kp4y/
1618006141.0,09-Apr-2021 15:09:01,dojoteef,MachineLearning,"Yeah, our work focused on super resolution rather than extending an image. Your results definitely look nice!",/r/MachineLearning/comments/mnoka6/r_infinitygan_towards_infiniteresolution_image/gtzbscl/
1618005679.0,09-Apr-2021 15:01:19,dojoteef,MachineLearning,"I was more comfortable with VAEs, especially since GANs tended to have training stability issues (or at least I had that perception in 2017).",/r/MachineLearning/comments/mnoka6/r_infinitygan_towards_infiniteresolution_image/gtzavaa/
1618002380.0,09-Apr-2021 14:06:20,dojoteef,MachineLearning,"Interesting work! Seems similar to my paper [Spatial PixelCNN: Generating Images from Patches](https://arxiv.org/abs/1712.00714). We also use a global latent code, combined with coordinates trained on image patches to generate images of arbitrary size. We did it with VAEs instead of GANs (though we did consider GANs).

Sadly, we couldn’t get the paper published since we only demonstrated the work on MINST due to my limited compute in 2017 running on a home server. Oh well...

EDIT: Hijacking my comment to remind people to be mindful reviewers. I submitted the  above paper to a CVPR workshop in 2018 and here’s an excerpt from one of the reviews:

>	Paper Strengths. Please discuss the positive aspects of the paper. Be sure to comment on the paper's novelty, technical correctness, clarity and experimental evaluation. Notice that different papers may need different levels of evaluation: a theoretical paper may need no experiments, while a paper presenting a new approach to a known problem may require thorough comparisons to existing methods. Also, please make sure to justify your comments in great detail. For example, if you think the paper is novel, not only say so, but also explain in detail why you think this is the case. [questions adapted from CVPR'17 review form]

Answer:

>	None.

The other reviews were not much better, though this particular review is indelibly imprinted in my memory.",/r/MachineLearning/comments/mnoka6/r_infinitygan_towards_infiniteresolution_image/gtz476n/
1617759712.0,06-Apr-2021 18:41:52,dojoteef,MachineLearning,Civil discussion only. That means state your point of view without invective. No bashing people or groups just because you disagree.,/r/MachineLearning/comments/mloj16/d_samy_bengio_resigns_from_google/gtn2nir/
1617705695.0,06-Apr-2021 03:41:35,dojoteef,MachineLearning,Article link: https://theworldyn.medium.com/podcast-speech-synthesis-using-tacotron2-7017a6508a68,/r/MachineLearning/comments/ml7ted/project_how_i_trained_spotify_podcast_speech/gtjzu40/
1617122758.0,30-Mar-2021 09:45:58,dojoteef,MachineLearning,">	- Don't work too hard

I can’t agree more! It’s important to take care of your mental and physical health.

In order to get my first paper out, I worked 100 hours a week for three weeks in a row. I was so utterly burned out I seriously got up and left the lab the day before the paper deadline and said nothing is worth this level of stress. Luckily I had an understanding wife who basically took care of everything else in my life so I could focus solely on getting the paper out the door. In hindsight, I know I should have just waited until the next conference cycle, but live and learn. Don’t make the same mistake.

*BTW, my previous career was in video game development for nearly a decade, so I’m familiar with crunch, and even then I’d never worked this hard.",/r/MachineLearning/comments/mgh6bh/d_machine_learning_phd_survival_guide_2021_video/gstg6pc/
1616948949.0,28-Mar-2021 09:29:09,dojoteef,MachineLearning,"My second point wasn’t about the size of p_ptr, but rather conceptually: there are `|V|+1` possible outputs from the model at time `t`. The additional element is due to the gate `g`.

I pointed out figure 3 from Abigail See’s paper because it specifically shows what you are asking for (as does Figure 1 from smerity’s paper, but it might not be as clear). At the top of the figure is a “final distribution” over an “extended vocabulary” that is implicitly defined by combining the vocabulary V with the gating mechanism: `g` in smerity’s paper and `p_gen` in See’s paper. At sampling time, when `g` indicates the word isn’t in `V`, then you sample from the attention over the local context rather than the vocab. That’s how you sample OOV words (they aren’t in the vocab `V`). You’ll notice that the final distribution has an entry for the token “2-0” which isn’t in the the vocab, but is in the local context.

I hope that clears things up. I feel like it isn’t easy to convey math/algorithm specifics on reddit, so if I still haven’t adequately answered your question then I’m not sure I’ll be able to. Sorry.",/r/MachineLearning/comments/mexhnh/d_pointer_sentinel_mixture_model_why_is_p_ptr_is/gslc5ii/
1616944713.0,28-Mar-2021 08:18:33,dojoteef,MachineLearning,"Sorry, your initial question wasn’t clear, but hopefully I understand your question now.

* p_ptr adds probability to vocab entries previously “seen” (via attention) in the context
* There are |V|+1 possible outputs (vocab and sentinel)

Additionally, Figure 3 and Section 2.2 from [Get To The Point: Summarization with Pointer-Generator Networks](https://www.aclweb.org/anthology/P17-1099/) might help your understanding as well.",/r/MachineLearning/comments/mexhnh/d_pointer_sentinel_mixture_model_why_is_p_ptr_is/gsl376e/
1616934973.0,28-Mar-2021 05:36:13,dojoteef,MachineLearning,"From the paper intro:

>	Pointer networks (Vinyals et al., 2015) provide one potential solution for rare and out of vocabulary (OoV) words as a pointer network uses attention to select an element from the input as output.",/r/MachineLearning/comments/mexhnh/d_pointer_sentinel_mixture_model_why_is_p_ptr_is/gskrlpb/
1616771087.0,26-Mar-2021 08:04:47,dojoteef,MachineLearning,"Both you and u/proof_required need to cool it a bit. It’s okay to disagree, but not using long acrimonious exchanges. If this thread between you two devolves further you will receive a temp ban.",/r/MachineLearning/comments/mdldtt/d_how_facebook_got_addicted_to_spreading/gsarh6y/
1616700425.0,25-Mar-2021 12:27:05,dojoteef,MachineLearning,You might be interested in this [recent post](https://reddit.com/r/MachineLearning/comments/md5ds0/r_replacing_rewards_with_examples_examplebased/).,/r/MachineLearning/comments/m2o75d/d_predicting_the_middle_of_a_time_series_to_make/gs7hnzt/
1616592623.0,24-Mar-2021 06:30:23,dojoteef,MachineLearning,"My approach is:

1.	Ensure I can replicate the numbers reported in their paper.
2.	Standardize on total # of parameters and compute budget (e.g. same number of optimization steps or total FLOPS).

For most research, the hope is that models generalize to other similar datasets and are not exceedingly sensitive to hyperparameters. That’s the case you should make with the fair comparisons.

If you have the time and compute budget, you could try all combinations of reported hyperparameters on every model, though I wouldn’t consider that a necessary requirement.",/r/MachineLearning/comments/mc524d/fair_model_comparison_r/gs1nxoq/
1616590605.0,24-Mar-2021 05:56:45,dojoteef,MachineLearning,"Check out the work of [Sarah Adel Bargal](http://cs-people.bu.edu/sbargal/). Specifically, you might be interested in this paper: [Explainable Deep Classification Models for Domain Generalization](https://arxiv.org/abs/2003.06498). It uses saliency maps of attention to explain what regions of the image most lead to a particular classification. It additionally discusses a method for better ensuring the model doesn't use spurious visual clues to make predictions (see Figure 1).",/r/MachineLearning/comments/mc1u8b/d_how_does_one_test_for_shortcut_learning_of_deep/gs1k13m/
1616464609.0,22-Mar-2021 18:56:49,dojoteef,MachineLearning,ACL conferences are also [moving toward OpenReview](https://www.aclweb.org/portal/content/association-computational-linguistics-aclrollingreview-system) with the same stipulations (that reviews remain private). A pilot is being prepared for EMNLP 2021.,/r/MachineLearning/comments/mb3nib/n_neurips2021_will_be_using_openreviewnet_to/grvs7l4/
1616344392.0,21-Mar-2021 09:33:12,dojoteef,MachineLearning,"Please provide a more substantial description of the approach, otherwise this post will be taken down.",/r/MachineLearning/comments/m9zmq2/p_face_make_up_powerd_by_deep_learning_change/grpqx75/
1616343920.0,21-Mar-2021 09:25:20,dojoteef,MachineLearning,Please describe what ML techniques you used.,/r/MachineLearning/comments/m9zmq2/p_face_make_up_powerd_by_deep_learning_change/grppxi0/
1615835189.0,15-Mar-2021 12:06:29,dojoteef,MachineLearning,"Sequence to sequence models have a source sequence and a target sequence. They try to predict the target sequence conditioned on the source sequence. If you remove the source sequence, then BART devolves into an LM. Just like the original Transformer paper applies it for machine translation in a sequence to sequence setup, and subsequent papers like GPT remove the source encoder and are left with an LM.",/r/MachineLearning/comments/m5nxc6/d_bart_as_a_language_model/gr1lbi6/
1615828765.0,15-Mar-2021 10:19:25,dojoteef,MachineLearning,"BART is a sequence to sequence model. If you remove the input sequence, it is essentially just an LM.",/r/MachineLearning/comments/m5nxc6/d_bart_as_a_language_model/gr16ub6/
1615466604.0,11-Mar-2021 04:43:24,dojoteef,MachineLearning,"Your idea sounds a lot like using Transformers with a masked infilling loss. This has been studied a bunch in NLP, with architectures like BERT, but relatively less so in RL. See this example from RL: [Masked Contrastive Representation Learning for Reinforcement Learning](https://arxiv.org/abs/2010.07470).

Note that similar to BERT, the idea you are putting forth breaks the autoregressive property typically assumed in sequence modeling since it uses a bidirectional encoder. In order to allow planning without the future context, you can only use a single directional encoder, thus losing some of the benefits of the architecture since it cannot look ahead to predict the present anymore.",/r/MachineLearning/comments/m2o75d/d_predicting_the_middle_of_a_time_series_to_make/gqkfaio/
1615464706.0,11-Mar-2021 04:11:46,dojoteef,MachineLearning,"Have you considered the Quadro RTX 5000? Here are comparisons between [Quadro RTX 5000 vs P100](https://askgeek.io/en/gpus/vs/NVIDIA_Quadro-RTX-5000-vs-NVIDIA_Tesla-P100-PCIe-16-GB) and [Telsa T4 vs Quadro RTX 5000](https://askgeek.io/en/gpus/vs/NVIDIA_Tesla-T4-vs-NVIDIA_Quadro-RTX-5000). It’s also not “gaming grade”, if that helps sell it to your boss.",/r/MachineLearning/comments/m2lyob/p_buying_new_server_5xt4_or_5xp100/gqkcog6/
1615428671.0,10-Mar-2021 18:11:11,dojoteef,MachineLearning,"Tbh, I think this is because it’s difficult to compare the various long context Transformer models. See section 4.1 of [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732):

>	While the field is bustling with new Transformer models, there is hardly an easy way to compare these models side by side. Many research papers select their own benchmarks to showcase the abilities of the proposed model. This is also coupled with different hyperpa- rameter settings like model sizes and configurations which can make it difficult to correctly attribute the reason for the performance gains.",/r/MachineLearning/comments/m2diub/d_where_are_longcontext_transformers/gqit2dr/
1615428311.0,10-Mar-2021 18:05:11,dojoteef,MachineLearning,"Haven’t had a chance to read through the paper, but the abstract and Figure 1 seem pretty surprising.",/r/MachineLearning/comments/m2d69q/r_pretrained_transformers_as_universal/gqisdkn/
1615302830.0,09-Mar-2021 07:13:50,dojoteef,MachineLearning,"Alright, I’m out of ideas for now as to how the paper’s results can be valid. Though, without being about to review the paper, I cannot be certain in my assessment. If you’re willing to share a link to the paper, then you can likely get a better judgment of validity.",/r/MachineLearning/comments/m1514c/deleted_by_user/gqc2iap/
1615301748.0,09-Mar-2021 06:55:48,dojoteef,MachineLearning,And how many images are in x_train?,/r/MachineLearning/comments/m1514c/deleted_by_user/gqbzwfw/
1615299722.0,09-Mar-2021 06:22:02,dojoteef,MachineLearning,"Could it be a terminology issue? Some researchers refer to the validation set as a test set and vice versa. Since you say this done on MNIST, one clue this occurred would be if they authors state they have fewer than 60k training images. The official MNIST train/test split is 60k train/10k test. In supervised settings researchers will occasionally split the train set into a validation set of 5-10k images and leave the remaining for training. Since the approach described is unsupervised, no held out validation set is needed.

Another possibility is the use of cross-validation as an alternate means of assessing the model, which again allows training on the full data, but in discrete folds, thus technically training on the test set.

This is why I say more detail is needed to know whether or not what the researchers have done is valid. Without seeing the paper, it doesn’t make sense to jump to a conclusion. Otherwise we will likely go back and forth like this with me possible explanations where it can make sense to “train on a test set”.",/r/MachineLearning/comments/m1514c/deleted_by_user/gqbvgys/
1615293540.0,09-Mar-2021 04:39:00,dojoteef,MachineLearning,"You need to provide more information about what the authors claim. Training on the test set is not inherently wrong; it all depends on the claims made in the paper. For example, in the [Deep Image Prior](https://en.m.wikipedia.org/wiki/Deep_Image_Prior) paper, you train on a single image (which in some sense is your test set) and assess how well the network denoises the image (amongst other tasks). Since there are other ways to assess the model, e.g. signal to noise ratio of the resultant image, having a test set is not strictly necessary for demonstrating an improvement.",/r/MachineLearning/comments/m1514c/deleted_by_user/gqbkmua/
1614950785.0,05-Mar-2021 05:26:25,dojoteef,MachineLearning,"We read it during our lab’s weekly reading group. The writing is surprisingly tame, though there were a few surprising anecdotes and statistics cited that were interesting. Our biggest critique was that the paper mostly read like an essay on the dangers of large NLP models, without describing possible mitigation strategies in depth.

I wish there were more substantive proposals on how to address the concerns listed. Seems crucial for a position paper to stand for something, rather than simply being against prevailing concerns that many (maybe not most) in the field worry about anyway. Then again, the audience for this paper (social scientists, lawyers, etc) probably aren’t aware of these concerns, so maybe it’s enough to merely highlight negative societal impact.",/r/MachineLearning/comments/ly64bp/r_on_the_dangers_of_stochastic_parrots_the_paper/gprqnti/
1614819416.0,03-Mar-2021 16:56:56,dojoteef,MachineLearning,"Great reference! Considering I’m a PhD student in NLP at UMass Amherst, I’m probably going to suggest our reading group discuss these two papers together.",/r/MachineLearning/comments/lwysts/n_google_study_shows_transformer_modifications/gplmhfv/
1614469327.0,27-Feb-2021 15:42:07,dojoteef,MachineLearning,"You could consider model extraction. It’s typically an issue service providers worry about, since someone with blackbox access to your model can potentially recreate it by querying it.

For your particular case, maybe consider [Exploring Connections Between Active Learning and Model Extraction](https://arxiv.org/abs/1811.02054) and subsequent work.",/r/MachineLearning/comments/ltyvpj/d_how_to_maintainpreserve_the_active_learning/gp3jsa9/
1614258806.0,25-Feb-2021 05:13:26,dojoteef,MachineLearning,"No school sets a hard cutoff of having 3+ first author pubs at top conferences for acceptance into the PhD program, let alone combined with co-organizing a conference workshop. There might be two or three students total who can pull this off, but not the hundreds that eventually make it in to the top schools every cycle.

Please don’t spread fud. Imposter syndrome is already prevalent enough, no need to add to people’s anxieties.",/r/MachineLearning/comments/lrqmd5/d_admissions_standards_at_top_programs/gop9d8b/
1613916736.0,21-Feb-2021 06:12:16,dojoteef,MachineLearning,"I released a [dataset of stories ](https://storium.cs.umass.edu)that are 19K tokens on average, but the longest are over a million. Our human evaluations show that relevance is the biggest factor in whether authors decide to use model generated text in their story, making this a good platform for assessing long document understanding and generation.",/r/MachineLearning/comments/lot770/d_very_long_sequence_data_books_understanding/go873iy/
1613765554.0,19-Feb-2021 12:12:34,dojoteef,MachineLearning,"PhD applications can be stressful, but you got in to a good school. Attending a school in the top 10-15 in the world for your field will not lower your marketability or opportunities. Take a step back, breathe, and be happy at your good fortune.",/r/MachineLearning/comments/lnopcm/deleted_by_user/go1ld3x/
1613604730.0,17-Feb-2021 15:32:10,dojoteef,MachineLearning,"I’d recommend you look at the work from [Mark Reidl’s group](http://eilab.gatech.edu/publications). Some of his older publications are related to hand crafting story graphs and using planning based approaches to ensure narrative coherence. This is a technique that was prevalent for quite a while until the recent advances in NLP. You should be able to find other relevant work by looking at citations and references of those papers on [Semantic Scholar](https://semanticscholar.org).

More recent work tries to learn all of this purely from text. My [dataset](https://storium.cs.umass.edu) collected from [Storium](https://storium.com) includes a narrator and annotations, e.g. challenges, goals, etc that can help learn these traits directly from the dataset.

Hope those provide you a good starting point for your research. Good luck!",/r/MachineLearning/comments/lm6zi6/p_question_about_generating_stories/gntog4h/
1613309331.0,14-Feb-2021 05:28:51,dojoteef,MachineLearning,"[WMT 2020 @ EMNLP](https://www.aclweb.org/anthology/events/emnlp-2020/#2020-wmt-1) had a very low resource translation shared task. You could see what people tried. I imagine rules would work nicely to augment such systems, but it’s difficult to figure out exactly where the rules fit in, both during training and at inference time.",/r/MachineLearning/comments/ljnkl7/d_is_there_a_case_to_be_made_for_rulesbased_nlp/gnee2c0/
1613222102.0,13-Feb-2021 05:15:02,dojoteef,MachineLearning,"I don’t mean supported indefinitely. That’s why I list Ubuntu LTS as an example (they provide 5 years of support). I could also mention Python 2.7, which was supported for an additional 6 years after being officially sunset in 2014.

My basic premise is that it would be good to know upfront what type of support a project will have after being sunset. By support I mean critical bug fixes, packages compiled for the latest OS/platform, etc. If some company goes all in and spends a year or two writing code in Swift, they will need time to transition to a new paradigm. Without knowing how long they will be supported after Google decides to sunset the project, why would they take the risk and give it a try?",/r/MachineLearning/comments/lii6kh/d_swift_for_tensorflow_is_being_archived_and/gn7go53/
1613157797.0,12-Feb-2021 11:23:17,dojoteef,MachineLearning,"I guess there are two ways to think about the approach Google takes. One is if you value stability, it often hurts those who adopt a technology only to have it sunset. On the other hand, there are lots of learnings that happen from these sorts of projects that can be valuable for future projects. Without trying wacky ideas you often get stuck in the same rut.

Ultimately, I think as outsiders (and possibly even insiders have this issue) it would be useful if Google explicitly stated what projects are guaranteed to be supported long term, similar to Ubuntu LTS.",/r/MachineLearning/comments/lii6kh/d_swift_for_tensorflow_is_being_archived_and/gn3apeu/
1613062283.0,11-Feb-2021 08:51:23,dojoteef,MachineLearning,This type of question is best asked in r/LearnMachineLearning or a similar sub.,/r/MachineLearning/comments/lhodmh/d_older_person_switching_careers/gmyf0qo/
1612724045.0,07-Feb-2021 10:54:05,dojoteef,MachineLearning,"I’m mainly speaking about US institutions for PhD applicants. Note, many programs still require the GRE for MS. Also, programs might still list the GRE as a field in the application for PhD candidates, but as optional.

Top universities that I have verified no longer require the GRE include: Stanford, UW, UIUC, Cornell, Columbia, CMU, MIT, GaTech, etc

Schools that waived the GRE for covid (but may not waive it in the future): Berkeley, NYU, UT Austin",/r/MachineLearning/comments/lej57x/d_yet_another_rant_on_phd_applications/gmhn9no/
1612712209.0,07-Feb-2021 07:36:49,dojoteef,MachineLearning,"Please note that most major CS departments in the US have removed the GRE requirement for PhD applicants as it has been shown not to correlate well with completion of a PhD.

As for questioning the merit of the OP’s publication: please don’t do that. It is demoralizing for no good reason, especially when someone is clearly already feeling despair.",/r/MachineLearning/comments/lej57x/d_yet_another_rant_on_phd_applications/gmgcchz/
1612490080.0,04-Feb-2021 17:54:40,dojoteef,MachineLearning,Please keep any discussions civil.,/r/MachineLearning/comments/lcvz1f/pytorch_lightning_flash_appears_to_be_copying/gm2dj5r/
1611770629.0,27-Jan-2021 10:03:49,dojoteef,MachineLearning,"I’d also weigh in and say that punitive measures like trying to remove someone’s name is wrong. Instead, for the camera-ready version you can list each person’s contributions if you want to make the work explicit. Not crediting someone for their work, or similarly crediting someone who didn’t do any work sounds like a bad idea.",/r/MachineLearning/comments/l68uza/discussion_editing_a_papers_author_names_after_an/gkz93tx/
1611515799.0,24-Jan-2021 11:16:39,dojoteef,MachineLearning,"I’m glad the idea of human evaluation based leaderboards are being picked up by other researchers! I published a paper in [EMNLP 2020](https://www.aclweb.org/anthology/2020.emnlp-main.525/) about a similar evaluation platform for story generation. The main difference is that we have real authors evaluate the generated text, rather than crowd sourced workers. We demonstrate why this is superior for evaluating long documents (mainly because crowd workers cannot be expected to read a novella to evaluate the next few generated sentences for a long story, thus not doing a great job in evaluating overall relevance of the generated text to the story as a whole).

Edit: Our storytelling leaderboard link: https://storium.cs.umass.edu/dashboard

Will get GPT-3 up there soon 🤞🏽",/r/MachineLearning/comments/l44ofg/rallen_institute_launches_genie_a_leaderboard_for/gkmgfmk/
1611350309.0,22-Jan-2021 13:18:29,dojoteef,MachineLearning,Just in case you read this (will also have my PI send an email). I do research on interactive storytelling and would love to test GPT-3 out for it (we applied back in the summer). Please see our [dashboard](https://storium.cs.umass.edu) comparing GPT-2 models on storytelling with users of the collaborative storytelling game [Storium](https://storium.com).,/r/MachineLearning/comments/l2ij0c/daccess_to_gpt3/gk8omjg/
1611060330.0,19-Jan-2021 04:45:30,dojoteef,MachineLearning,"For language models (like GPT-3), the predominant sampling approaches (top-k, nucleus, greedy, etc) are indeed biased for text generation because they sample from a truncated distribution.",/r/MachineLearning/comments/kzyrhf/p_persistent_antimuslim_bias_in_large_language/gjtpd8s/
1610984035.0,18-Jan-2021 07:33:55,dojoteef,MachineLearning,"Generally speaking, this is why I always like to read through the code for any package I use. Oftentimes there are hidden assumptions that are not mentioned in the documentation. I’ve also discovered quite a few bugs this way too. The most insidious bugs are when you use another researcher’s released code assuming it is bug free. We all make mistakes, and PhD researchers rarely, if ever, have code reviews to help find and fix issues.",/r/MachineLearning/comments/kzv2r0/d_psa_buggy_caching_behaviour_in_huggingfaces_old/gjq72mz/
1610591530.0,13-Jan-2021 18:32:10,dojoteef,MachineLearning,Is this essentially the US response to China announcing a similar public invest in AI a few years ago?,/r/MachineLearning/comments/kww5nf/n_the_white_house_launches_the_national/gj6pmei/
1610484183.0,12-Jan-2021 12:43:03,dojoteef,MachineLearning,"I’m pretty sure most people (myself included) who are upset about the state of reproducibility in ML are mainly complaining about:

1.	papers leaving out critical details that make reproducibility untenable (e.g. I just finished reviewing two papers for NAACL that were severely underspecified)
2.	when code is provided: code that either severely underperforms the numbers stated in the paper, or code that simply cannot be made to run despite a researcher spending significant time and effort (which sadly happens more often than I would expect)

These are not issues of guaranteeing exactly identical results. Instead, research should address at least one of the two issues above (but ideally both). Considering peer review for papers doesn’t require reviewers to look at supplemental material (e.g. code), I would be ecstatic for researchers to properly address point #1. This requires reviewers (and publication venues) to insist on reproducibility for publication. Currently, reproduction checklists are being implemented in most major conferences, but as far as I know, nothing enforces people to actually follow through as of yet.",/r/MachineLearning/comments/kvv94p/d_do_you_yourself_write_100_reproducible_ml_code/gj1eukx/
1609865864.0,05-Jan-2021 08:57:44,dojoteef,MachineLearning,"Here’s a reasonably [recent paper](https://arxiv.org/abs/1812.02900) that has an [associated repo](https://github.com/sfujim/BCQ) you could try out.

The repo also includes code for their [updated paper](https://arxiv.org/abs/1910.01708) which extends their approach (for continuous action spaces) to discrete action spaces.",/r/MachineLearning/comments/kqyppz/how_to_train_a_dqn_with_human_experience_p/gi727az/
1609860068.0,05-Jan-2021 07:21:08,dojoteef,MachineLearning,"Glad you asked! I’m putting the finishing touches on a library that allows for this use case. It’s a configuration library for Pytorch that allows for specifying a model using json ([jsonnet](https://jsonnet.org/) actually). It automatically wraps all the default Pytorch modules, learning rate schedulers, and optimizers such that they can be serialized and deserialized with json. It also allows decorating new classes to as well. That means you can write out a full specification of your model that can be loaded along with the state dict without requiring pickle (thus preventing arbitrary code execution).

I’m hoping to have it ready to release this week (though that might be optimistic). I’ll try to remember to comment on this thread when I release the package.",/r/MachineLearning/comments/kqzuu7/d_how_can_someone_accept_potentially_unsafe/gi6py41/
1609598054.0,02-Jan-2021 06:34:14,dojoteef,MachineLearning,"As a third year PhD student who read parts of both Bishop and Murphy before taking any formal classes in ML (before starting my PhD), I agree with /u/leonoel. I found Murphy’s book much harder to grok. It really feels like a book that tries to survey the whole field, which is great for researchers, but not so good for beginners. Now that I feel comfortable using Murphy as a reference, I still would not recommend it as a primary source for learning ML.",/r/MachineLearning/comments/kod9ze/p_probabilistic_machine_learning_an_introduction/ghtuqjg/
1609421821.0,31-Dec-2020 05:37:01,dojoteef,MachineLearning,Super interesting! Looks like they didn’t experiment with keeping the residuals but adding positions at each layer (which is done in this work). He also doesn’t mention whether they included position information in the values as well as the key and query vectors (this paper only adds positional information into the key and query vectors).,/r/MachineLearning/comments/knawu8/r_shortformer_better_language_modeling_using/ghlrnp2/
1609388435.0,30-Dec-2020 20:20:35,dojoteef,MachineLearning,"Thanks for clarifying how you assign the positions!

Also, the best reported Transformer-XL model from their paper uses 1600 tokens in the cache. It’s mentioned in the second paragraph of section 4.1 and you can see it in their repo [here](https://github.com/kimiyoung/transformer-xl/blob/44781ed21dbaec88b280f74d9ae2877f52b492a5/pytorch/run_wt103_large.sh#L36).

BTW, my lab mates have seen a similar phenomena (Transformers do not seem to use tokens outside a local window of up to a few hundred tokens or so). Considering how poorly current models keep coherence over long documents I wonder what you think the next steps for improving language models are, especially considering your paper shows improved perplexity with shorter contexts.",/r/MachineLearning/comments/knawu8/r_shortformer_better_language_modeling_using/ghkq1a9/
1609378207.0,30-Dec-2020 17:30:07,dojoteef,MachineLearning,"Interesting work! I quickly read the paper, so I might have missed some details.

1.	With your PIA approach, are the sinusoidal position embeddings added before or after the linear projection of the keys and queries?
2.	How do you choose the ordinal position for a token in a batch? Is it an ever increasing position (treating each token as having a unique position from the full corpus), or something different?
3.	Transformer-XL supports the notion of increasing the sequence length of the cached representations at inference time (and actually uses much longer sequences at test time than during training). This improves perplexity during inference at the cost of speed. Does your PIA approach allow for something similar? Did you account for the sequence length at test time when comparing speed with the Transformer-XL model?

I might have more questions after I get another chance to read through the paper. Once again, interesting work!",/r/MachineLearning/comments/knawu8/r_shortformer_better_language_modeling_using/ghk6ylf/
1609342482.0,30-Dec-2020 07:34:42,dojoteef,MachineLearning,It sounds like you are asking for feedback on how to improve a reddit bot. What’s the intended use case?,/r/MachineLearning/comments/kn38ep/p_redditor_response_simulator/ghi8dqk/
1609335606.0,30-Dec-2020 05:40:06,dojoteef,MachineLearning,How does this compare to Semantic Scholar’s Research Feeds feature?,/r/MachineLearning/comments/kn1hdm/p_personal_paper_recommendation_newsletter/ghhxeiy/
1609182353.0,28-Dec-2020 11:05:53,dojoteef,MachineLearning,"If you’re interested in the latest advances for Transformer models, check out [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732). The author does a good job surveying the field (and even covers a couple of their own models).",/r/MachineLearning/comments/klr2ux/d_what_isare_the_best_survey_papers_youve_read_on/ghbb3z5/
1609117094.0,27-Dec-2020 16:58:14,dojoteef,MachineLearning,"If you’re willing to roll your own, you can see [an example](https://github.com/dojoteef/storium-backend/blob/5fbd1b65a61eef059e262df8b438f6ac7f481f91/src/figmentator/figment/scheduler.py#L178) from my latest research project that makes use of asyncio.",/r/MachineLearning/comments/klc8r4/d_deploying_ml_models_batching/gh8p228/
1609102322.0,27-Dec-2020 12:52:02,dojoteef,MachineLearning,"As the newest mod I can provide some perspective from both sides of the discussion. I started lurking back in 2016. I had no prior exposure to ML and just started teaching myself from the plethora of online resources. Back then the percentage of research discussion on r/ML was higher than it is now and I found excellent discussion that helped me grow as a hobbyist hoping to transition into the field. So I commiserate with the concern that research discussion seems to have diminished over time.

On the other hand, now that I am a mod it is difficult to draw the line on what should be removed. The problem is that there is so much content posted I can’t always do a thorough job of vetting content before making a decision, so I tend to be more permissive (even if that heuristic still fails at times considering the modmail we receive when content gets removed). Being less permissive risks losing out on legitimate content.

Ultimately, I think moderating comes down to either allowing for more open discussion, which requires individuals to sift through more content to find what interests them (filtering by post flair can help with that). Or it quickly devolves into personal biases that reflect our interests as mods, rather than what the community finds interesting.

And while I like the idea of considering novelty of posts, as a researcher who take part in peer review, it’s very clear that novelty is highly subjective even among experts.",/r/MachineLearning/comments/kkwnn9/d_research_focused_machine_learning_subreddit/gh7wcoj/
1608494021.0,20-Dec-2020 11:53:41,dojoteef,MachineLearning,"The problem is that in many European programs you need a masters degree before starting your PhD. Though I do see CS programs in Denmark that allow a combined MS/PhD, so it doesn’t seem like it’s a universal issue.",/r/MachineLearning/comments/k2pd9n/d_why_you_should_get_your_phd/ggiem5q/
1608286118.0,18-Dec-2020 02:08:38,dojoteef,MachineLearning,"This is from Section 2.3 (Improvements in Asymptotic Complexity):

>	Recently, Kitaev et al. [14] introduced Reformer. A method that groups positions based on their similarity using locality-sensitive hashing (LSH) and only computes the attention within groups. For groups of fixed size, the asymptotic complexity of Reformer becomes linear with respect to the sequence length. Note that Reformer constrains the queries and keys of self-attention to be equal. As a result, it cannot be applied to neural machine translation, image captioning or memory networks, or generally any application with heterogenous queries and keys. In addition, as it uses hash collisions to form groups it can only handle a small number of bits, thus significantly reducing the quality of the grouping. Instead, our method uses clustering to group the queries, resulting in significantly better groups compared to hash collisions.

They also compare to Reformer in Table 1.",/r/MachineLearning/comments/kfi25l/d_r_how_is_fast_transformers_with_clustered/gg8la1p/
1608267461.0,17-Dec-2020 20:57:41,dojoteef,MachineLearning,"I wonder why they didn’t compare against VQVAE2. Rather, they have a single qualitative comparison to the original VQVAE in the appendix.",/r/MachineLearning/comments/kfdz61/r_taming_transformers_for_highresolution_image/gg7zdwl/
1608129347.0,16-Dec-2020 06:35:47,dojoteef,MachineLearning,The paper lists the GMail autocorrect LM as an example currently in production. I know other large corporations have similar private data they train on. Support chat bots are an example. And corporations like Bloomberg and Salesforce have troves of private data that I’m sure they leverage for LM training (in Bloomberg’s case for summarizing actionable investment insights for example).,/r/MachineLearning/comments/ke01x4/r_extracting_training_data_from_large_language/gg17uuy/
1608128443.0,16-Dec-2020 06:20:43,dojoteef,MachineLearning,"I’m really interested in the ethical and legal consequences of these results. On the ethics side, it seems:

* Differential privacy doesn’t address the concerns for web data (as stated in Section 8)
* Manually curating the data is not feasible at the scale of large LMs. Whatever automatic approaches are used will have to be carefully analyzed for unintended leakage.
* Overall Section 8 really doesn’t highlight currently effective mitigation strategies, yet large LMs are becoming a backbone for many large tech-focused corporations.
* This result reinforces the recent questions surrounding corporate self-governance of AI ethics when it can effect the bottom line.

On the legal side:

* How does this impact training on copyrighted works? This is a thorny issue. For example, Japan has explicitly allowed it as long as the copyrighted work is not perceptible in the output. The results from this paper call large LMs into question, especially if we can extract training data verbatim (even if seen only once).
* What legal liability does this open smaller companies and academic researchers to when fine tuning large pretrained LMs (seemingly the predominant approach the past couple of years).",/r/MachineLearning/comments/ke01x4/r_extracting_training_data_from_large_language/gg16c48/
1607721466.0,11-Dec-2020 13:17:46,dojoteef,MachineLearning,"Lol, sure you can. 38 here and I’ve been using vim since undergrad... always learn new stuff. Keeps you fresh!",/r/MachineLearning/comments/kahxan/d_pytorch_tools_best_practices_styleguides/gfg12zq/
1607706209.0,11-Dec-2020 09:03:29,dojoteef,MachineLearning,"I’m not a boomer and I’m all about vim+tmux, though I’m starting to consider [kakoune](https://kakoune.org)+tmux.",/r/MachineLearning/comments/kahxan/d_pytorch_tools_best_practices_styleguides/gfepc68/
1607705661.0,11-Dec-2020 08:54:21,dojoteef,MachineLearning,"Just wondering about the [performance table](https://github.com/yifding/hetseq#performance-table) in the Github repo. You do train much larger batch sizes, so speed up training, but the training loss is much higher for the larger batch sizes. Did you tune the learning rate for the larger batch size?",/r/MachineLearning/comments/kb3qor/p_training_bert_at_a_university/gfeo0a3/
1607609084.0,10-Dec-2020 06:04:44,dojoteef,MachineLearning,"Maybe not unit tests per se, but check out the ACL 2020 best paper award winner: [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442/).",/r/MachineLearning/comments/kac51m/d_do_you_think_about_unit_testing_your_machine/gfa0j3q/
1607014725.0,03-Dec-2020 08:58:45,dojoteef,MachineLearning,"This post is very divisive; there are more comments than upvotes... While there is some valid discussion, many comments are devolving into attacks. We are locking the post for now.

There is a [follow-up post](https://reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/) that contains emails related to the firing. We will be monitoring that post as well to keep the discussion civil.",/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/gei2qty/
1606828834.0,01-Dec-2020 05:20:34,dojoteef,MachineLearning,"You might want to read this ACL 2020 paper: [Language (Technology) is Power: A Critical Survey of “Bias” in NLP](https://www.aclweb.org/anthology/2020.acl-main.485/). Here’s the first sentence of the abstract:

>	We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process.",/r/MachineLearning/comments/k4jl5s/d_is_there_any_work_that_defines_the_different/ge981ll/
1606540966.0,27-Nov-2020 21:22:46,dojoteef,MachineLearning,"I have enough autonomy to do the work that I find interesting. And considering I’m doing a PhD, no, you don’t have to be a genius. Just have perseverance.",/r/MachineLearning/comments/k28qgr/d_why_you_shouldnt_get_your_phd/gdu83l7/
1606511150.0,27-Nov-2020 13:05:50,dojoteef,MachineLearning,"I agree that there are numerous issues with getting a PhD, though it’s not necessarily bad for everyone. While I’m only in my third year, I feel I haven’t lost my creativity, nor am I pressured to conform.

Though, you do bring up some good points. I think people should be more critical in deciding which advisor they want to work with, as many issues you brought up stem from the advisor/student relationship. Considering it is often a 5+ year experience, it really behooves PhD applicants to fully assess what it will be like working for a particular advisor and department. For example, I remember a potential PhD student who came to visit our lab and was super eager to find out if the program was brutal, because they were drowning in their current masters program.

My advice: speak to current students, whether in person if you visit the school (probably not this year), or by email. It really makes a world of difference!",/r/MachineLearning/comments/k28qgr/d_why_you_shouldnt_get_your_phd/gdsvtol/
1606457460.0,26-Nov-2020 22:11:00,dojoteef,cscareerquestions,"Try looking at the list of resources [here](https://martiansideofthemoon.github.io/2018/05/29/grad-resources.html). It covers the gamut, including statement of purpose. Also check out the linked post from 12 accepted PhD students [on their experiences](https://blog.nelsonliu.me/2019/10/24/student-perspectives-on-applying-to-nlp-phd-programs/).",/r/cscareerquestions/comments/jly1gn/quitting_a_big_tech_job_for_a_phd/gdqpgh8/
1606224319.0,24-Nov-2020 05:25:19,dojoteef,MachineLearning,You’re looking for the term [grammatical error correction](http://nlpprogress.com/english/grammatical_error_correction.html). See also [here](https://paperswithcode.com/task/grammatical-error-correction).,/r/MachineLearning/comments/k00b5l/d_nlp_how_do_i_turn_a_sentence_with_bad_grammar/gdftro1/
1606143447.0,23-Nov-2020 06:57:27,dojoteef,MachineLearning,This was literally [posted two days ago](https://reddit.com/r/MachineLearning/comments/jy5w7q/r_this_ai_can_convert_your_photo_to_brushstroke/) by the author who created it...,/r/MachineLearning/comments/jzie8y/news_imagetopainting_translation_with_style/gdc2qjw/
1605977809.0,21-Nov-2020 08:56:49,dojoteef,MachineLearning,See [To Dissect an Octopus: Making Sense of the Form/Meaning Debate](https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html) and [Is it possible for language models to achieve  language understanding?](https://chrisgpotts.medium.com/is-it-possible-for-language-models-to-achieve-language-understanding-81df45082ee2).,/r/MachineLearning/comments/jyc7ku/d_is_it_proved_that_language_models_cannot/gd2rzqg/
1605916256.0,20-Nov-2020 15:50:56,dojoteef,MachineLearning,Removed as this was already posted a short while before: https://reddit.com/r/MachineLearning/comments/jxrg9m/d_state_of_deepfakes_2020/,/r/MachineLearning/comments/jxvbhi/d_the_state_of_deepfakes_in_2020/gcztw5i/
1605901461.0,20-Nov-2020 11:44:21,dojoteef,cscareerquestions,"Considering I’m not done with my PhD, I can’t speak with confidence on this point, but I certainly hope it’s worth it. My goal is to conduct research after completing my PhD, so if I’m lucky enough to get a position that lets me do so, then I think it will be worth it.",/r/cscareerquestions/comments/jly1gn/quitting_a_big_tech_job_for_a_phd/gcz134h/
1605888149.0,20-Nov-2020 08:02:29,dojoteef,gradadmissions,This is great! There will also be a [student-run webinar](https://www.cics.umass.edu/admissions/phd-applicant-support-program) by current CS PhD students from UMass Amherst on Monday November 23rd at 11am EST.,/r/gradadmissions/comments/jxoc7u/panel_for_admissions_in_cs_phds/gcy9cmk/
1605884580.0,20-Nov-2020 07:03:00,dojoteef,cscareerquestions,"I know it's a little late, but thought I could share my experience. I worked for a decade and moved up the ladder (was a lead engineer, then an engineering manager), before going back for a PhD. If money or advancement is your primary motivating factor, I would caution going for a PhD. If you are passionate about conducting research, then it can be fulfilling (though it will be a considerable burden on your time compared to working at a company).",/r/cscareerquestions/comments/jly1gn/quitting_a_big_tech_job_for_a_phd/gcy2fx2/
1605878372.0,20-Nov-2020 05:19:32,dojoteef,MachineLearning,Please link to the actual paper as well.,/r/MachineLearning/comments/jxncnt/d_evaluation_of_text_generation_a_survey/gcxs62o/
1605533282.0,16-Nov-2020 05:28:02,dojoteef,MachineLearning,">	BERT is designed to pre-train deep bidirectional representations from unlabeled text

This is an excerpt from the abstract of the BERT paper.

>	We demonstrate that language models begin to learn these tasks without any ex- plicit supervision when trained on a new dataset of millions of webpages called WebText.

This is from the GPT-2 abstract.

What part of these approaches implies supervision in your mind?",/r/MachineLearning/comments/jv5qka/r_unsupervised_pretraining_when_where_is_it_used/gchr2pg/
1605531612.0,16-Nov-2020 05:00:12,dojoteef,MachineLearning,"It’s used quite literally everywhere in NLP. BERT, GPT-2/3, etc. It feels like half the research papers released in NLP these days use a fine tuned version of a pretrained model.",/r/MachineLearning/comments/jv5qka/r_unsupervised_pretraining_when_where_is_it_used/gchotfe/
1605472020.0,15-Nov-2020 12:27:00,dojoteef,MachineLearning,"Seems very similar to [Quick, Draw!](https://quickdraw.withgoogle.com/). Did you use that as a basis for your project?",/r/MachineLearning/comments/jup6uy/p_ready_set_draw_a_game_where_ai_will_guess_your/gcf7jaj/
1605445153.0,15-Nov-2020 04:59:13,dojoteef,MachineLearning,"I wrote a fairly robust [model hosting framework](https://github.com/dojoteef/storium-backend/tree/master/src/figmentator/figment) for my [recent research](https://github.com/dojoteef/storium-backend). It is agnostic of the deep learning framework used and it also supports installing requirements like your framework. I built it using [FastAPI](https://fastapi.tiangolo.com) with [asyncio](https://docs.python.org/3/library/asyncio.html) for serving.

It’s currently entangled with my research code, but I’ve been contemplating breaking it out into a separate package that can be used by anyone.",/r/MachineLearning/comments/jufl4o/p_inferrd_the_heroku_for_machine_learning_with/gcdwnx3/
1605398169.0,14-Nov-2020 15:56:09,dojoteef,MachineLearning,"It all depends on the license the model developers released their code and models with. For example, I typically license my research code with the MIT or BSD 3-clause license, which allow commercialization with appropriate attribution.

When people release using a permissive license they have already agreed to allow others to profit from their research.",/r/MachineLearning/comments/ju8wi8/d_is_huggingface_inference_api_business_model/gcb0vrw/
1605397495.0,14-Nov-2020 15:44:55,dojoteef,MachineLearning,"I feel frustrated that even as a reviewer I have little ability to correct issues in the review process during the discussion period. I wonder if the ACs should step into the conversation (not only to prompt reviewers to discuss the paper, but to remind reviewers what are valid criteria for rejection).

For example, for one of the AAAI papers I reviewed, two reviewers stated that the paper should be rejected because the authors addressed something other than what the reviewers believed to be the central line of inquiry for a particular research area. Basically, they are saying “I wouldn’t have chosen to research this, therefore it should not be published.” Nevermind the fact that they all listed the paper as novel (I could find no similar paper whatsoever in a literature review). Nor the fact that the results being reported were interesting (though the reviewers rationalized that it must somehow be a mistake or fabrication).

I politely asked the other reviewers to reflect on why their objection made the paper unpublishable, but there isn’t more that I can do as a reviewer. It’s like watching a train wreck in slow motion and you are powerless to stop it. I can only hope that the AC reads my comments from the discussion and agrees that these are not valid reasons to reject. Oh well.",/r/MachineLearning/comments/ju6mbf/d_the_gradient_how_can_we_improve_peer_review_in/gcazb7e/
1605060481.0,10-Nov-2020 18:08:01,dojoteef,MachineLearning,"It’s crazy, but it happens. I’m reviewing for AAAI right now and there is a paper I gave an 8, but another reviewer gave it a 2. I’m speaking with that reviewer now during the discussion period, so hopefully we can figure things out.

The issue seems to hinge on whether we can trust the numbers reported in the paper.",/r/MachineLearning/comments/jrocd9/d_iclr_reviews_are_out/gbw7f6b/
1604850606.0,08-Nov-2020 07:50:06,dojoteef,MachineLearning,"If you want peace of mind, you can also email the CVPR program chairs to verify. From the [call for papers](http://cvpr2021.thecvf.com/node/33):

>	Authors are encouraged to contact the Program Chairs about clarifications on borderline cases.

The email address is at the bottom of the page.",/r/MachineLearning/comments/jq9z3e/d_iclr_dual_submission_with_cvpr/gbmcqp3/
1604804650.0,07-Nov-2020 19:04:10,dojoteef,MachineLearning,This paper is actually quite different from the CTRL model. In fact it uses several different specialized models to generate the different styles. Control codes alone are not capable of recreating the results from this approach.,/r/MachineLearning/comments/jprrah/r_text_style_transfer_wo_parallel_data/gbjka6y/
1604761504.0,07-Nov-2020 07:05:04,dojoteef,MachineLearning,"Link posts need to provide context by linking to a paper/code describing the machine learning techniques used.


EDIT: Post has been removed, but will be approved once a link is provided as per the sub rules.",/r/MachineLearning/comments/jps5uw/n_digital_domains_deformation_simulation_system/gbghsd3/
1603716024.0,26-Oct-2020 05:40:24,dojoteef,MachineLearning,"Also the-eye seems to have a proper [DMCA notice](https://the-eye.eu/dmca/) on their site, in addition to the video linked in the tweet thread. Make of that what you will.",/r/MachineLearning/comments/ji7y06/p_dataset_of_196640_books_in_plain_text_for/ga5t9n8/
1603715345.0,26-Oct-2020 05:29:05,dojoteef,MachineLearning,"EDIT: Looks like I jumped the gun and was off by a letter in my search, so got the wrong info. It’s actually [bibliotik](https://www.reddit.com/r/opendirectories/comments/f2teym/project_liberation_bibliotik_terabytes_of_ebooks/). Really not sure about legality for research use now (especially at a public university in the US). My original comment (which is incorrect) follows below.

Is this from bibliotek.dk? If so, then it might even be legal to use for research (IANAL). See their [FAQ](https://bibliotek.dk/eng/overlay/help/29):

>	What are you allowed to do with data from bibliotek.dk? Users of bibliotek.dk - both private and business users - are free to use data from bibliotek.dk (that is, business use only for internal use). Commercial use demands an agreement.",/r/MachineLearning/comments/ji7y06/p_dataset_of_196640_books_in_plain_text_for/ga5sccs/
1603581660.0,24-Oct-2020 16:21:00,dojoteef,MachineLearning,LOL. Turns out you linked to a video of my advisor giving a lecture for the NLP class at UMass Amherst.,/r/MachineLearning/comments/jhh6y3/d_knowledge_distillation_in_nlp/g9ywg5k/
1603496106.0,23-Oct-2020 16:35:06,dojoteef,MachineLearning,"I’m pretty disappointed that this paper was rushed out. It seems pretty clear the authors just wanted the publicity before the NAACL anonymity period deadline (which is today). That’s likely why the paper reports that their biggest model hasn’t even finished training yet.

I understand why people try to game the system; the review process can often seem broken, so people will take whatever advantage they can get. I just think we should figure out a way to change the publication incentives so that this type of behavior no longer feels necessary, especially since it disproportionately hurts individuals and institutions that are not as well established due to the lack of name brand awareness to pull off these stunts.

Regardless, good luck to the authors with their seemingly inevitable NAACL submission. I really hope I’m not assigned as a reviewer for their paper. 🤞🏽",/r/MachineLearning/comments/jgfd2d/r_mt5_a_massively_multilingual_pretrained/g9th5kb/
1603075439.0,18-Oct-2020 19:43:59,dojoteef,MachineLearning,"That certainly isn’t the case everywhere. I know plenty of individuals (myself included) who went back for a PhD after successful careers working in industry. In my case, I started grad school nearly a decade and a half after completing undergrad.

Admittedly, it is less common, but it definitely happens.",/r/MachineLearning/comments/jdds4k/deleted_by_user/g9acugv/
1603020772.0,18-Oct-2020 04:32:52,dojoteef,MachineLearning,"There are already a number of services that allow a user to hum to discover a song. It’s great that Google is getting in on the action, but it’s not exactly new. Maybe they will do a better job though? Who knows.

https://www.midomi.com/
https://www.musipedia.org/",/r/MachineLearning/comments/jddqo1/n_googles_next_step_in_music_recognition_a/g97jd9i/
1603020296.0,18-Oct-2020 04:24:56,dojoteef,MachineLearning,This topic has been discussed quite a lot on this sub; the two questions you in your post have definitely been asked and answered before. I’d suggest doing a search for the term “PhD” restricted to this sub and what has previously been said. Good luck!,/r/MachineLearning/comments/jdds4k/deleted_by_user/g97iju8/
1602945563.0,17-Oct-2020 07:39:23,dojoteef,MachineLearning,"The stat I found most interesting is that people who reviewed for NeurIPS 10 or more times were rated by area chairs as abjectly the worst reviewers (within the delineated groupings). First time NeurIPS reviewers were the best, especially reviewers who had previously reviewed for specialized conferences. The trend doesn’t bode well for the conference, and makes me think I should continue publishing in NLP-focused venues in order to have high quality reviewers assigned to my papers.",/r/MachineLearning/comments/jcpj71/d_what_we_learned_from_neurips_2020_reviewing/g93ud4i/
1602944770.0,17-Oct-2020 07:26:10,dojoteef,MachineLearning,"While I agree that it sucks to not receive adequate reasoning for rejection, according to their experiment, the false positive rate for rejections was 6%. That’s pretty good considering how stochastic the review process is in general.

My guess is that they won’t reinstate the desk reject because it took a lot of time from the area chairs, but did not result in as many early rejections as they hoped for.",/r/MachineLearning/comments/jcpj71/d_what_we_learned_from_neurips_2020_reviewing/g93swqz/
1602942263.0,17-Oct-2020 06:44:23,dojoteef,MachineLearning,"I guess I’m surprised just how well this works across a variety of faces, considering previous models using FFHQ seemed to whitewash generated faces. Do you have any insight into why this model performs better at preserving such details?

I’m specifically thinking about how this work contrasts with [PULSE](https://openaccess.thecvf.com/content_CVPR_2020/html/Menon_PULSE_Self-Supervised_Photo_Upsampling_via_Latent_Space_Exploration_of_Generative_CVPR_2020_paper.html), which had a lot of discussion surrounding that tendency, e.g. an upsampled Obama photo tended to look more caucasian. This model seems to avoid that issue, at least when looking at Figure 11 from the paper. Is it simply the fact that the model here is trained in a supervised setting, while PULSE merely explores the pretrained latent space of a model?",/r/MachineLearning/comments/jcuch4/p_creating_real_versions_of_pixar_characters/g93otaa/
1602940666.0,17-Oct-2020 06:17:46,dojoteef,MachineLearning,Like “Spider-Verse” versions of a movie.,/r/MachineLearning/comments/jcuch4/p_creating_real_versions_of_pixar_characters/g93m8ne/
1602819452.0,15-Oct-2020 20:37:32,dojoteef,MachineLearning,Posts must be related to machine learning. Please describe the machine learning algorithms or research used as part of this project.,/r/MachineLearning/comments/jbzvbf/p_ai_extension_to_fight_fake_news/g8ywrux/
1602600877.0,13-Oct-2020 07:54:37,dojoteef,MachineLearning,"BTW, here’s an interactive demo site for [Netron](https://lutzroeder.github.io/netron/) with an example pointing to [squeezenet](https://lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2Fonnx%2Ftutorials%2Fmaster%2Ftutorials%2Fassets%2Fsqueezenet.onnx) from the [onnx tutorials](https://github.com/onnx/tutorials/blob/master/tutorials/assets/squeezenet.onnx).",/r/MachineLearning/comments/jaeso9/d_why_did_none_of_you_tell_me_about_netron/g8p41lz/
1602522231.0,12-Oct-2020 10:03:51,dojoteef,MachineLearning,"I understand your concerns, so I will delve a little deeper into why the post is being kept.

While it’s true that some women will find the post off-putting, the same is likely true for people with other gender identities. Fortunately, as the post is currently written, individuals can engage with it at their own comfort level. That is: 1) it can be avoided altogether as the title is clear and the post is behind a NSFW tag, 2) people can click to view the post and read only the text and choose not to view the images, 3) people can fully engage with the post by viewing the post and images.

Before I became a mod, there was a penis GAN post that I found off-putting, merely based its title. I purposefully never viewed that post. Now that I am a mod, I felt obligated to read this post to determine whether it should be removed. I appreciate the fact that the images were links; I haven’t clicked to view the images. Thus I’ve decided my comfort level with the post and been able to adhere to that comfort level.

As for the concerns of women specifically, I am not a woman, but I did make an effort to understand a female perspective. In fact I spoke with my wife, who is a clinical psychologist, about her thoughts on the post, which helped inform my decision-making process. As soon as I mentioned the post, before fully describing its contents, her first thought was of the potential positive ethical implications (possible harm reduction). Yet she immediately jumped to a slew of concerns related to the unethical treatment of individuals in porn and questions of disproportionate objectification of women. Despite those issues, she didn’t think they warranted removal of the post on a research-focused sub.

Naturally, my wife does not represent all women, and I didn’t exclusively use her guidance as my reasoning for why the post should be kept. That said, considering my wife focuses her research on cultural factors related to body image, and generally is fairly socially conscious, I do consider her views seriously, especially in relation to social issues.

I hope that gives a little more reasoning for the post staying up. Of course, my reasoning might not help alleviate your concerns; I’m very sorry if that is the case and sincerely hope this post does not lead you to avoid this  sub in the future.",/r/MachineLearning/comments/j8njqq/p_generating_and_animating_porn_using_ai/g8lve1a/
1602519618.0,12-Oct-2020 09:20:18,dojoteef,MachineLearning,"Please don’t antagonize the commenter. Let’s not fuel a cycle of down voting someone who merely has a difference of opinion, especially someone indicating discomfort.",/r/MachineLearning/comments/j8njqq/p_generating_and_animating_porn_using_ai/g8lq776/
1602426057.0,11-Oct-2020 07:20:57,dojoteef,MachineLearning,"The mods wanted to weigh in on why we left this post up, as we have gotten several reports on this thread. A major reason is that the post clearly describes the process undertaken and the ethical challenges faced. Additionally, the post is behind a NSFW tag, with a clear title, so individuals can avoid it if they wish. We understand that not everyone will agree with this decision, but we believe in fostering open and respectful discussion, even of difficult subjects, which the OP has demonstrated this far. We have been moderating the comments to keep the discussion civil and will continue to do so.",/r/MachineLearning/comments/j8njqq/p_generating_and_animating_porn_using_ai/g8hmqir/
1602342464.0,10-Oct-2020 08:07:44,dojoteef,MachineLearning,Please describe the research that was used to generate this video. Link to an associated paper and/or code repository.,/r/MachineLearning/comments/j8ldyv/pjfk_says_i_like_your_funny_words_magic_man_lip/g8buvot/
1601745949.0,03-Oct-2020 10:25:49,dojoteef,MachineLearning,">	Reduce B by pruning to match S.
>
>	Call it B'
Copy all parameters of S to B' as a one-to-one bitexact copy.

These types of statements are littered throughout your proof. The problem is that they are not mathematically precise. How do you guarantee you can prune B and still match S? Are there any bounds on this statement? Etc.",/r/MachineLearning/comments/j48hx3/d_what_are_some_good_examples_of_models_that_are/g7j4l1v/
1601739785.0,03-Oct-2020 08:43:05,dojoteef,MachineLearning,"I agree that a Monte Carlo simulation like this is very noisy. You would hope they ran lots of simulations and reported the variance (haven’t fully read the paper yet to verify this point). It also assumes that papers with similar mean review scores would have similar variability in scores independent of other factors, which might not hold.

How would you design the analysis to address these perceived shortcomings?",/r/MachineLearning/comments/j442wp/r_an_open_review_of_openreview_a_critical/g7itlpl/
1601380828.0,29-Sep-2020 05:00:28,dojoteef,MachineLearning,"You make some very valid points. There have definitely been discussions among the mods recently of what criteria should be used for removing posts. /u/programmerChilli  gave some reasoning for the current moderation [here](https://reddit.com/r/MachineLearning/comments/j1s3yw/_/g718ln9/?context=1), where he solicits community feedback.",/r/MachineLearning/comments/j1s3yw/d_recent_increase_in_self_promotion_content/g71u9m8/
1600983072.0,24-Sep-2020 14:31:12,dojoteef,MachineLearning,"In theory this sounds nice, but for responsible AI, the devil is in the details. For example, how does Trove ensure those submitting photos own the rights? How do you guarantee submitted photos do not contain sensitive information? Etc

The reason I ask is that none of this information seems to be on the website, at least the FAQ didn’t address it. Rather the FAQ tells users they shouldn’t do these things, but that doesn’t mean they will either read these points in detail (like removing photo metadata), or not accidentally submit confidential/privileged data.

It’s fine if you don’t have a moderation team that actively filters the data, but if you don’t then such limitations should be clearly communicated.",/r/MachineLearning/comments/iz5vhv/n_introducing_a_new_way_to_responsibly_collect/g6gzasj/
1600884435.0,23-Sep-2020 11:07:15,dojoteef,MachineLearning,"Sorry, but at least the company I worked for, you could not send an email, personal or not, with or without a disclaimer. This was a well-known tech company in SF. Though, your experience may be different.",/r/MachineLearning/comments/ixvyyv/d_job_application_failed_technical_assessment/g6ca9js/
1600829009.0,22-Sep-2020 19:43:29,dojoteef,MachineLearning,"Typically companies do not provide feedback on interviews to avoid potential litigation. It’s definitely weird and annoying, but that’s the reason why. How I know: as an engineering manager who interviewed lots of people, I was not allowed to provide feedback to people I interviewed, with this as the stated reason by HR.",/r/MachineLearning/comments/ixvyyv/d_job_application_failed_technical_assessment/g6a4wx5/
1600607573.0,20-Sep-2020 06:12:53,dojoteef,MachineLearning,"Letters of recommendation provide information that does not exist in other parts of an application for many students: what is their potential to conduct research. If you are one of the few applicants that has published a first author paper before applying for a PhD, a recommendation letter might not be necessary. For most applicants, potential advisors want to know how individuals face challenges, how creative the person is when it comes to problem solving, etc. These are important considerations for conducting research and completing a PhD, but are otherwise poorly represented when looking at academic achievement alone.",/r/MachineLearning/comments/iw6lk4/d_phd_applications_that_do_not_require_references/g5z3tta/
1600427682.0,18-Sep-2020 04:14:42,dojoteef,MachineLearning,"Just to add more context to this, some of the individuals who have stated they are going to publish in Findings got reasonably high scores as well: [4,3.5,4 for example](https://twitter.com/VictoriaLinML/status/1305942337646817280?s=20).",/r/MachineLearning/comments/iv3q19/d_emnlp_findings_vs_aaai/g5owdk6/
1600168849.0,15-Sep-2020 04:20:49,dojoteef,MachineLearning,I received an accept rather than an accept-findings.,/r/MachineLearning/comments/it4pxq/d_emnlp_2020_results_announced/g5cffo8/
1600168704.0,15-Sep-2020 04:18:24,dojoteef,MachineLearning,"While I got an acceptance, I’m super demoralized by the process. Reviewer number one stated our paper was very poorly written, while reviewer number three said it was well written...

Presumably the AC is the only reason our paper was accepted because we got borderline scores from the reviewers and the final reviewer scores are unchanged. Though, the meta review states our rebuttal addressed most of the reviewer concerns. I guess it could be that the reviewers said so during discussion, but failed to update their scores 🤷🏽‍♂️

I’m confident my paper is better than my previous publication, which received higher scores at ACL 2019. Needless to say, the process feels way too stochastic for my comfort.",/r/MachineLearning/comments/it4pxq/d_emnlp_2020_results_announced/g5cfa06/
1600136715.0,14-Sep-2020 19:25:15,dojoteef,MachineLearning,"I’d second this viewpoint. It’s really nice being able to watch a presentation ahead of time, then talk to the authors in a zoom session, especially when it’s not super crowded.",/r/MachineLearning/comments/isr2hx/d_virtual_icmlneuripseccv_conferences_worth_it/g5be0my/
1599953205.0,12-Sep-2020 16:26:45,dojoteef,MachineLearning,"Maybe we are talking past each other, but I'll try one more time. CS has a pretty accepted definition of constant time; scaling linearly with the number of processors does not meet that definition. Thus, the claim in the paper is misleading. It's not a misunderstanding on my part if the claim is misleading. The burden is on the author to clearly describe their work. When someone says almost constant time, I'd expect runtime proportional with the [inverse Ackermann function] or a similar class of function, which this is not.

BTW, it's interesting that the wikipedia article you link lists CNNs running on a GPU as an example of being [naively parallelizable](https://en.m.wikipedia.org/wiki/Embarrassingly_parallel#Examples)...",/r/MachineLearning/comments/ir87xc/r_massively_parallel_and_asynchronous_tsetlin/g4zzagb/
1599943852.0,12-Sep-2020 13:50:52,dojoteef,MachineLearning,"It’s not a misunderstanding on my part. By their definition an MLP or a  Transformer would exhibit “almost constant time” scaling. Considering [DeepSpeed](https://www.deepspeed.ai/news/2020/05/27/fastest-bert-training.html) can train BERT in 44-minutes, I’m pretty sure if you ran it on the comparatively tiny SEMEVAL and IMDb datasets like they do in this paper Microsoft could easily claim BERT has “almost constant time” scaling. Clearly, that is not a reasonable statement. No one who works on Transformers for classification would claim constant time scaling.

This is not to diminish the work of the researchers. They definitely did a big effort to write their own CUDA kernels and show this approach is at least viable for some benchmark datasets. The issue I have is purely with this dubious statement of constant time performance.",/r/MachineLearning/comments/ir87xc/r_massively_parallel_and_asynchronous_tsetlin/g4z7h16/
1599934027.0,12-Sep-2020 11:07:07,dojoteef,MachineLearning,"Even if it has been submitted to AAAI, the [call for papers](https://aaai.org/Conferences/AAAI-21/aaai21call/) allows for preprints and does not seem to explicitly disallow publicizing the preprint as long as the authors do not mention that the paper has been submitted for review at AAAI.

I’m definitely not a fan of preprints breaking double blind, which is easily broken when reviewers check for plagiarism and dual submissions (that’s why I recommend completing the entire review process as if the paper isn’t a dual submission and do the check at the very end). On the other hand, I am not sure this violates the double blind policy as written.",/r/MachineLearning/comments/ir87xc/r_massively_parallel_and_asynchronous_tsetlin/g4yezaf/
1599912974.0,12-Sep-2020 05:16:14,dojoteef,MachineLearning,"Great work! I’m glad the authors were able to extend their approach to CUDA and (presumably) as a result demonstrate the effectiveness on some benchmark datasets. I think that has been the main issue sticking point in the previous published research.

That said, please remove the “Supporting Almost Constant-Time Scaling” from the title, abstract and body of the paper. It is disingenuous and very deceptive. With the implied definition of almost constant-time scaling in the paper, it seems that any algorithm that doesn't saturate the parallel processing capabilities of the hardware achieves ""almost constant-time scaling"".

> From approximately 7,000 clauses and onwards, however, we observe proportionally increasing execution time, e.g., execution time doubles going from 7,000 to 14,000 clauses (Figure 11). This can potentially be explained by the Tesla V100 GPU having 5,120 cores.

From the sounds of it, you all actually achieve linear scaling. That's a great result! Please don't try to oversell what's possible.",/r/MachineLearning/comments/ir87xc/r_massively_parallel_and_asynchronous_tsetlin/g4x8lr3/
1599330654.0,05-Sep-2020 11:30:54,dojoteef,MachineLearning,"I feel like r/machinelearning is just not an appropriate place for discussing an individual’s “reputation”. Feel free to ask about his podcast, teaching, or research instead. Talking about a individual’s motives, intents, or other people’s views on an individual (rather than a specific statement or work from the individual) just risks unnecessary character attacks which shouldn’t be normalized on this sub.",/r/MachineLearning/comments/in3vjh/d_what_is_lex_friedmans_reputation_in_the_ai_world/g45fzdb/
1599227354.0,04-Sep-2020 06:49:14,dojoteef,MachineLearning,"Really interesting paper for those interested in grounded language and generalization. This tweet thread does a good job of summarizing the takeaways:


[https://twitter.com/NPCollapse/status/1301814000255217664](https://twitter.com/NPCollapse/status/1301814000255217664)",/r/MachineLearning/comments/imabzf/r_grounded_language_learning_fast_and_slow/g3zerl7/
1599174015.0,03-Sep-2020 16:00:15,dojoteef,MachineLearning,"You are not getting orders of magnitude speedup when you bypass the CPU and load textures directly from disk to the GPU. Rather, the bigger speedup is from switching optical disk drives and magnetic disk drives to NVMe, which can be about a 6x speedup.

Realistically, you are likely to see a 2-3x speedup at most by loading directly from disk to GPU. For example, here's a research paper from 2017 which explores the idea: [SPIN: Seamless Operating System Integration of Peer-to-Peer DMA Between SSDs and GPUs](https://www.usenix.org/system/files/conference/atc17/atc17-bergman.pdf).

The UE5 tech demo is mostly to sell gaming executives on the idea of using Epic's tech, which includes build hype amongst journalists and gamers. By requiring NVMe (which the PS5 has), you can expect the 6x speedup combined with the 2-3x speedup to achieve the order of magnitude speedup being hyped.",/r/MachineLearning/comments/ilvkyi/d_nvidias_rtx_3000_series_and_direct_storage_for/g3x1w1c/
1599139896.0,03-Sep-2020 06:31:36,dojoteef,MachineLearning,"You are not insane. The problem is that people believe interviews are somehow objective, but more often than not, they reflect the biases of the interviewer. Most coding and/or technical questions are not the kind you would be under pressure to solve on a regular basis. Thus, interviews often select for people who are good interviewers, rather than people who are a good fit for the role.

Just like any measure, an interview is a proxy for what you really care about. We have difficulty defining appropriate measures for assessing real people; is it any wonder that many of our quantitative measures for machine learning systems are also flawed?

I’ve been a lead engineer and an engineering manager, and I can attest to missing out on talented individuals who interview poorly being a common occurrence. One of the *most* talented people I have ever worked with did quite poorly during their interview. We really needed someone at the time, so we took the risk, and whoa did we underestimate this person’s abilities. They quickly moved into a leadership position.",/r/MachineLearning/comments/ilkgkj/d_what_are_some_good_resources_to_practice/g3uc4ux/
1598216629.0,23-Aug-2020 14:03:49,dojoteef,MachineLearning,"I have some experience working remotely as a research assistant.

Before starting my PhD, I had been working for nearly a decade. I knew I wanted to go for a PhD, and needed practical experience doing research for upcoming applications. I had quit my job and began teaching myself enough background to be able to start on a research career. I had a few open source “research” projects under my belt that I used to contact a new professor (they had just graduated and didn’t have any students yet). I ended up working with the professor remotely for around 6 months (for free) and he was kind enough to help me get a preprint of the work out in time for PhD applications (which was a huge undertaking on his part). He also wrote me a letter of rec, which my current advisor stated was one of the highlights from my application material (despite being very successful in industry – was previously a lead engineer and an engineering manager; one of my letters of rec was from the head of platform development at Riot Games at the time, someone I worked with closely for several years).

Needless to say, professors value research experience over all else. I’m not sure where to find postings like the one I found if you’re looking for professors to work with. At the time, I found the professor on [deeplearning.net](http://deeplearning.net/deep-learning-job-listings/). Looks like it hasn’t been updated in a while.

I’d search around various professors websites and possibly send an email if they seem like a good fit. I’d recommend targeting newly appointed professors and avoid the top schools, as they are likely inundated. You can find rankings at [csrankings.org](https://csrankings.org).

Good luck!",/r/MachineLearning/comments/ifa5fe/d_work_as_a_research_assistant_remotely/g2mhiut/
1597766782.0,18-Aug-2020 09:06:22,dojoteef,MachineLearning,"This question seems better suited for stackoverflow or r/learnmachinelearning. When you do post to one of those forums, I’d recommend you explicitly state what frameworks you are using, possibly provide a minimal code example for what you’re trying to do and where it is failing.",/r/MachineLearning/comments/ic165f/d_how_to_train_an_autoencoder_with_different/g1zwdbm/
1597766606.0,18-Aug-2020 09:03:26,dojoteef,MachineLearning,"I’m sorry you’re struggling. I know it’s tough, but this question seems better suited for stackoverflow or r/learnmachinelearning.",/r/MachineLearning/comments/ic2s9n/d_debugging/g1zvvpu/
1597427028.0,14-Aug-2020 10:43:48,dojoteef,MachineLearning,"I’ll share my experience. The practical lessons I learned should hopefully apply, but the specifics might not translate well: I’ll explain why. I completed undergrad in 2004, having doubled majored in CS and Math. I subsequently worked in industry (video games and a tech company), where I became a lead engineer and then an engineering manager. So when I started as a first year PhD student in 2018, I had lots of practical experience, though I had never worked on NLP (my research area). I had taught myself probability and the basics of ML, and even did a couple of ML side projects for experience over the span of a couple of years before beginning my PhD.

My first project was suggested by my advisor. The goal was to speed up the Transformer architecture for sequence to sequence tasks. I decided to start from square one and implement a Transformer from scratch only by referencing the paper, rather than existing reference implementations. That truly made me appreciate the clarity of the paper, yet it was a tough slow process. The types of problems I had to solve ran the gamut, from “simple” out of memory errors, to more insidious random sampling errors that kept the model from converging.

During the first semester I kept feeling like I was behind somehow because I kept shooting for the ICML paper deadline, as if it was realistic to start from scratch in a new field, come up with a novel idea, perform experiments, and write a paper all on the span of three months...

By the end of the semester, I had a strong Transformer baseline working and had already started on researching new ideas for speeding up decoding, yet I didn’t have a workable idea yet. It was a tough semester, since I had classes in addition to my research. Luckily, I was an RA my first semester.

My second semester I was a TA and taking two classes in addition to my research. I worked insane hours, but I got a publishable idea and submitted a paper to ACL, which was accepted. Having worked in the video game industry, I was familiar with crunch time, but I honestly worked harder my second semester than I have at any point in my life. I nearly had a breakdown.

So, yes I gained a lot of experience and got a published paper out in my first year (which few other PhD students in my University cohort did), yet the toll was extremely high. If I had it to do over again, I really would not have worked myself so hard. I nearly decided a PhD wasn’t worth it, especially because of the stress on myself and on my wife who had to take up all the slack since I worked a string of 100 hour work weeks. No publication is worth that amount of stress.

My advice is not to worry about whether or not you’ll publish in your first year. Rather, it’s more important that you use the time to set yourself up for success later in your PhD studies. My understanding is that early pubs are rarely the ones that stand out in terms of research impact, which is what most people care about.

Good luck with your research!",/r/MachineLearning/comments/i9mdd2/d_those_of_you_who_published_in_a_top_conference/g1gqbnn/
1597321492.0,13-Aug-2020 05:24:52,dojoteef,MachineLearning,"Considering the ACL 2020 [best paper](https://venturebeat.com/2020/07/09/ai-researchers-create-testing-tool-to-find-bugs-in-nlp-from-amazon-google-and-microsoft/) award winner is about functional tests for models, it seems researchers in NLP find value in testing. You could make the case that these are integration tests, rather than unit tests, but ultimately there is value in designing hand written tests for models in addition to monitoring metrics on a validation set.

That said, the OP does talk about shape based unit tests, which I agree might be less useful. Unit testing data augmentations on the other hand is useful for researchers. I’ve been working on some research where the data augmentations had bugs in it, which set back the research a couple of weeks due to training time, etc. Catching it early through unit tests would have likely sped up the research.",/r/MachineLearning/comments/i8e0cu/d_reusable_unit_tests_for_deep_learning_projects/g1bk7uu/
1596908751.0,08-Aug-2020 10:45:51,dojoteef,MachineLearning,"I think the following [tweet](https://mobile.twitter.com/mgb_infers/status/1292019673685536768) sums up my perspective. We need a system that provides feedback to reviewers about their review quality and specific ways they can improve future reviews. Yes, this requires effort (possibly from ACs and senior reviewers), but this is the only way to meaningfully improve review quality, especially with explosive growth in the field. What matters more than the score, is the actual quality of the review. A labmate got a positive review for EMNLP, but it was one of the lowest effort reviews ever. The **entire review** (summarizing the paper, listing strengths and weaknesses, reasons to accept/reject, etc) is only 60 words. This comment alone is twice as long!",/r/MachineLearning/comments/i60xmr/r_ban_reviewers_who_write_low_quality_reviews/g0t3hxq/
1596818408.0,07-Aug-2020 09:40:08,dojoteef,MachineLearning,"Ugh... utterly disheartened. We really need conferences to improve the quality of reviews. At least, it was only one of the reviews that makes a mockery of the process. The other two at least wrote something competent, even if I disagree a bit with their judgement.",/r/MachineLearning/comments/i5fpnl/d_emnlp_2020_paper_reviews/g0p32av/
1596814177.0,07-Aug-2020 08:29:37,dojoteef,MachineLearning,"I’m definitely anxious. I feel very confident in the work I did, and I believe the subsequent paper is pretty strong. Yet, it’s hard to know if the reviewers will value the contribution, especially with the explosive growth in the field. Hopefully the [guidance](https://mobile.twitter.com/emnlp2020/status/1288500781935722497) provided to reviewers, will make a difference.",/r/MachineLearning/comments/i5fpnl/d_emnlp_2020_paper_reviews/g0otmz8/
1596663022.0,05-Aug-2020 14:30:22,dojoteef,MachineLearning,"Does Amazon internally use SageMaker for its ML needs? If so, what were the workflows like before the advent of SageMaker, and how have they changed after switching to SageMaker. What were the major lessons learned during the transition period?",/r/MachineLearning/comments/i4a3cf/d_ama_we_are_aiml_experts_from_aws_ask_us/g0ho8jo/
1596632243.0,05-Aug-2020 05:57:23,dojoteef,MachineLearning,"I certainly don’t disagree if you are saying the opportunity cost isn’t worth it, if what you care about is money. I generally disagree that you will need to pay extra out of pocket to live. Additionally, PhD students often get paid reasonably well for internships (though you are limited to 3, I think, for international students).",/r/MachineLearning/comments/i3ve2e/d_universities_attended_by_cs_phd_students_at/g0ftx0d/
1596631096.0,05-Aug-2020 05:38:16,dojoteef,MachineLearning,"I’m not advocating that people go to Stanford, rather than going to a university in their home country. People can do what they want. But many of your statements are just wrong. CS PhD students *don’t* pay their own tuition, rather tuition is paid and they get paid a stipend in addition. This isn’t a Stanford specific phenomenon; that’s the way it works generally, even for international students. For example, as a CS PhD student, I get paid $33k per year, just like *every* PhD student in my department. There might be cases where this is not true, but they are the rare exception, especially at a top school. Maybe you’re mixing it up with a Masters degree, where you pay your own costs.

Additionally, it is much easier to get an internship in the summer at places like Google, when they are literally a 10 min drive away from Stanford. Again, not advocating for going to Stanford. But saying it’s just as easy to get similar opportunities at top Universities in Brazil seems to ignore certain realities afforded by Stanford’s CS PhD program.

NOTE: I do not go to Stanford.",/r/MachineLearning/comments/i3ve2e/d_universities_attended_by_cs_phd_students_at/g0frxcc/
1596589371.0,04-Aug-2020 18:02:51,dojoteef,MachineLearning,Interesting. Are these only ML-focused CS PhD students?,/r/MachineLearning/comments/i3ve2e/d_universities_attended_by_cs_phd_students_at/g0eap4s/
1596465962.0,03-Aug-2020 07:46:02,dojoteef,MachineLearning,"I think he makes a very valid point. How exactly we quantify “general intelligence” determines whether or not we actually achieve AGI. The moving goal post for AI has existed for a long while, likely because we are not all talking about the same thing when we say AGI. Just look at the Turing Test and Chinese Room Experiment. Fundamentally, our commonly professed notions of intelligent behavior are similar to the US Supreme Court Justice Potter Stewart‘s statement regarding what constitutes hard-core pornography: “I’ll know it when I see it.” That is left to individual judgement, with all the problems an argument like that entails. No wonder many lay people think we are so close to AGI with research like GPT-3 coming out.

Maybe a serious researcher should catalogue a specific list of capabilities which constitutes a notion of general intelligence, and use that to set a reasonable goal post, rather than the vague descriptions often bandied about. I honestly think the GPT-3 paper does a good job of listing limitations. Maybe this can be built upon to describe functionality that is required to claim AGI has been achieved.",/r/MachineLearning/comments/i2wunb/d_a_very_short_history_of_some_times_we_solved_ai/g07ned1/
1596131857.0,30-Jul-2020 10:57:37,dojoteef,MachineLearning,"This almost sounds like a trick question. You often can fit the training data exactly, so exclusively focusing on training data performance seems like a flawed notion, unless you have a compelling reason to ignore generalization, e.g. maybe you want to empirically show a class of loss functions cannot exactly capture the training data distribution. For most research, you must look at validation/test data to ensure your model generalizes.",/r/MachineLearning/comments/i0qbm8/d_what_constitutes_a_project_being_ready_for/fzr462w/
1596115126.0,30-Jul-2020 06:18:46,dojoteef,MachineLearning,Wow. That was certainly a more interesting read than I was expecting from an annotated fruit dataset. I especially liked your descriptions of the various conditions lemons can exhibit and your annotations video. Looks like you did a really thorough job producing the dataset. Kudos!,/r/MachineLearning/comments/i0he48/p_lemon_dataset/fzq5dqc/
1595962408.0,28-Jul-2020 11:53:28,dojoteef,MachineLearning,"I understand your concern. I worked in the video game industry for nearly a decade, which is notorious for overworking people (I was relatively lucky in this regard, but know horror stories from numerous friends). I definitely would not advocate trading family time to release research code!

On the other hand, what you're describing seems to be systematic abuse by a company that wants you to do unpaid work: having you state you will release code in your published paper, but not paying you to do so. That seems like the fundamental problem. Like all cases of abuse, it sounds daunting to find a new job, but staying at a company with questionable practices catches up to you sooner or later. I'd recommend trying to find a company doing research that values you. Those are my 2 cents.",/r/MachineLearning/comments/hzdiru/d_if_you_say_in_a_paper_you_provide_code_it/fzjguxr/
1595941944.0,28-Jul-2020 06:12:24,dojoteef,MachineLearning,"BTW, if you haven’t read the paper, I would highly recommend doing so. It won the best paper award at ACL this year. While the paper focuses on NLP tasks, I think the general methodology is applicable to any ML model, i.e. writing unit test-like evaluations for models.",/r/MachineLearning/comments/hzaq92/d_behavioral_testing_of_nlp_models_with_checklist/fzic9ys/
1595941264.0,28-Jul-2020 06:01:04,dojoteef,MachineLearning,"I don’t think it’s malicious on the part of authors. I’m sure they intended to release the code when they stated so in the paper. Unfortunately, since there is no hard requirement that you follow through, it falls off of people’s radar. One big issue I’ve seen is that authors feel the need to tidy up their code for release, and it often takes longer than they expect. Sometimes they’re already deep into another research project. Another issue that occasionally comes up is getting a release from the company/organization that conducted the research to release the code.

I think prompting authors by emailing them and asking for the code is a good approach. It can spur them to prioritize getting the code ready for release. In fact, that might be a good checklist item for conferences to put as part of the camera ready: either remove that claim from the paper or ensure the code is released.",/r/MachineLearning/comments/hzdiru/d_if_you_say_in_a_paper_you_provide_code_it/fzib7sb/
1595859126.0,27-Jul-2020 07:12:06,dojoteef,MachineLearning,"If you’re interested in artistic language, then I would consider UMass Amherst (where I am pursuing a PhD). My advisor [Mohit Iyyer](https://people.cs.umass.edu/~miyyer/) conducts research in creative language (like works of fiction) and using NLP to [analyze social biases](https://arxiv.org/abs/1909.03343). [Brendan O’Connor](http://brenocon.com/) focuses on applying NLP to the social sciences, including analyzing biases on social media. Finally, we just hired a new faculty member [Laure Thompson](http://www.cs.cornell.edu/~laurejt/) who is starting in the Fall and focuses on digital humanities, including analyzing cultural/historic works.

Additionally [USC Institute for Creative Technologies](https://ict.usc.edu/research/faculty/), CMU, GATech, Cornell, Columbia and University of Washington all produce research investigating the analysis and generation of works of fiction. You can also look at the recent [Workshop on Narrative Understanding, Storylines, and Events](https://sites.google.com/view/nuse) to provide more leads.",/r/MachineLearning/comments/hyhsjk/discussion_where_to_apply_for_a_phd_in_ml_with_an/fzeloz3/
1595811232.0,26-Jul-2020 17:53:52,dojoteef,MachineLearning,"If the tokenization scheme is not reported in the paper (which it is not), then you are assuming the authors used the Moses tokenizer. If reviewers make assumptions like these, without requiring authors to report the methodology in the paper, they are simply adding to the replication issues that all the conferences are actively trying to address. For example, the numbers from Vaswani et al. in the original Transformer paper [do not use a standard BLEU evaluation](https://github.com/pytorch/fairseq/issues/346#issuecomment-436588347), so their numbers are not directly comparable.

In general, both authors and reviewers should make an effort to ensure reproducibility. This requires being explicit about how the evaluation was performed (including how tokenization effects the reported numbers).",/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/fzcw106/
1595771895.0,26-Jul-2020 06:58:15,dojoteef,MachineLearning,"It’s baffling to me that a MT paper published in ICML 2020 is still reporting performance using multi-bleu.perl. I understand wanting to make your numbers comparable with older papers, but multi-bleu.perl is dependent upon tokenization and specifically states so in its output!

> It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.

They should at least use [sacreBLEU](https://github.com/mjpost/sacreBLEU) (or detokenize then run mteval) and include those numbers in the paper in addition if they really feel the need to report multi-bleu.perl. I have little faith in reviewers if this wasn’t suggested.",/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/fzauvlo/
1595729753.0,25-Jul-2020 19:15:53,dojoteef,MachineLearning,"Another paper which investigates attention in Transformer models with the hopes of speeding up the attention computation: [Hard-Coded Gaussian Attention for Neural Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.687/).

I'm actually starting to work on some new ideas that can be seen as similar to the compressive history approach. Ultimately, I think those are the best ways forward, as not every token being modeled has the same amount of entropy, thus it should be possible to compress the history based on entropy without too much (or any) loss in modeling ability. At least, that's my current opinion.",/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/fz9h1ze/
1595172164.0,19-Jul-2020 08:22:44,dojoteef,MachineLearning,"I wouldn’t call most research useless, though it is often incremental. You also seem to be calling out replication and experimental design issues. I agree that these are a concern. Sadly, the field often does not require strong experimental design, instead favoring novelty over scientific methodology (often relegating it to future work). It’s the responsibility of reviewers to ensure these issues are addressed as well as they can be in a paper before recommending acceptance.


I’m reviewing some papers for EMNLP right now, and despite one paper reporting numbers that are SOTA, I’m likely going to argue that it needs be revised and submitted to a later conference, because the experimental design does not instill any confidence. Somehow their larger baseline models perform worse than the smaller baselines, except when they add their methodology. Considering they didn’t report any validation numbers (or mention validation in any way), it seems likely that they overfit to the training data on the larger models, except when employing their approach. They make no effort to explain the phenomenon (not in the text, nor via a graph of validation performance as training progresses, etc). That doesn’t make for a fair comparison. Hopefully the other reviewers (and ultimately the AC) agree that this is indeed an issue.",/r/MachineLearning/comments/hu0tdy/is_it_just_me_or_are_most_research_papers_useless/fykcjd6/
1595088837.0,18-Jul-2020 09:13:57,dojoteef,MachineLearning,"Please try to follow the rules. [Be nice: no offensive behavior, insults or attacks](https://www.reddit.com/r/MachineLearning/about/rules/).",/r/MachineLearning/comments/htacsz/the_computational_limits_of_deep_learning/fyh1dor/
1594989100.0,17-Jul-2020 05:31:40,dojoteef,MachineLearning,"The bazaar model reminds me of the [Indian Buffet Process](https://en.wikipedia.org/wiki/Chinese_restaurant_process#The_Indian_buffet_process), which leads a small handful of reseachers to become popular, with a long tail of less popular researchers. This favors the rich get richer by more established researchers. See this Chinese Restaurant Process [animation](https://en.wikipedia.org/wiki/File:Chinese_Restaurant_Process_for_DP(0.5,H\).webm) for a visualization of a related process.",/r/MachineLearning/comments/hslo4l/d_anonymizing_arxiv_submissions_to_ensure_a/fycq5km/
1594988351.0,17-Jul-2020 05:19:11,dojoteef,MachineLearning,"Thanks for starting this conversation!

A number of established researchers have expressed concern about not being able to disseminate their ideas faster, i.e. via arXiv. What seems to be talked about less is how arXiv+Twitter is a system of the rich get richer, because only established researchers have the Twitter followings to reach a large audience (or their PhD students).

Reviewing is also being discussed by ACL (and associated conferences). There are two proposals to improve reviewing: a [short-term proposal](https://www.aclweb.org/adminwiki/index.php?title=Short-Term_Reform_Proposals_for_ACL_Reviewing) (can be implemented within a short period of time) and a [long-term proposal](https://www.aclweb.org/adminwiki/index.php?title=ACL_Rolling_Review_Proposal) (requires more time to implement).

The long-term proposal tries to address the issue of preprints by having a rolling review process, with reviews done in batches each month, similar to a journal. This way, the turnaround time before you can publicize your work is hopefully much shorter. Unfortunately, this proposal likely doesn’t work as well outside of the ACL system. There are multiple conferences affiliated with ACL each year (A/E/NAACL, CoNLL, EMNLP, etc), which allows this proposal to have some chance of working. The goal being, that from the time you finish writing a paper, to the time you can publicize it is 2-3 months, rather than potentially up to a year (assuming preprints are not allowed).

My concern is that even this rolling review idea will get shot down, because:

* Considering the number of NLP papers submitted to conferences like ICML, ICLR, and NeurIPS there will still be strong pressure to allow preprints, since these conferences do.
* Many think the review process is too stochastic. If their paper does not get accepted, they still want it publicized.
* Established researchers want to disseminate their ideas *now*, not in 2-3 more months.",/r/MachineLearning/comments/hslo4l/d_anonymizing_arxiv_submissions_to_ensure_a/fycp44e/
1594865124.0,15-Jul-2020 19:05:24,dojoteef,MachineLearning,"I agree. I was at ACL last week, and I had a MUCH easier time speaking with people at their Q&A sessions because it wasn't packed. It was great to be able to listen to the pre-recorded talks that each person had (at 2x speed-up), then take a quick look at the paper, and finally jump into a Q&A session where I was already up to speed on their work and could hold an in-depth conversation on their work.

On the other hand, I did lament the fact that it was much less serendipitous, and MUCH harder to have a sense of social presence during the conference.

I think I'd prefer a hybrid conference approach, where there are pre-recorded talks I can watch, and then I can go to the poster sessions in person if I want (even if it turns out to be more packed).",/r/MachineLearning/comments/hrz0yw/online_conferences_suck_d/fy7lu3x/
1594837788.0,15-Jul-2020 11:29:48,dojoteef,MachineLearning,"I agree that obsession is toxic, and certainly is unneeded to successfully complete a PhD!

On the other hand, the type of obsession that Ian Goodfellow mentions often goes hand-in-hand with becoming *the top in a field*. It comes at great personal expense and requires a *tremendous* amount of hard work, in addition to any luck and natural talent you may have. The essay titled [On Being Smart](https://kam.mff.cuni.cz/~matousek/mustafa-onbeingsmart.pdf) points out the often false narrative that intelligence (rather than hard work) leads to success, by citing people including Fields Medalists and Gauss.

It isn't restricted to pursuits of intelligence either; this phenomenon often distinguishes [top athletes](https://www.forbes.com/sites/roddwagner/2019/03/01/the-seven-lessons-from-free-solo-on-working-without-a-rope/#6de3bdf01c0a), [business people and others](http://americasobsessives.com).

Most people would not consider those examples to be healthy. You should instead listen to [the advice of the Nobel Prize\* committee](https://www.nobelprize.org/hard-work/) and ensure not to overwork yourself!

\* I couldn't find similar guidance from the ACM (the organization which bestows the Turing Award), so decided to use the Nobel Prize guidance instead.",/r/MachineLearning/comments/hrnsvu/d_how_obsessed_do_you_need_to_be_to_succeed_in_ml/fy64xle/
1594764483.0,14-Jul-2020 15:08:03,dojoteef,MachineLearning,"Those are very interesting results. Have you considered finding images from the training set (FFHQ) that exhibit various diverse attributes (age, gender, race, etc) and see how PULSE performs on them? You would hope images from the training set would be easier to recover, even when downscaled. If they aren’t easy to recover from a downscaled image, but the GAN is able to model the diversity (compute the latent from the trained image, then sample that latent to generate an image), then it seems like there are aspects of the latent space that aren’t being adequately searched (with your method or the one from PULSE).",/r/MachineLearning/comments/hr87b0/p_improving_pulse_diversity_in_the_iterative/fy2zysw/
1594752586.0,14-Jul-2020 11:49:46,dojoteef,MachineLearning,"I think both the lack of standardized rejection criteria, and how ACs did not provide clear justification for the rejection, needs to be addressed in future conferences (if they decide to continue the desk rejection process). It’s very discouraging to see ACs engage in poor reviewing practices; I remember discussions at ACL this year mentioning how level of experience was not correlated with higher quality reviews. It’s sad that as a community we allow this to happen.

A better way they could have experimented with the desk rejection process, would be to have ACs flag papers they thought should be desk rejected, but let all papers go through review and see whether or not the papers were ultimately accepted. Such an experiment would provide an empirical measure of the noisiness of the process. We already know reviews overall are noisy (hence we have multiple reviewers in hopes to reduce the noise a bit); my guess is that desk rejection is even more noisy.",/r/MachineLearning/comments/hqrxvs/d_neurips_desk_rejects_have_gone_out/fy2baut/
1594676767.0,13-Jul-2020 14:46:07,dojoteef,MachineLearning,"This made me think about the [ACL Anthology](https://aclweb.org/anthology). Just last week the current lead maintainer of the anthology was [describing issues](https://slideslive.com/38929529/anthology-report) they have to work through, including author disambiguation and working with the folks at Google Scholar to fix an indexing bug (see slides 11-15).

I can hardly imagine how much time and effort it takes to keep resources like Google Scholar and Semantic Scholar correctly ingesting all the different citation formats across disparate journals, conferences, and books. I agree that I wish they could do better, but I realize it’s a large effort to get things right.",/r/MachineLearning/comments/hqaazh/d_hintons_google_scholar_profile_keeps_inflating/fxz1zhz/
1594486723.0,11-Jul-2020 09:58:43,dojoteef,MachineLearning,"The linked blog post discusses the use of super resolution techniques, using multiple source images. While this is better than single source super resolution, it has the same issues of potentially hallucinating features not supported by the source images. Have you instead considered conditioning the predictions on an ordered series of images, rather than the use of super resolution?",/r/MachineLearning/comments/hp94e4/research_using_artificial_intelligence_to_scale/fxpb4ar/
1594406973.0,10-Jul-2020 11:49:33,dojoteef,MachineLearning,"Well, they show it is empirically faster (when they train w/RL), thus it is clearly faster. They (rightly) shy away from stating the learned algorithm is faster than O(n log n), thus it can only be faster by a constant factor.",/r/MachineLearning/comments/hoimid/r_strong_generalization_and_efficiency_in_neural/fxk3vmn/
1594405079.0,10-Jul-2020 11:17:59,dojoteef,MachineLearning,"There are two major factors:

* Big-O notation only considers bounding behavior, thus ignores constants.
* Quicksort is a randomized algorithm, that has an O(n^2) worst-case performance, but via statistical analysis, is shown to have O(n log n) performance on average. Empirically, it's been shown to be faster than other O(n log n) algorithms like mergesort, which is why people often use it.

As far as I know, we do not have any techniques for analyzing the exact theoretical performance of ML-induced algorithms, so empirical analysis is all that can be done (except proving a bound based on the available operations as done in the paper). The wording from the paper makes that clear:

> In supervised learning, our neural controller can imitate quick sort perfectly, achieving the same performance as the teacher across all instance sizes, therefore capable of learning O(n log n) algorithms.

Then they show their RL version has faster empirical performance (thus faster by some constant factor), but do not state it is a faster class of algorithm.",/r/MachineLearning/comments/hoimid/r_strong_generalization_and_efficiency_in_neural/fxjzy37/
1594327245.0,09-Jul-2020 13:40:45,dojoteef,MachineLearning,Please see [The Early Phase of Neural Network Training](https://arxiv.org/abs/2002.10365) for some of the importance of the early stages of training.,/r/MachineLearning/comments/hnx1jn/r_what_are_your_hot_takes_on_the_direction_of_ml/fxgm7qt/
1594319944.0,09-Jul-2020 11:39:04,dojoteef,MachineLearning,"If I’m correctly understanding your perspective, this is a deeper philosophical question than you are making it out to be. Surely morals and ethics cannot be thought of as absolute truths, hence any attempt to include ethical considerations in an ML model could never be thought of as “reflective of reality”. For example, some concepts like child prostitution are nearly universally considered immoral, yet they occur. Thus an ML model that predicted it as a valid action in the world would indeed reflect reality. A more concrete example is a language model like GPT-2/3 which will occasionally output hate speech, as it was encountered during training. Hate speech objectively exists in the world, but is frowned upon by many people. Having a customer service chatbot produce such language would likely be considered troublesome by most companies.


I’m not stating that your concern doesn’t exist; I’m merely making the observation that it is indeed more complicated than you imply.",/r/MachineLearning/comments/hnx1jn/r_what_are_your_hot_takes_on_the_direction_of_ml/fxg6u40/
1594156089.0,07-Jul-2020 14:08:09,dojoteef,MachineLearning,"Again, I appreciate your engagement on this topic!

> Opt into a discussion that has such colourful language as “pathetic”... I honestly don’t know how to respond to that. What is toxic? The word or the concept?

Considering the [dictionary definition](https://www.merriam-webster.com/dictionary/pathetic) that most appropriately fits your statement is:

> : pitifully inferior or inadequate
>
> // the restaurant's *pathetic* service

It's hard to think you meant that in a constructive manner. It's clearly a derisive statement, regardless if you think it is apt. As a former engineering manager, before starting my PhD, I can say for sure, that if someone spoke in that manner in a professional setting they would be spoken to about it. I've had to mediate my share of angry disputes because software engineers bandied about thoughtless remarks amongst each other, often times with less charged wording which still escalated.

> In fact I am quite offended that posts such as mine are being conflated with legitimately abusive and harassing ones

Having such thoughts doesn't mean you are a bad person, and the comment certainly doesn't rise to the same level as hurling a racial slur. The question is, whether this is the type of venue to speak in that manner. This sub began as a way to *foster* discussion of machine learning. A more professional tone is a reasonable ask. For example, moderation was asked to address all the beginner focused posts: hence the [Simple Questions Thread](https://www.reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwzzt9z/) and the required tags on posts.

> May I ask what your thoughts are given that the consensus in this thread seems to be that this sort of moderation is not wanted?

* The topic currently has 69% upvotes and my [informal assessment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx3tf1y/) indicated 61% of commenters see *some* form of toxicity (there were around 150 comments at the time; 13 people saw *some*, 8 saw none; my methodology required people to state an explicit stance, thus invalidating comments like [this one](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwrul1l/), which does not explicitly state a stance, though some might assume it implies one).
* ""this subreddit is not a democracy"" (to quote one of the mods I've spoken with). This is also in line with [the following essay u\/Snoo-6492 referenced](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtx8t7/). I pulled out the relevant quotes [here](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvhvue/) ""the core group has rights that trump individual rights"".

> unduly burden the moderation team

The mods decided to sticky this thread, so they are aware that this is an important topic. Additionally, change never happens if you assume it is too difficult to try, before even considering if there are ways to reduce the burden. For example, I came up with an [approach](https://www.reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwzzt9z/) that makes use of the AutoModerator. That's one potential way to reduce the burden. Maybe there are additional ones.


> Where do we go from here?

If you want to help, try to think about ways that will still allow you to speak in the manner that you want, while not putting the onus on others to [simply grow a thicker skin](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsbyoc/) as others have suggested. For example, another random idea is to keep flippant remarks behind a spoiler tag. Then people can decide to engage with the material if they want. Not saying this is a great option, but it's another example of trying to figure out ways to make it possible to address concerns across the moderation spectrum.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx8v30i/
1594077629.0,06-Jul-2020 16:20:29,dojoteef,MachineLearning,"Why are you getting angry? I've made sure to keep a civil tone and only stick to facts.

> You literally called using this language (""designed to"") toxic elsewhere in this thread

I do not call out any specific instance of toxicity in that comment, and certainly not about the words ""designed to"". I said your words could lead to frustration on the part of a reader. Had I called out the words ""designed to"" as toxic, then I agree that would be a pretty ridiculous definition of toxicity.

> > I've provided ample evidence of people disengaging with the sub
>
> **Where do I make an argument in this sub-thread about people disengaging with this sub?**

You don't. I'm saying this is a consequence of toxicity. Please re-read my comments if you need to see where I stated this.

> > You're claiming additional moderation will stifle conversation, so we should keep things the same
>
> **Where in this sub-thread do I make that argument? Nowhere.**

Maybe I conflated your concern with moderation with that of others in this thread. What consequence of moderation are you concerned about, i.e. why argue against additional moderation?

> I really, really don't understand.

Hopefully I cleared things up.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx5fr88/
1594070720.0,06-Jul-2020 14:25:20,dojoteef,MachineLearning,"By stating:

> There are many problems with this statement (and reminiscent of Lisa's anti-tiger rock)

You are behaving in a way that you are deriding me for:

> Perhaps this is a reaction to having an opportunity to give what you believe is a zinger? This is terribly immature, if so (""uncivil"", if you prefer),

Your statement implies I'm like Homer Simpson: incapable of forming a cogent thought. This is a (not so veiled) insult and an ad hominem attack, designed to derail the conversation.

&nbsp;

&nbsp;

As for the actual discussion:

> >Now for your side. Can you point to people who have disengaged from the sub, because they felt too heavily moderated?
>
> This is incredibly off-topic.

This is very on-topic. I've provided ample evidence of people disengaging with the sub (many from Twitter). You're claiming additional moderation will stifle conversation, so we should keep things the same. I'm merely asking for you to provide evidence of potential harm, like you asked of me.

Note, I pointed out several instances in my OP of moderation being stepped up in the past, so clearly moderation has increased before, so some people might have disengaged from the community as a result. Please back up your claim that moderation has stifled legitimate conversation, by providing links, like the Twitter threads I linked.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx52c82/
1594057804.0,06-Jul-2020 10:50:04,dojoteef,MachineLearning,"> Do you have data to back that up?

This seems to be a common refrain from you in this discussion, so I'll humor you. Multiple comments in this thread have talked about people disengaging with this sub due to toxicity/incivility. Many of these have already been linked, but I'll collate some of them here:

* https://twitter.com/smerity/status/1030570635318054912
* https://twitter.com/fchollet/status/1030532125777264640
* https://twitter.com/timnitGebru/status/960280569253978113
* https://twitter.com/jeremyphoward/status/958871408402038784
* https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fws2tz6/
* https://reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwnkjsn/
* https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvd0z8/ (this one is talking about people I know IRL who do not venture into the sub)

You might disagree with what is considered toxic, but it's clear that something is having an effect on these researchers (and they've termed it toxicity).

Now for your side. Can you point to people who have disengaged from the sub, because they felt too heavily moderated?",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx4c6pq/
1594055460.0,06-Jul-2020 10:11:00,dojoteef,MachineLearning,"I don't disagree that it's a tough problem. I guess I was just pointing out research I was aware of. Here's another research paper from ACL 2020 related to [detecting toxicity](https://www.aclweb.org/anthology/2020.acl-main.396.pdf).

If used blindly, a system like this could have a detrimental effect on communication. On the other hand, if it merely marks a posts that need to be approved by mods, then this might reduce the burden. I think I remember seeing u/programmerChilli mentioning this somewhere. I think this is the [comment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx2vkk4/), though I could have sworn he mentioned the use of ML...",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx479uq/
1594049656.0,06-Jul-2020 08:34:16,dojoteef,MachineLearning,"Thanks for your respectful response!!!

I'll quote myself [from here](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx3tf1y/) (I just wrote these comments):

> discussing approaches to reduce toxicity is what I was really after, rather than merely debating about the existence of toxicity

For example, at the end of this [comment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwzzt9z/) I make a suggestion of tagging top-level comments such that people can opt-in to the type of discourse they are comfortable with. Maybe there is a better approach. If there is, that's the type of discussion I was hoping to have.

So in the case of your removed comments (which I did not report), there might have been an alternative to removing your comment: either start your own top-level comment with an appropriate tag referencing the thread you want to comment on, or keep the entirety of the comment civil, e.g. not using a loaded term like pathetic. Again, I would certainly welcome additional ideas on ways to address the concerns of undue censorship, while still respecting the fact that not everyone is comfortable with the level of discourse on this sub.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx3vn5o/
1594048497.0,06-Jul-2020 08:14:57,dojoteef,MachineLearning,"Thanks again for taking the time to express your opinions. I will say, due to time constraints, this is likely the last comment I'll make in this particular conversation. Feel free to write a follow up with any responses and I will read them.


For the example you provided, I certainly agree that the comment is laden with condescension and ad hominem attacks. My contention is that even though the two individuals might have (seemingly) completed their conversation despite this, everyone has a different tolerance for how much incivility they are willing to endure to be part of the conversation. I certainly cannot provide numbers, but examples [like this](https://twitter.com/smerity/status/1030570635318054912) take a toll on who is willing to take part in the conversation. I can't help be correlate Smerity's post with the timeline that led [u\/olBaa to disengage](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fws2tz6/) with the sub on account of toxicity. As I alluded to before, I'm also aware of people IRL who avoid the sub.

> I don't have a problem with the idea of ""tagging"" top-level comments; I'm just afraid this may bring additional complexity and work for moderators.

I verified that the AutoModerator supports the ability to do this, so the moderation would only require the community to help the mods by reporting comments that break these rules.

> thanks for taking the time to clarify your position and find at least one illustrative example.

Lastly, I just want to state that this example is in the first edit I made to my OP, which predates this entire thread [starting here](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsy5st/). That's part of the issue with [comments like this](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvjw3q/), which ask for specific examples, but don't even bother to look at what I've provided.


> I'm not denying that they could exist, but if there are not explicit examples provided, it is extremely difficult to articulate (or justify) a solution.

Again, discussing approaches to reduce toxicity is what I was really after, rather than merely debating about the existence of toxicity. Considering the number of people who have claimed there is at least some toxicity (a quick look over the thread I counted 8 people claiming no toxicity whatsoever, and 13 claiming there is at least some toxicity, including you who stated ""I can see why one would consider it toxic""). This makes it me believe it's unlikely that people claiming there is no toxicity truly believe this, only that they don't see an issue with the *level* of toxicity (possibly in comparison to a different baseline, like Twitter, or other subs).

Thanks again for being respectful and I'm happy to hear that you don't find my suggestion onerous. If you have additional suggestions for how to improve the level of discourse, I would love to hear it.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx3tf1y/
1594007797.0,05-Jul-2020 20:56:37,dojoteef,MachineLearning,"I doubt this approach is ready for primetime, but there is a paper out in ACL 2020 that does [politeness transfer](https://www.aclweb.org/anthology/2020.acl-main.169.pdf) ""which involves converting non-polite sentences to polite sentences while preserving the meaning"".",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fx2erfl/
1593961264.0,05-Jul-2020 08:01:04,dojoteef,MachineLearning,"That's a great question! Maybe transfer learning could work, but I certainly didn't think through the idea too carefully. It's possible the approach highlighted in the paper is heavily dependent upon a 1:1 correspondence of high and low level images.",/r/MachineLearning/comments/hler8n/nd_facebook_research_releases_neural/fx04j6x/
1593958497.0,05-Jul-2020 07:14:57,dojoteef,MachineLearning,"First, I want to state that I'm very sorry to u/farmingvillein. I want to state that I take full responsibility for things getting off-topic and acrimonious.


This will be a bit of a long post, but I really hope you bear with me and read it all the way through. Let me give some background first, before answering your questions:

From my perspective I have a limited amount of time, and cannot possibly reply to each comment line by line. Commenting on this thread has already taken a tremendous amount of my time. Despite the time required to answer you, I think it's worth expounding more clearly on the situation, since you seem to be making a good faith effort.


When u/farmingvillein engages me [here](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtpbgv/) I believe it begins a clear misunderstanding between us. From that point on we begin to talk past each other, which I will detail:


* u/farmingvillein is responding to this [comment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwslwhx/), where I was *only* reasoning about why u/StellaAthena wrote naked comments, without links, not about whether that is a good or bad approach to moderating r/MachineLearning. It seems u/farmingvillein is speaking *only* about the removal of troublesome comments, but seems to imply the system of moderation is working as intended, thus in my mind opening up a *more general* question of moderation.
* I then proceed to articulate a more [general answer](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwu4eq6/).
* u/farmingvillein's subsequent [response](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwu7g9b/) begins with: ""I still don't 100% follow."" then goes on to discuss the specific case of moderating the removed comments.
* After reading the start of that comment, my first response is to reassess and think that clearly we have a problem of talking past each other. So I say I'm sorry that things are hard to follow, and try to take responsibility for it being derailed by stating: ""I'll admit I may not have done a great job of organizing this post overall."" Afterwhich, I try to engage with the general question of moderation again, and even give a positive response to one of u/farmingvillein's suggestions: ""This seems like an interesting idea.""
* The [next comment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvjw3q/) is when things seem to get acrimonious in my mind (though admittedly, u/farmingvillein would likely refer to my previous comment as setting up a confrontation). Specifically, this remark ""A good anti-toxicity rule is to not assume that your reader is impaired."" seems to come out of no where, as I cannot see anywhere where I am implying this fact. Thus I begin to assume I must be getting gaslighted.
* My [next response](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvlbw9/) is *most definitely* defensive.
* Needless to say, things degenerate from this point on...


Normally, I would not have responded to u/farmingvillein's comment that appeared to be gaslighting me, but considering this is a post I started, I thought it was important to comment.


Here are some additional points that I think are very important for this overall discussion:

* I understand people have differences of opinions on ways to address what I termed toxicity (and whether it should be addressed at all). I was hoping this discussion would focus on ways to improve the situation, not dissect individual examples of what went wrong where (though I do that here to address your concerns). Below I will give an example of what I was hoping the discussion on this topic would lead to, and would love to hear feedback on it.
* I understand that the more frequently you check Reddit, the more likely you are to run into a comment that has hate or toxicity, but has not been removed yet, via moderation.
* My main concern is toxicity that is not being removed (and potentially to figure out ways to remove problematic content faster)
* For example, I brought up [this example](https://www.reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwmm640/), as it highlights a type of comment that current moderation misses in my view. Yes, many people liked that comment, but I do believe it is problematic, because it is very visible and can be seen to imply a disregard for others.


&nbsp;

That said, this discussion has made me think of a potential way to address the issues I've seen, while still allowing an open discussion platform on this sub:

Maybe everyone can tag **top-level** comments, similar to the [D], [R], and [P] for posts, so people can engage only in the types of commentary they care about. All replies below that top-level comment would stick to the type of replies that user indicated by the tag on their comment. Here's a possible breakdown:

* [S] indicates serious research-like commentary that will be held to a high standard of quality. Should stay closely on topic and **only** discuss the merits of ideas and **assume** others engaging are doing so in good faith. No snarky comments, character attacks, etc. Would be subject to a higher bar of moderation.
* [A] indicates a digression or an aside of some sort. This can include snarky comments if people like, though would be subject to current forms of moderation.

Here are some strengths of this idea:

* Tries to keep discussion open. Any post can have open discussion as it currently exists (though blatant hate, character attacks, etc is removed).
* Segregates content at the top-level such that people can engage with the type of commentary they care about more easily, while avoiding potentially off-putting behavior.
* Has a low additional overhead for people commenting (only requires a small tag for top-level comments).
* Provides a clear guideline for moderation.
* Not entirely certain how auto moderation works, but maybe the AutoModerator can ensure every top-level comment is appropriately tagged?

Thanks for taking the time to read this super long post. What do you think of the moderation idea?",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwzzt9z/
1593950020.0,05-Jul-2020 04:53:40,dojoteef,MachineLearning,"See this similar comment you made that [I responded to](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvlm4h/). In that comment I call out a similar example that the mods addressed from both sides. The mods should take out the portion of the thread that went off topic and should notify people why this did so, e.g. [removing these 17 comments]. It doesn't matter ""which side"" the person is on. I've tried to state that in multiple places when responding to people (like the comment I linked), including my OP.

> This issue exists on both sides of these conversations, with both sides feeling put upon to some degree.

Note, I haven't been trying to respond to every single instance of incivility. I hardly have time to give as many responses as I have so far on this thread. I just found the rest of u/Ikkath's comment to be a great example of the issue, because outside the quote I highlighted, the user had brought up a reasonable counterpoint.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwzoow4/
1593920950.0,04-Jul-2020 20:49:10,dojoteef,MachineLearning,"I'd be excited to see this applied to some of the early generation of 3D games, especially ones that used voxel or raycasting engines like Delta Force and Outcast. I know Outcast got a [remake](https://youtu.be/jgmMvc0LoeE), but imagine doing this automatically!

If anyone who works at GOG reads this sub, they should jump on this!",/r/MachineLearning/comments/hler8n/nd_facebook_research_releases_neural/fwyu6m3/
1593881410.0,04-Jul-2020 09:50:10,dojoteef,MachineLearning,"I want them enforced for everyone, *myself included* when I step over the line. Yes there is an amount of subjectivity on the part of mods, which the r\/AskHistorians [round table addresses](https://www.reddit.com/r/AskHistorians/comments/4ijkk9/rules_roundtable_10_civility_and_debating_with/) (see the sections titled: ""Wait, so how do you decide if someone is being uncivil?"" and ""OK fine, but how do I argue with people if I can't call them a poopy head?"").",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwx1a3s/
1593881075.0,04-Jul-2020 09:44:35,dojoteef,MachineLearning,"So my reason for bringing this topic up, isn't identity politics, though I do bring up a number of examples of issues around that on this sub. For example, in my lab's NLP reading group, male faculty members and some students have also stated a negative perception of the sub. I mainly brought up the women and people of color, because they seem disproportionately effected.

I'll be honest, I don't engage with Twitter either. In fact the main way I interact with Twitter, is through links from this sub! I agree that Twitter really does have an issue, where people who seemingly know better, engage in behavior that is quite puerile, though I have occasionally seen individuals apologize after the fact.

> How many of your sample actually go on ml reddit and find it toxic independently vs are expecting to find it because they are told it is toxic by people with potentially other reasons for claiming toxicity?

I really don't know. I doubt anyone does, and I think it's exceedingly difficult to collect that data in an objective manner (Who exactly do you poll? Are they a representative sample of who might engage with this sub?). That's why I am inclined to raise the bar of civility, and apply it equally to any and all groups. I think if mods call out exactly what the offending behavior is, [like this](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsbyoc/) or [this](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwws01c/), people will begin to recognize what is acceptable and what crosses the line. They can certainly remove the offending snark and resubmit their comments.


I guess I don't see how asking for that negatively effects the ability to have an open discussion. If anything it seems more inclined to make discussions fruitful (like I think our discussion has been). I really thank you for taking the time to share you well thought out opinions.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwx0n98/
1593878690.0,04-Jul-2020 09:04:50,dojoteef,MachineLearning,"> /r/ML benefits from having open discussions, even at the cost of trash posts. Strong moderation is extremely difficult to get right without stifling debate, even with the best of intentions.

I agree it's important not to stifle debate, but here is [an example](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwws01c/) from this current thread of behavior that I think the sub should tamp down on. That certainly isn't the only example, see [here](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsbdgn/) and [here](https://old.reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsbyoc/) (note this is referring to both individuals).

The main problem I see with the logic that you and others have stated about not wanting to stifle debates, is that you are assuming the debates truly are open. If people are sitting out of the debate due to toxicity, then we aren't actually getting a diversity of opinion. Of course, you can state that moderation also stifles debate. My point is that by setting a few additional ground rules, similar in spirit to the [Civility Prime Directive](https://www.reddit.com/r/AskHistorians/comments/4ijkk9/rules_roundtable_10_civility_and_debating_with/), the sub can go a long way to keep discussions on topic and welcoming. People can certainly edit their comment to remove the offense called out by the mods and resubmit. What are you thoughts?",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwww5m7/
1593876401.0,04-Jul-2020 08:26:41,dojoteef,MachineLearning,"> It’s really pathetic and betrays what the real agenda is here.

This is a prime example of my point. I see lots of great comments, like the rest of your statement, but with the addition of a barb or snarky remark that seems intended only to incite. This is one of the most common forms of incivility that is not currently addressed by the rules, and where including a notion of civility in the moderation would be helpful.


By including an insightful comment in addition to the barb, the comment frequently gets upvoted, validating the entire comment, despite the inclusion of vitriol.


Ultimately, I think this is where many people who have weighed in on the OP saying: what do you expect from pseudonymous discourse on the internet? This is often followed up with: this is better than the vast majority of Reddit subs, and we would hate to stifle discussion.


My opinion is that if we moderated these discussions such that individuals do not include such incivility in their posts, the community would be more welcoming to discussion. Maybe it doesn't have to be full on r/AskHistorians level of moderation (I doubt anyone has the time for that), but I think it goes a long way to improving discourse on this sub, hopefully leading to a more welcoming environment.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwws01c/
1593874808.0,04-Jul-2020 08:00:08,dojoteef,MachineLearning,"I understand your frustration. My goal was to [validate](https://www.psychologytoday.com/us/blog/the-squeaky-wheel/201106/the-antidote-anger-and-frustration) what you were saying. My wife is a psychologist, and it's a point she has tried to hammer home to me a ton. In this instance I did not do a good job of validating your position. I'm sorry.


When you categorically state things like:
> No. Your post is classically toxic and gas-lighting behavior which is designed to shut-down discussion.


This statement attributes an intent to my actions (which in my mind I did not intend). This can lead to frustration. Consider rephrasing these types of statements in a less accusatory manner. I might be guilty of this behavior as well. I will try my best to catch myself (and please point out if I don't).


So, how can we move forward to speaking about the issue of toxicity on this sub? Ultimately, that is what I care most to discuss. If I can distill your perspective, it's that you want clear quantifiable facts: 1) what constitutes toxicity, and 2) the effect of toxicity as defined in #1. Is that correct?",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwwp79k/
1593843099.0,03-Jul-2020 23:11:39,dojoteef,MachineLearning,"Thanks for that well reasoned perspective! I especially enjoyed your ""too long; definitely read"" ;-)


As for expecting too much compared to other subs on Reddit, that's why I specifically call out r/AskHistorians, because they seem to have a much more civil discourse. Here's [a thread](https://reddit.com/r/AskHistorians/comments/hka5nf/the_founding_narrative_of_the_united_states_gives/) from that sub that's super highly upvoted (it would be the #3 upvoted post of all time in r/MachineLearning). While there are many removed comments, the remaining are truly having a discussion about the topic at hand, rather than digressions, attacks, or what have you. So it is possible to improve the quality of discussion on this sub. Maybe it doesn't have to be at the same level of r/AskHistorians, but improving what we have seems valuable.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvp6iu/
1593840052.0,03-Jul-2020 22:20:52,dojoteef,MachineLearning,I don't disagree with you. In fact one of the mods calls [both individuals out](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsbyoc/) asking for more civility.,/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvlm4h/
1593839820.0,03-Jul-2020 22:17:00,dojoteef,MachineLearning,"> > I'm sorry that you are having trouble following my point.
>
>
> I am very aware of your claim: that there is toxicity. (By the way, this line here feels very toxic and dismissive to me, and I would suggest that your post be banned under the examples you have proffered so far. Can we discuss and perhaps escalate this? A good anti-toxicity rule is to not assume that your reader is impaired.)

Hold up. I am in no way trying to be toxic here. Literally my next sentence is:
> I'll admit I may not have done a great job of organizing this post overall.

It feels like you are leading the discussion down a rabbit hole. My only guess is that you're intending to set this up as an example of ambiguity of language, and why the examples I've proffered can be seen as both toxic and non-toxic based on perspective. I'm sorry, but if that isn't your intention, then I really do not understand this interaction at all.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvlbw9/
1593837151.0,03-Jul-2020 21:32:31,dojoteef,MachineLearning,"> (The ""via consensus"" qualifier here is very important--there are plenty of examples where small groups of people push through great things (startups, sciences), and basically tell other people to execute their vision. That's not the scenario we have here, however--we have a community that, ostensibly, intends to keep being run as a community, i.e., with at least some level of discussion and agreement around norms and operations.)

This [excellent essay](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtx8t7/) provides interesting commentary on your notion that the entire community should weigh in.

> The second thing you have to accept: members are different from users. A pattern will arise in which there is some group of users that cares more than average about the integrity and success of the group as a whole. And that becomes your core group, Art Kleiner’s phrase for “the group within the group that matters most.”

> The third thing you need to accept: the core group has rights that trump individual rights in some situations. This pulls against the libertarian view that’s quite common on the network, and it absolutely pulls against the one-person/one-vote notion. But you can see examples of how bad an idea voting is when citizenship is the same as ability to log in.

Consider reading the sections: ""Three Things to Accept"" and ""Four Things to Design For"" in that essay for recommendations on social media design, though the entire essay is interesting. It seems many aspects of Reddit mirror the points brought up in that essay.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvhvue/
1593835501.0,03-Jul-2020 21:05:01,dojoteef,MachineLearning,"I'm sorry that you are having trouble following my point. I'll admit I may not have done a great job of organizing this post overall. My purpose hasn't been to dissect individual examples of toxicity, but to look at them in aggregate.

That said, I think my basic tenet is unchanged. I'm talking about **any and all forms of toxicity** (subtle or not). That's why I linked the r/AskHistorians [Civility Prime Directive](https://www.reddit.com/r/AskHistorians/comments/4ijkk9/rules_roundtable_10_civility_and_debating_with/).

> The main other fix I could imagine is requiring some additional threshold (karma or similar) to participate in this forum. Which would obviously have some costs, but would be a reasonable item for discussion.

This seems like an interesting idea. On the one hand this [great essay](https://old.reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtx8t7/) states:
> Second, you have to design a way for there to be members in good standing, some way in which good works get recognized. The minimal way is, posts appear with identity. You can do more sophisticated things like having formal karma or listing “member since” dates or noting who is a Pro user who helps fund the system.

I'm a little concerned about being exclusionary, especially for people who don't use reddit much, but decide they want to engage with the r/MachineLearning sub (that was certainly my path to this sub a few years ago).",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvfobw/
1593833617.0,03-Jul-2020 20:33:37,dojoteef,MachineLearning,"Thanks for your comments! I equally hope that you take my response as an earnest attempt to engage.

> there a plenty of women, poc, lgtbq people who have dark, warped, robust, or unconvential sense of humour, and many more would not find anything wrong in playful sarcasm or even jokes based on stereotype or race providing it was not unduly placed on one group

Sure these people exist, but I highly doubt they make up the majority of women, people of color, or LGBTQ people.

> even if people consider it not great that some people act this way, it's not clear that presents any undue obstacle to them as a group actively participating or joining in the community

We certainly disagree on this point. I share an office space with several female CS PhD students. I also have a number of female CS PhD students who are friends (some of whom are people of color). Anyone one of them I've talked to about this sub shies away from it due to the perception of toxicity (admittedly I haven't broached the subject with them all). Sure this is anecdotal, but considering the number of people I've talked to about this, I cannot help but feel the issue is more pronounced than you make it out to be. This conversation makes me want to see if there's someway I can get a survey of ML practitioners at both [GHC](https://ghc.anitab.org) and [Tapia](http://tapiaconference.org) to see the percentage of people who avoid this sub due to perceived toxicity.

> The implication in your post is that it's ""more acceptable"" to be left alone to be mobbed if you are not a minority, because you experience less day to day obstruction and/or disempowerment and so have more ""energy"" to cope.

I'm sorry if you got that impression. I don't want anyone to get mobbed, because I want discourse to be civil. Discuss the merits of arguments. Often times short snarky comments lose that nuance. I'm only saying that on average it tends to effect marginalized groups more (certainly at an individual level that could be different).",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvd0z8/
1593832424.0,03-Jul-2020 20:13:44,dojoteef,MachineLearning,"Then I don't think we are really in disagreement. I **don't** want to silence any side. Absolutely not. I want conversations to be civil. People should be able to discuss the merits of topics, even playing devil's advocate if they like. I'm only stating that when people engage in snarky behavior, it tends to lead discussions in a bad direction. I can only assume that somehow my asking for discourse on this sub to be more civil seems to have been twisted in many comments to assume I mean to silence legitimate discourse, which truly is the farthest thing from my mind.


According to this sub, [Twitter's short tweets](https://old.reddit.com/r/MachineLearning/comments/hhonq4/d_yann_lecuns_last_substantial_post_on_twitter/fwc34kh/) cause people to write inflammatory comments that will get the most retweets. Supposedly this sub is an improvement because it allows [long form commentary](https://reddit.com/r/MachineLearning/comments/hiv3vf/d_the_machine_learning_community_has_a_toxicity/fwil2rf/), yet somehow snarky tweet length comments still have a tendency (in my view) to cause the same issues here on this sub that people complain about on Twitter, hence the title of this post.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwvbas9/
1593816007.0,03-Jul-2020 15:40:07,dojoteef,MachineLearning,"Thanks for sharing! I read the essay in its entirety. It seems to be advocating for ways to provide the “core members” of a community to control the general direction of online groups, especially as those communities grow in size. It seems similar what I’ve stated: consider setting up rules similar to r/AskHistorians. But since it seems you aren’t super excited about the idea, I’m wondering your take on the essay. Maybe you have a different perspective.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwulkag/
1593806386.0,03-Jul-2020 12:59:46,dojoteef,MachineLearning,I think my [comment here](https://old.reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwu444a/) articulates why I believe the current “system” isn’t working as well as it could be. So I am looking at was to improve.,/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwu4eq6/
1593806222.0,03-Jul-2020 12:57:02,dojoteef,MachineLearning,"Thanks for engaging with an insightful response! I guess we can disagree about the false dichotomy. I agree you are both correct that allowing people to engage with potentially disrespectful comments, allows people to address them. Rather, my concern is that it is left to the person being attacked to “defend” themselves at times. For those who are constantly marginalized in our field, that really does have an impact. I would ask you to speak with those who identify as women, African Americans, Latinx etc and ask their opinion on the topic. Talk to not just a single person, but multiple people. See if you don’t recognize a troubling pattern in ML and whether seeing an interaction, like the one I referenced, does make them hesitate to engage on this sub.


My feeling is that it takes a little extra effort to think about the repercussions of what you write, but the resultant inclusivity is well worth it.

Thanks again for being willing to listen!",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwu444a/
1593797664.0,03-Jul-2020 10:34:24,dojoteef,MachineLearning,"> Tone policing is not a means of ""including women and people of colour in the conversation"". Reddit is pseudonymous, and most of the time you have no idea who are you talking to.

Okay, I'm really trying to understand your side of the discussion. Are you stating, because you cannot verify someone's identity, as Reddit uses pseudonyms, you are making the assumption that they might be trolling in some way?

> Downvotes are not ""damage"", no matter how hard do some of us wish that everyone in our society was a narcissist.

I understand that it you might not view it as damage, but it seems like when part of a comment is about an individual's identity, and they get downvoted, part of that downvoting can be perceived on an attack to their identity?

> Don't accuse them of not reading your comment **immediately after they quote your comment.**

I'm legitimately sorry for the confusion. It seemed like you read my comment, but not the linked thread, because you didn't mention that in particular. Rather you seemed to digress and imply I had a black and white view of the world. I can tell you I most certainly do not, and feel like nearly everything (even some foundations of Mathematics) are indeed in a massive grey area.

Again, I really am trying to understand your views. I do not want this to be an acrimonious disagreement. I'm actually trying to better understand your side of the argument, so I'm sorry if that isn't coming through. But without sometimes it's really hard to understand a person's view with terse remarks that appear to digress into opinions of what they are implying. If I am implying something, it is not intentional.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwto1vb/
1593797046.0,03-Jul-2020 10:24:06,dojoteef,MachineLearning,"> The predictable (and upvoted) snarky comments are necessary pushback against Tiny Images being taken down

Couldn't that snarky comment just as well have been this reasoned argument (the next part of your sentence):
> to prevent researchers from going further, and taking down other borderline datasets, like large NLP corpuses.

That's merely what I'm asking for.

> The public view of the community then follows a self-reinforcing loop that goes nowhere good.
I'm saying there already is a self-reinforcing loop: by allowing snarky comments that do not truly state their intent, but rather leave it to the reader to infer intent, leads to bad faith arguments.


A (possibly contrived) ML perspective: assume everyone comes in with their own prior on the toxicity of r/MachineLearning. Those who encounter less direct threat to their ideas or even to their very existence over their lifetime, assume lower overall toxicity. While those who encounter some amount of this threat on a daily basis come in with a much lower threshold for claiming toxicity. I'd say most people who are currently in the machine learning community have less of this threat, thus might not see such minor issues as powerfully suppressive, unlike those who systematically encounter these issues throughout their daily lives. When then finally do speak up, they are immediately shot down and feel a lack of support like in this [thread](https://reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwnkjsn/).",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtmu2d/
1593795106.0,03-Jul-2020 09:51:46,dojoteef,MachineLearning,"I am actively trying to engage you. Did you read my linked comment? That doesn’t provide a false dichotomy. A black woman specifically gets downvoted and attacked on a thread. I can’t provide quotes (like u/StellaAthena), because the comments are removed, though the damage is already done and the downvotes still remain. Are you intentionally just trying to shutdown valid discussion?",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtj2d8/
1593794071.0,03-Jul-2020 09:34:31,dojoteef,MachineLearning,Care to share the reasons behind your opinion? Here’s a [comment](https://reddit.com/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtd9hm/) that sums up my reasoning.,/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwth31v/
1593792078.0,03-Jul-2020 09:01:18,dojoteef,MachineLearning,"Therein lies the rub when it comes to low effort snarky comments. You assume that I should give the poster some credit, but why? I pointed out just one aspect of why that comment was bad (a point we both seem to agree about), but in that same thread [other comments](https://reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwnkjsn/) had to be removed for blatant sexism/racism because an individual from one of the marginalized groups decided to speak up. They and those who try to defend them got severely downvoted.


Sure you can try to make the case that the original snarky comment was meant in jest and didn't directly effect u/goblix, but it certainly sets up an environment that encourages such terrible behavior. I admit, it really sucks not being able to have off the cuff remarks that some (such as yourself) may deem harmless. But if the choice is between that, and including women and people of color into the conversation, then I say we moderate our tone.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwtd9hm/
1593790268.0,03-Jul-2020 08:31:08,dojoteef,MachineLearning,"*Responding to your EDIT* I specifically call out a couple of top rated comments:

> The [top rated comment](https://reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwmm640/) discussing the news that the Tiny Images dataset has been pulled as it contains verifiably pornographic images ""shot in a non-consensual setting (up-skirt)"", sarcastically makes light of this fact, with another highly rated response calling it a [""Bit of a silly moral panic IMO""](https://reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/fwmqnab/).

**NOTE**: I'm pretty sure that my edit to the post existed before you made an edit to your comment, but if not, then I apologize and certainly do not mean to somehow change the history of the discourse. Please let me know if that is the case, and I'll modify this comment.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwt9v4z/
1593789750.0,03-Jul-2020 08:22:30,dojoteef,MachineLearning,"Here is one example of where we disagree:

> I support the removal of the dataset, but this ""not an acceptable response"" is a personal judgment made by you, and the rest of your statement incorrectly characterizes the comment

The post specifically mentions ""including non-consensual pornographic material and the ability to identify photo subjects through reverse image search engines"". Making snarky comments about someone's potential suffering being perpetuated by an academic dataset really does not seem acceptable.

> (does not sarcastically make light of that specific fact)
Without the text of the comment specifically calling out a particular aspect of the dataset, you are choosing to assume the comment somehow doesn't encompass all of the issues with the dataset (e.g. non-consensual pornographic material). That's very charitable of you. I don't have a way of knowing what the user's intention is, so I am unwilling to jump to such a conclusion, especially because if they had read the post they would clearly have seen this. Maybe you can assume that the Redditor didn't read the full post, but then why should we encourage comments (via upvotes) from users who aren't even taking to time to read and understand a post before commenting? That just seems like a way to derail legitimate conversations about difficult subjects.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwt8wix/
1593788153.0,03-Jul-2020 07:55:53,dojoteef,MachineLearning,"I completely agree that ""political toxicity"" is tricky. I do not believe mods should be endorsing one side or another in an argument. What I do suggest is to have Redditors make a good faith effort and try to understand the argument the other side is making and not respond with low effort responses that end up derailing valuable conversations.

That's part of the reason why [r/AskHistorians states](https://www.reddit.com/r/AskHistorians/comments/4ijkk9/rules_roundtable_10_civility_and_debating_with/):

> **Wait, so how do you decide if someone is being uncivil?**
>
> &nbsp;
>
> More than perhaps any of our other rules, moderating based on civility requires us to take a bit of a ""know it when we see it"" approach. We realize that our user base on AskHistorians is global, and that standards of what's considered ""bad language"" vary from country to country, and that language issues can cause people to seem rude without the intent of giving offense. We will also use at a poster's comment history to see whether they have shown a pattern of incivility using their account, to decide whether they fall on the side of ""possible misunderstanding"" or ""usually abrasive."" To be clear, this is not the only metric we use, but if the user history demonstrates a pattern of being abusive, we take that into account.
> That said, though, we tend to err on the side of removing content if we think it's not being posted in good faith or if we believe the intent is to mock another user.


These and many other points I bring up are described well in their wiki and the follow up ""Roundtable"" discussions, which is why I linked them and specifically as people to read those posts. They give reasoned arguments for their positions. They try not to be reactionary.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwt60ub/
1593783525.0,03-Jul-2020 06:38:45,dojoteef,MachineLearning,"My guess is that the mods ended up removing those comments, so they can't be directly linked to anymore, but doesn't mean it didn't already cause harm.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsy5st/
1593783426.0,03-Jul-2020 06:37:06,dojoteef,MachineLearning,"I certainly lament that fact, because not too long ago I was in the same position of transitioning into the discipline. I wholeheartedly agree that some people can be quite unkind to new folk. I think there has been some effort to move some of the conversations into r/learningmachinelearning which has (hopefully) helped by providing a more welcoming sub, but there are still those on this sub who respond poorly to legitimate questions.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsy03q/
1593783141.0,03-Jul-2020 06:32:21,dojoteef,MachineLearning,"> The drama is good to have too, it's important to know about ML ugly side too. I'd rather see it discussed than pulled under the rug.

I wholeheartedly disagree. If you want a sub about drama, then maybe a new r/MLDrama sub is more appropriate. Mixing both in this sub does a disservice to highlighting the science and engineering efforts that people who are interested in this topic are looking for.

One way to envision this sub, is that a post is similar to an oral presentation at a conference, with thread comments being questions/comments from the audience. If that were the case, the level of discourse would be much more respectful, because most of the Redditors would no longer hide behind a wall of anonymity which too often leads people to lead with invective, rather than actually taking a moment to think before commenting.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsxjlj/
1593782566.0,03-Jul-2020 06:22:46,dojoteef,MachineLearning,"Here's a [comment](https://reddit.com/r/MachineLearning/comments/7ukfr3/d_is_this_subreddit_too_harsh/dtl5oci/) from the thread, which tries to draw a distinction between ""blunt comments"" and ""toxicity"". I'm stating that often ""blunt comments"" *are* a form of toxicity, *especially* when you are attacking a person's work and possibly their motivations/character. The thread that actually began the dispute.

[""spewing BS about algorithm performance"" and ""At best, this is an exaggeration; at worst, it's a flat-out lie.""](https://www.reddit.com/r/MachineLearning/comments/7tuhof/p_practical_deep_learning_for_coders_2018/dtgeutm)

If you legitimately worked for a year on a project and someone made a claim likening your work to a lie, how do you expect to react? I know I would certainly feel attacked if the claims are posted anonymously on a large public forum and did not back up the claim with publicly available references.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwswn8b/
1593781519.0,03-Jul-2020 06:05:19,dojoteef,MachineLearning,Please read my edit for examples.,/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsv2gs/
1593781487.0,03-Jul-2020 06:04:47,dojoteef,MachineLearning,"I've edited the post to try to highlight the concerns you have. I had initially tried to avoid calling out specific people, which is why I shied away from having links to a bunch of examples. Maybe that was shortsighted.",/r/MachineLearning/comments/hkc697/d_rmachinelearning_not_just_twitter_has_a/fwsv0vd/
1593660471.0,01-Jul-2020 20:27:51,dojoteef,MachineLearning,"I really can't tell if you are intentionally trying to distort my words, or if for some reason I am not being clear, so I will give you the benefit of the doubt and try one more time. This is not about Yann's race. Both Obama and JayZ also enjoy power and privilege.


**emphasis added**
[Cambridge English Dictionary - privilege:](https://dictionary.cambridge.org/dictionary/english/privilege)
> an advantage that only one person or group of people has, usually **because of their position or because they are rich**

[Dictionary.com - privilege:](https://www.dictionary.com/browse/privilege)
> a right, immunity, or **benefit enjoyed** only by a person **beyond the advantages of most**:
the **privileges of the very rich**.


The benefit or advantage he holds that most do not, is that many hold his words in high esteem (rightfully so). I agree it certainly took much personal effort to attain his position. Really hope this clears things up.",/r/MachineLearning/comments/hhonq4/d_yann_lecuns_last_substantial_post_on_twitter/fwnpsl3/
1593528786.0,30-Jun-2020 07:53:06,dojoteef,MachineLearning,"I simply mean:

    Chief AI Scientist at Facebook + Turing Award Winner = Power + Privilege

In no way am I implying he isn’t deserving of this power and privilege, or that it was handed to him, or anything other contrivance. I’m simply stating that his stature and accomplishments clearly indicate he has more power and privilege than 99.99% of people working in ML. Hope that clears things up for you. Cheers.",/r/MachineLearning/comments/hhonq4/d_yann_lecuns_last_substantial_post_on_twitter/fwhap65/
1593528269.0,30-Jun-2020 07:44:29,dojoteef,MachineLearning,"A speedup of 50x over a vanilla Transformer on image generation is pretty huge! (The 4000x number is a more dubious number, as storing the internal state is standard practice for inference in Transformers). Heck I reported a 5x speedup for MT by making use of syntax, but that’s with a drop in BLEU (https://github.com/dojoteef/synst). I’d really like to see the performance on some standard MT datasets, because I’ve investigated using the RBF kernel to replace softmax attention in MT and achieved identical BLEU, though surprisingly got lower perplexity.",/r/MachineLearning/comments/higqq0/r_transformers_are_rnns_fast_autoregressive/fwh9oh0/
1593400684.0,28-Jun-2020 20:18:04,dojoteef,MachineLearning,"Maybe the people on this sub should take a moment and reflect on Yann’s request:

> I'd like everyone to please stop attacking @timnitGebru and everyone who has been critical of my posts.

He is a powerful researcher with plenty of privilege who can certainly take care of himself, so if he says he doesn’t need help attacking people who are critical of his posts, maybe listen to him.",/r/MachineLearning/comments/hhonq4/d_yann_lecuns_last_substantial_post_on_twitter/fwbytrt/
1593374686.0,28-Jun-2020 13:04:46,dojoteef,MachineLearning,"I understand your frustration, but speaking in absolutes and referring to an entire field as pseudoscience is a bit extreme and does not foster open communication. I list concrete reasons why simply providing source code and asking reviewers to fully vet the submitted code will not necessarily produce your desired outcome: higher quality scientific research.

&nbsp;

Here is an example that does not conform to the narrow view you have espoused: I submitted a paper to a conference (it’s currently in review), that I believe anyone would be hard pressed to replicate *before* we release the paper publicly, because to allow others to replicate it before likely *requires* breaking double-blind review. Otherwise there is no way to provide access to a unique resource we have without exposing who we are. Reviewers could easily run our code, but it took 3 months of running the code on our platform to gather the human feedback that demonstrates the power of our research idea, as our main result is based around evaluation via a large number of human experts (which is the resource we cannot provide without exposing who we are).

From your narrow criteria, it sounds like our paper shouldn’t be publishable and should be considered pseudoscience, even though after review it should be possible to replicate the result, since we no longer need to worry about the double-blind review requirement.

&nbsp;

I’d challenge you to think through the implications of your black and white mentality to see if it really fixes the issues you are concerned about. Again I agree that having more transparency and including code can be useful. I’m saying it will not realistically address all your frustrations. Remember that [perfect is the enemy of the good](https://en.m.wikipedia.org/wiki/Perfect_is_the_enemy_of_good).",/r/MachineLearning/comments/hguhu7/d_why_is_it_ok_for_academia_to_not_properly/fwap5ff/
1593367879.0,28-Jun-2020 11:11:19,dojoteef,MachineLearning,"Here’s a copy/paste of [my response](https://old.reddit.com/r/MachineLearning/comments/hguhu7/d_why_is_it_ok_for_academia_to_not_properly/fw9x59w/) to a similar suggestion from a different thread:

> While I wholeheartedly agree that it is preferable that code/data be made available, it is not a panacea for fixing the review process. Depending on the paper, the process to replicate the results may not be feasible (requires access to inordinate compute capability or specialized resources like expert annotators, etc). Even with code provided, it is no guarantee that there isn’t a bug which leads to the reported numbers, thus simply replicating isn’t enough. Rather a complete audit of the code might be necessary. This can also take an inordinate amount of time, and is still error prone. For example, there have been major security bugs that have been caught in open source code (like OpenDNS) in use by large populations, so even having multiple eyes on code is often not enough to catch potentially major bugs. Anecdotally, I’m aware of several cases of such bugs in ML research code (some that were found before publication and some that were found after).
> 
> &nbsp;
>
> tl;dr Could requiring code help some reviews? Sure. I’m just not convinced that code + toy model detects that many more cases of invalid results being published without significantly increasing reviewer burden.",/r/MachineLearning/comments/hguhu7/d_why_is_it_ok_for_academia_to_not_properly/fwacd2o/
1593359885.0,28-Jun-2020 08:58:05,dojoteef,MachineLearning,"While I wholeheartedly agree that it is preferable that code/data be made available, it is not a panacea for fixing the review process. Depending on the paper, the process to replicate the results may not be feasible (requires access to inordinate compute capability or specialized resources like expert annotators, etc). Even with code provided, it is no guarantee that there isn’t a bug which leads to the reported numbers, thus simply replicating isn’t enough. Rather a complete audit of the code might be necessary. This can also take an inordinate amount of time, and is still error prone. For example, there have been major security bugs that have been caught in open source code (like OpenDNS) in use by large populations, so even having multiple eyes on code is often not enough to catch potentially major bugs. Anecdotally, I’m aware of several cases of such bugs in ML research code (some that were found before publication and some that were found after).

&nbsp;

tl;dr Could requiring code help some reviews? Sure. I’m just not convinced that code + toy model detects that many more cases of invalid results being published without significantly increasing reviewer burden.",/r/MachineLearning/comments/hguhu7/d_why_is_it_ok_for_academia_to_not_properly/fw9x59w/
1593358136.0,28-Jun-2020 08:28:56,dojoteef,MachineLearning,I agree with all of these points!,/r/MachineLearning/comments/hhaioa/d_how_to_approach_publishing_a_machine_learning/fw9tzgv/
1593357811.0,28-Jun-2020 08:23:31,dojoteef,MachineLearning,"I think it’s important to be realistic. Publishing an ML paper is very hard, let alone when you do not know what reviewers are looking for. Before starting my PhD, I was a lead engineer helping to guide 25 engineers build an MMORTS at a video game company, and later spent time as an engineering manager at a Bay Area company. I moved up the ladder quickly because the people I worked with recognized my abilities. Despite my experience, publishing a paper in ML seemed elusive without guidance. By having an advisor, there are lots of little things that you learn that improve your chances of publishing, quite frankly because they are often both good science and help articulate your ideas in a way reviewers are more likely to accept.

&nbsp;

Is it possible to get your first publication without collaborating with someone? Probably. Though I imagine it is much more based on luck, e.g. relying on the randomness of the review process that has false positives (and false negatives). I just completed the second year of my PhD and feel considerably more confident in my ability to potentially get a publication on my own, than I would have two years ago (though I am sure I still have much to learn). 

&nbsp;

If you truly want a publication, then I would strongly consider collaborating with someone on your first publication. You can always publish alone later. If you are intent on publishing alone for your first paper, then I would suggest that you have great patience. That’s a virtue that all researchers need, but is even more important if you are relying on the randomness of the review process.

&nbsp;

FYI: Even a highly gifted individual like [Chris Olah](http://colah.github.io/posts/2020-05-University/), who skipped undergrad entirely and became a researcher in the field received guidance that enabled him to publish research (in fact it was with some of the top people in the field who were excited to work with such a young talented individual). While the blog post I linked to is related to whether it makes sense to skip undergrad entirely, I think you can analogously think of it as: should I consider a non-traditional path to a goal when I believe myself to exceptionally talented (while also noting very few people truly have such exceptional talent, and often they consider receiving guidance more quickly fills in gaps in their knowledge rather than figuring it out on their own)?

&nbsp;

I know this has been a long post, but I hope it gives you some points to consider when deciding if and how you might go about publishing your first ML paper on your own. Good luck!",/r/MachineLearning/comments/hhaioa/d_how_to_approach_publishing_a_machine_learning/fw9tezy/
1593354343.0,28-Jun-2020 07:25:43,dojoteef,MachineLearning,"I think it’s important to be self critical, such that you are able to discover your weaknesses and be willing to change them. My hope is that Professor De Freitas will learn from these issues and improve. One hopeful sign is the [following tweet](https://twitter.com/NandoDF/status/1277199544128847874?s=20) from the same thread where he states (emphasis added): 

> There’s also the insidious biases, which are the hardest to surmount, eg Professors who foster a culture of killing oneself on paper deadlines, to the point of not sleeping the night before. This favoured single males. **I’m guilty of this and I apologise to all my students.** 7/

Let’s hope he has internalized that his public reaction to the reviewers is one of those “insidious biases” that he has exhibited in the past.",/r/MachineLearning/comments/hhemnl/discussion_thread_about_prof_nando_de_freitas_at/fw9njt6/
1593270110.0,27-Jun-2020 08:01:50,dojoteef,MachineLearning,"Short answer: because reviewers are not rejecting papers that exclude these analyses.

&nbsp;

Long answer: Some papers actually do these types of analyses, though it is often heavily dependent upon the conference where the paper is submitted. Many of the general purpose conferences in ML, such as ICML, NeurIPS, and ICLR have less of a focus on these types of analyses. On the other hand, the NLP conferences tend to have more expectations in this regard, where papers more often include both quantitative and qualitative analysis, including focusing on common error cases and failure modes. For example, the original Transformer paper only looks at aggregate metrics (at least they did some ablations) and was submitted to NeurIPS. Many subsequent papers have been devoted to conducting the types of analyses that were missing from the original paper.

Another issue is paper length for conference submissions. People have a small amount of space to cover research that might have taken up to a year or more to complete. 8-10 pages  makes it difficult to simultaneously frame your problem, accurately describe your approach, clearly delineate how you went about testing this approach, report and analyze the results, while ensuring you adequately covered all the bases such that your work is reproducible. Often times, people make difficult decisions on what analyses to cut, because they do not think it will effect their reviews compared to a different aspect of the paper. Some papers try to include these additional analyses in an appendix, but reviewers are not required to read the appendix before assigning a score, so it isn't always beneficial to spend the time and effort.

&nbsp;

All that said, I do lament when I read a paper and I'm not convinced it has conducted enough analyses to make me confident that the paper actually demonstrates an improvement. This definitely happens more frequently than I'd like. Hopefully, these issues will slowly improve over time, as the exponential growth in papers (and subsequently reviewers) begins to taper off. For example, ACL 2020 has [a tutorial](https://www.aclweb.org/anthology/2020.acl-tutorials.4) on how to be a good reviewer to help combat these trends.",/r/MachineLearning/comments/hguhu7/d_why_is_it_ok_for_academia_to_not_properly/fw66p37/
1593231202.0,26-Jun-2020 21:13:22,dojoteef,MachineLearning,"I’m a former engineering manager in the Bay Area (before starting my PhD). If these are legitimate concerns, then you should try to discuss them with your manager and potentially HR.

&nbsp;

Creating a new account on Reddit to accuse a coworker of untoward behavior without at least trying to address the issue at your job looks like a [hatchet job](https://www.wordnik.com/words/hatchet%20job) to an outside observer. It would be different if you said you tried those avenues and failed. This is advice I would give *regardless* of who you are calling out.",/r/MachineLearning/comments/hgm0nh/d_a_google_insider_opinion_on_the_timnityann/fw4wwcd/
1593182564.0,26-Jun-2020 07:42:44,dojoteef,MachineLearning,"I think I understand why you believe Yann LeCun is the foremost expert in this area, but I think your logic is mistaken. While LeCun has a lifetime of experience, in many research and commercial systems, it isn’t focused on a narrow area of expertise addressing bias and fairness. In contrast Timnit Gebru’s focus is that narrow area. The ideas she has helped put forth, like [model cards](https://arxiv.org/abs/1810.03993) and [datasheets](https://arxiv.org/abs/1803.09010) for explicitly denoting potential biases and limitations of pretrained models and datasets, are being widely adopted. In fact, if you read the blog post /u/regalalgorithm linked, you’ll see the authors of PULSE even made a model card addressing the limitations of their approach, which was a wise decision.

&nbsp;

> Once race is introduced, people have a tendency to yell and scream and refuse to discuss anything calmly.

I empathize with your sentiment, but whether you intended it or not, when you use emphatic statements like “100% disagree”, you are potentially leading the discussion away from being calm and measured. While this might seem innocuous, emotion is harder to gauge in writing than in person, since you cannot literally hear a speaker’s tone in writing. I don’t intend this comment as a slight against you, since we are all guilty of this at times (myself included). I just thought it was helpful to point out. Cheers.",/r/MachineLearning/comments/hfz4y2/n_yann_lecun_apologizes_for_recent_communication/fw2eot5/
1593014209.0,24-Jun-2020 08:56:49,dojoteef,MachineLearning,"An example of algorithmic bias would be the [straight-through](https://arxiv.org/abs/1308.3432) estimator, which is a biased estimator. The expected bias introduced by the estimator is independent of the data fed to the estimator. This is different than having a biased dataset (one that does not reflect the true distribution you are trying to model), and is also different than our societal notion of bias (which is much more difficult to quantify as there is no agreed upon mathematical definition).",/r/MachineLearning/comments/heiyqq/dr_a_letter_urging_springer_nature_not_to_publish/fvv1uza/
1592800856.0,21-Jun-2020 21:40:56,dojoteef,MachineLearning,"The notion that bias only comes from data is incorrect. The problem is that ML models have a tendency to amplify existing bias in a dataset. If you want an example read the EMNLP 2017 best paper [Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints](https://www.aclweb.org/anthology/D17-1323/).

Here’s an example: assume you have a classifier that learns to classify cows. If 65% of your photos have cows standing in a field, and fields are in 70% of the photos, the model is likely to amplify the dataset bias by assuming any photo with a field has a cow in it. This is a well known phenomenon.",/r/MachineLearning/comments/hdi9hq/discussion_about_data_bias_vs_inductive_bias_in/fvlvdv0/
1592798400.0,21-Jun-2020 21:00:00,dojoteef,MachineLearning,"There have been countless race and gender related gaffs in ML research that are well publicized. Is it any wonder that NeurIPS is trying to get authors to consider broader impacts of their work? Soumith Chintala had a [tweet](https://twitter.com/soumithchintala/status/1274837416935919616?s=20) in this discussion that really nails why this is important for _researchers_ to consider, not just product focused teams:

> Today, ML Researchers are inadvertently powering product of a lot of non-AI companies who ignorantly start with a pre-trained BERT/ResNet/YOLO from the internet. Probably ignoring the License, Readme, Terms of release.
> 
> Somethin to think about, maybe the stakes are a bit higher?",/r/MachineLearning/comments/hdi9hq/discussion_about_data_bias_vs_inductive_bias_in/fvlrz3z/
1592454072.0,17-Jun-2020 21:21:12,dojoteef,MachineLearning,"The bitter lesson says nothing about more compute being better than novel algorithms. CS is grounded in the study of computability and devising efficient algorithms. Deep learning is no different in that sense. The point being made is not to extract hand engineered features, but rather rely on the raw data.

It is abundantly clear that while we might have the ideas for the general building blocks needed to achieve more human-like intelligence we have yet to figure out how to appropriately fit those pieces together, let alone do that efficiently. I think that’s why some criticize the work of OpenAI. I think it’s important that someone investigates the limits of what we can currently achieve, so I’m happy OpenAI is doing this research, but let’s not kid ourselves. No matter how large they scale a Transformer architecture up, it alone will not lead to human-like intelligence. There are still missing pieces to the puzzle that compute alone cannot solve.",/r/MachineLearning/comments/hay15t/r_openai_image_gpt/fv783pv/
1592277363.0,15-Jun-2020 20:16:03,dojoteef,MachineLearning,"The paper describes it as essentially a modified form of knowledge distillation. That is training an initial network (the teacher) on some data, then training another network (the student) using the labels assigned by the teacher. I say they use a modified form of knowledge distillation, because they train the student using the teacher labels for a dataset unseen while training the teacher (typical knowledge distillation trains on the same dataset for both student and teacher).

&nbsp;

I am not surprised that the approach works, as knowledge distillation has been shown as a way to improve performance in numerous papers. Their contribution is demonstrating that this works better than pretraining.",/r/MachineLearning/comments/h9seb1/r_rethinking_pretraining_and_selftraining/fuz3h06/
1592229071.0,15-Jun-2020 06:51:11,dojoteef,MachineLearning,"I know it’s weirdly worded, but their “submission documentation” requires the inclusion of any external data used: see section B1 of the [general model documentation guidelines](https://www.kaggle.com/WinningModelDocumentationGuidelines).

> B1. All code, data, and your trained model goes in a single archive",/r/MachineLearning/comments/h91cas/d_facebook_and_kaggle_face_backlash_after/fuwg6l1/
1591447364.0,06-Jun-2020 05:42:44,dojoteef,MachineLearning,"I typically only have master and feature branches, since I rarely need to have something publicly released that stays stable while I develop new features. Though, if I do run into that case, I will likely have release branches as well.",/r/MachineLearning/comments/gxcf30/d_git_ml_research/ft43t6q/
1591395857.0,05-Jun-2020 15:24:17,dojoteef,MachineLearning,"Git is just as useful for solitary projects as it is when collaborating with others:

* Do you want to look back at what your code looked like yesterday, last week, last month, last year? Look at your commit history.
* Are you interested in trying out an idea you don’t know will work? Use a branch!
* Etc

In other words, I use it and structure it the same way whether I am collaborating with others or not.

&nbsp;

But I don’t stop there. I use good software engineering practices for everything. Comments are just as useful to your future self as they are to others. You aren’t going to remember your reason for the weird workaround you did on line 215 in two years when you’re looking at it three projects from now. Write a comment!


&nbsp;

I incrementally refactor my code when I notice I am repeating myself, or if I realize a better structure after working in that area of code for a while. I don’t want to try to remember the five different places I have identical code when I realize I need to change my approach. If I’m using a similar object or structure in multiple areas, I’ll look to generalize the code so it can be reused more easily.

&nbsp;

Researchers often complain that they don’t have the time to do these things. That’s usually only true if you have an extremely short timetable (like a hackathon). Anything longer and you’re likely to save time by incrementally improving your code. Admittedly, it’s a skill that takes time to learn and master, but it’s worth it in the long run.",/r/MachineLearning/comments/gxcf30/d_git_ml_research/ft0roed/
1591224846.0,03-Jun-2020 15:54:06,dojoteef,MachineLearning,"I know better than to jump into this conversation, but I'm going to do it anyway. The amount of consternation being put forth over the broader impacts statement feels like a knee-jerk reaction. *NO ONE* expects you to write a treatise on world peace. Rather, they want a concerted effort for you to consider the uses of your research by society at large. *NO ONE* expects that a theoretical advancement is suddenly going to rob individuals of their human rights. Rather, reviewers want to see if someone can modify your work for heinous uses. These aren't nebulous ethical requests. Focus on applications that have clear repercussions that the majority of the world considers inhumane or outright destructive. You can easily look this stuff up, like [International Human Rights Treaties](https://en.wikipedia.org/wiki/International_human_rights_instruments) that most of the world has agreed to. Or the fact that climate change is an imminent global threat. If you don't think any these apply to your work, then say so. This isn't meant to be a gotcha moment, unless your work is highly susceptible to abuse, which is likely only a tiny fraction of the 10K papers being submitted to NeurIPS. Good luck to all of you submitting to the conference and I sincerely hope you are able to overcome your discomfort over this broader impacts statement.",/r/MachineLearning/comments/gvnz5k/d_on_the_broader_impact_statement_for_neurips/fsssk6t/
1591101001.0,02-Jun-2020 05:30:01,dojoteef,MachineLearning,"RSUs typically take a full year before you can touch any of it, so you don’t get any of it in your first year. That’s partly what the signing bonus is providing. Second, that signing bonus is one time, and it’s a little bigger than the yearly income you could expect from the RSUs. That really puts the yearly compensation closer to what I stated.

Another factor you probably aren’t considering, but actually makes a huge difference is the tax on the RSUs. If you sell that stock within 2 years of it vesting (essentially within 3 years of being hired) you pay a much higher tax rate on it (nearly a 50% effective tax rate in California). This comes into play for actual _take home_ salary.

Again, I’d say your numbers a misleading in a way that isn’t beneficial to people actually trying to make an informed decision, like the OP. Considering they asking him to be part of a startup, there is the potential for equity. Typically startup salaries are 60-80% of base salaries you’d see at a FAANG, with only the hope of a a big payday from equity (through a buyout or going public). That’s the more realistic look at what the OP can ask for (if they were in the US; don’t know how the system works in the UK).

For startups the most important things to negotiate are the equity stake you’ll get, since the eventual payout is much more nebulous at the outset, and startups are often cash strapped so a higher salary isn’t usually in the cards.",/r/MachineLearning/comments/guso10/d_a_question_about_the_typical_salary_for_a_fresh/fsmnmwd/
1591071926.0,01-Jun-2020 21:25:26,dojoteef,MachineLearning,"Uh... I’m not following your math here for undergrad salaries. Those RSUs (restricted stock units) are typically vested over 4-5 years. So the base salary + the average expected stock vesting doesn’t seem to match what you’ve stated. More like $175k at the top end and $150k average, which, is much more inline with what I expect, versus what you’re stating of $220k a year on average, excluding any one time signing bonus. (I used to be an engineering manager in SF before going back for my PhD)",/r/MachineLearning/comments/guso10/d_a_question_about_the_typical_salary_for_a_fresh/fslpa73/
1590718479.0,28-May-2020 19:14:39,dojoteef,MachineLearning,"I definitely look forward to reading the paper in full after the EMNLP deadline, but a quick skim of the paper brings up some interesting highlights: 1) detecting training/test set contamination is hard at this scale, but may have limited impact, and 2) it's possible that we are starting to hit the fundamental limits of our current training paradigms.

Relevant quotes:

For point #1
> We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.


For point #2
> A more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the pretraining objective.

Overall, I'm pleasantly surprised with what I've skimmed so far. The paper seems to discuss real issues, including limitations of the work, without hype. Kudos to OpenAI for trying something at this scale!",/r/MachineLearning/comments/gsivhg/r_language_models_are_fewshot_learners/fs5nz7s/
1590433393.0,25-May-2020 12:03:13,dojoteef,MachineLearning,"This is a recent example of great experimental design at the BlackboxNLP workshop from ACL2019:

https://www.aclweb.org/anthology/W19-4819/

They preregister their experiments (so no posthoc hypothesis) and  measure statistical significance using standard psycholinguistic techniques (that’s the background of the authors). Great read for anyone interested in the area of NLP.",/r/MachineLearning/comments/gpgspf/d_examples_of_good_science_in_deep_learning/frsm7pq/
1590264584.0,23-May-2020 13:09:44,dojoteef,MachineLearning,"Typically not. In fact you are likely to be dinged on your grant review (at least for NSF and NIH) if you overstate so blatantly. Rather grant writers are encouraged to write statements like “this research  is a clear step towards better understanding...” or “this has the potential to improve outcomes...”

Additionally for those grants they take a much wider view of what constitutes a broader impact, since these represent potential multi-year funding with tax payer dollars. That wider view is not applicable to the NeurIPS statement.",/r/MachineLearning/comments/gp90ni/d_instead_of_authors_submitting_the_broader/frkw3b5/
1590260651.0,23-May-2020 12:04:11,dojoteef,MachineLearning,"While seemingly new and scary for a conference like NeurIPS, the notion of assessing broader impacts is not new. Many organizations that award grants (such as the NSF in the US) require a [broader impacts statement](https://www.researchinsociety.org/guiding-principles). While admittedly, what's expected in a grant's Broader Impacts section is different than what is being asked by NeurIPS, the fundamental idea is similar: what are the potential societal impacts of this research?

I would simply follow [the advice](https://medium.com/@BrentH/suggestions-for-writing-neurips-2020-broader-impacts-statements-121da1b765bf) linked by the NeurIPS website and additionally read the associated [position paper](https://acm-fca.org/2018/03/29/negativeimpacts/).

While these steps don't guarantee you'll do an amazing job at your first attempt, if you legitimately make an effort I doubt reviewers will reject a paper that otherwise demonstrates it is deserving of inclusion at NeurIPS.",/r/MachineLearning/comments/gp90ni/d_instead_of_authors_submitting_the_broader/frkp34h/
1590112285.0,21-May-2020 18:51:25,dojoteef,MachineLearning,"Wow, missed this blog post earlier, but their research productivity gain (emphasis added) is pretty telling:

> The main reason we’ve chosen PyTorch is to increase our research productivity at scale on GPUs. It is very easy to try and execute new research ideas in PyTorch; for example, *switching to PyTorch decreased our iteration time on research ideas in generative modeling from weeks to days.*",/r/MachineLearning/comments/go95w2/deleted_by_user/fremr8q/
1590073525.0,21-May-2020 08:05:25,dojoteef,MachineLearning,"LDA is popular, but using word embeddings for topic modeling is becoming more prevalent. Even [recent](https://www.semanticscholar.org/paper/Topic-Modeling-in-Embedding-Spaces-Dieng-Ruiz/d5f47453a6d00ede2881dfb65fc7ea141a50deeb) [research](https://www.semanticscholar.org/paper/The-Dynamic-Embedded-Topic-Model-Dieng-Ruiz/853be905f533fb347d58c463a61bc365e133c2ca) from David Blei's group is looking at using word embeddings for LDA. I can also state that it's possible to devise reasonable topic models that don't use LDA at all, but that's currently active research.",/r/MachineLearning/comments/gnucmg/d_topic_modeling_current_research_state/frck0ot/
1589924649.0,19-May-2020 14:44:09,dojoteef,MachineLearning,"Here’s a link to an upscaling paper I worked on when I was just getting my feet wet with research:

https://arxiv.org/abs/1712.00714

By training on image patches along with conditioning on a normalized coordinate system you can achieve arbitrary upscaling (though my early attempts at making it work for CIFAR10 didn’t work out so well, likely due to the images lacking enough fine grained detail). I wonder if the approach might work better with larger images...",/r/MachineLearning/comments/gmwgmx/discussion_is_this_kind_of_image_upscaling_model/fr6e7ke/
1589912189.0,19-May-2020 11:16:29,dojoteef,MachineLearning,"I'm very glad these guidelines were written, because it helps both authors and reviewers understand more clearly what constitutes publishable work.

&nbsp;

This is important because at some point everyone in the community has likely run into a reviewer that has clearly rejected a work based on SOTA, or thoughts that the method is ""too simple"". The first workshop I ever submitted to, I got a review that was so short, I'm positive my abstract was longer. For the write in section about Paper Strengths, they literally just wrote the word ""None"". For paper weaknesses, they stated:

> The technical contribution of this paper is very weak, since it is just applying existing <technique> to the studied problem... Although the paper claims that the proposed method can generate very high resolution images, but the upscaled results are far from satisfactory.

Considering the work found it's way to Twitter, and the first author of the work I built off of decided it was noteworthy enough to retweet it (they tweet infrequently, so that tweet from 2.5 years ago is still within their last 50 tweets), I'm pretty sure the criticism was overly harsh.

&nbsp;

Needless to say, please, please, if you are a reviewer, spend the time to do a good thorough job for the review. This isn't a trial by fire. If you were treated harshly in a review, don't take it out on other authors. Rather, make the effort to help push the field forward.",/r/MachineLearning/comments/gms5pf/d_emnlps_reviewing_guidelines_now_pretty_bluntly/fr5oxad/
1589831280.0,18-May-2020 12:48:00,dojoteef,MachineLearning,"It can be good experience to learn to implement a paper from scratch. For my first publication, I began from a blank slate and implemented a Transformer model by only referencing the paper and was able to match the performance listed in the paper. Admittedly, that’s overkill in the general case, as it took quite a while just to get it working, let alone to devise an improvement worthy of publication. On the other hand it was invaluable experience that helped me gain a very deep understanding for how Transformers work (in addition to knowing how to make them performant).",/r/MachineLearning/comments/gm31b1/d_beginners_friendly_machine_learning_conferences/fr251zt/
1589731576.0,17-May-2020 09:06:16,dojoteef,MachineLearning,It's nice to see a concise visual demonstration of various NLP data augmentation techniques. Great job!,/r/MachineLearning/comments/glf5v5/d_a_visual_survey_of_data_augmentation_in_nlp/fqxfoob/
1589548069.0,15-May-2020 06:07:49,dojoteef,MachineLearning,"No one mentioned this yet, but you could also limit (or add noise to) the input state for the model. If the model is missing certain information it is less likely to make an optimal decision (even if you specifically train it with some sort of mask/cloze like state infilling). This can also add explainability to the model.

&nbsp;

For example, imagine you want an aggressive model that is more likely to kamikaze. You would train it normally, but during inference you could simply mask out the health of the units (or mutate them to be higher than expected, or even put in a delay before the model ""notices"" changes in its state). With such incorrect information, it would more likely take risks that are likely to get it killed.",/r/MachineLearning/comments/gjncmi/d_is_it_possible_to_create_a_gaming_rl_algorithm/fqpjsbh/
1589547575.0,15-May-2020 05:59:35,dojoteef,MachineLearning,"One simple way to do this that only requires Python is to use the built-in capabilities of the [array](https://docs.python.org/3/library/array.html) library. In fact that's what I use in my Pytorch code for compact binary formats. It seems pretty trivial to add an interface layer on top to support any data type you want.

&nbsp;

Under the hood, TFRecord uses [protocol buffers](https://developers.google.com/protocol-buffers/). If you really want to go with a prebuilt solution, rather than rolling your own you could use protocol buffers, or consider [FlatBuffers](https://google.github.io/flatbuffers/). It was designed and written by a friend of mine who works at Google, originally as a compact way to store video game data. I believe it's [used in Tensorflow Lite](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/flat-buffer-model) as well.",/r/MachineLearning/comments/gjn016/d_alternatives_to_tfrecord_i_just_want_to_store_a/fqpj14a/
1588775767.0,06-May-2020 07:36:07,dojoteef,MachineLearning,Consider adding [asyncio](https://docs.python.org/3/library/asyncio.html)-based Python frameworks to your backend experience. For my current research I use [FastAPI](https://fastapi.tiangolo.com) to host machine learning models.,/r/MachineLearning/comments/gefyzd/facebook_ai_residency_canceled_due_to_covid19/fpnzraf/
1588470770.0,02-May-2020 18:52:50,dojoteef,MachineLearning,"I’ve experimented with a lot of different attention functions for machine translation. I was able to achieve better perplexity with some, but it never improved BLEU. That said, since LMs often report perplexity, it might be worth trying different attention mechanisms for that task. Though if you use the LM for pretraining, who knows if it’ll improve downstream performance; my experience with MT makes me think it’s unlikely.",/r/MachineLearning/comments/gch6h2/d_scaled_dot_product_vs_scaled_cosine_similarity/fpbih2k/
1588189233.0,29-Apr-2020 12:40:33,dojoteef,MachineLearning,"I’m targeting a paper for EMNLP which introduces a dataset that could be very useful for training an AI dungeon master. Unfortunately I won’t be able to release a preprint before the anonymity period, so it might take a little while longer before I get to share it.",/r/MachineLearning/comments/gab2ik/d_is_there_any_consensus_on_what_the_next_big_ai/fozec2g/
1587834777.0,25-Apr-2020 10:12:57,dojoteef,MachineLearning,Great job on the video! We've covered quite a few of the papers you brought up in our reading groups and had similar discussions. It's nice to see that distilled into a short video format.,/r/MachineLearning/comments/g6vdil/d_huggingface_is_now_on_youtube_with_a_dense/fok3m25/
1587821904.0,25-Apr-2020 06:38:24,dojoteef,MachineLearning,"You're right that there are many issues with automatic metrics, but they can be useful to help gauge performance of your model while developing it. Despite the ubiquity of automatic metrics, human judgements are still considered the gold standard. Note that researchers are actively looking for ways to improve automatic metrics to better align with human judgements. For example, [BLEU might be Guilty but References are not Innocent](https://arxiv.org/abs/2004.06063) is a very recent paper from Google investigating improvements to BLEU.",/r/MachineLearning/comments/g7t4rz/r_research_paper_that_puts_more_emphasis_on/fojim0y/
1587738966.0,24-Apr-2020 07:36:06,dojoteef,MachineLearning,Thanks for pointing that out!,/r/MachineLearning/comments/g70rma/r_yolov4_optimal_speed_and_accuracy_of_object/fofmloo/
1587699277.0,23-Apr-2020 20:34:37,dojoteef,MachineLearning,Hmm... why is this called YOLOv4? The naming seems a little disingenuous.,/r/MachineLearning/comments/g70rma/r_yolov4_optimal_speed_and_accuracy_of_object/foead30/
1586470867.0,09-Apr-2020 15:21:07,dojoteef,MachineLearning,"This survey is also quite excellent:
https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/",/r/MachineLearning/comments/fxn5lo/r_the_transformer_family/fmxs8u3/
1584930462.0,22-Mar-2020 19:27:42,dojoteef,MachineLearning,"At my university, the hiring process for CS faculty is still ongoing. Job talks are now being streamed rather than having individuals fly out.


Though on the other hand, my wife just applied for a faculty position (in a different field at a different college) and the department head said their current priority was figuring out how best to adjust to remote learning, so it might be a while before they continue their current search.


It's really hard to extrapolate from these two anecdotes what the job market will be like for next year's graduates. Hopefully it doesn't have too dramatic of an effect.",/r/MachineLearning/comments/fna75t/d_how_will_the_pandemic_affect_job_prospects_in/fl8rfh0/
1584452754.0,17-Mar-2020 06:45:54,dojoteef,MachineLearning,"Certainly an interesting idea!

I’m going to mention a number of issues that I think are pretty glaring because I feel it’s important to provide constructive feedback. Please realize it’s coming from someone that hopes the issues can be addressed because the basic premise seems interesting.

First and foremost I would tone down the sensationalism _drastically_. It’s HIGHLY misleading to state that you can somehow train deeper networks and still fit them on a single GPU while burying the fact that the model has much smaller dimensionality in an Appendix. The claim of running deep networks on a laptop GPU (most laptops do not come with a dedicated GPU), is even more disingenuous. Please don’t fall into the trap of overhyping your work.

From the wording of the paper it seems like you are comparing convergence speed while looking at the training curve. Only the caption mentions that the BPB number is from a held out validation set. This makes it easy to miss. I initially thought you were reporting numbers on the training set and nearly chalked it up as a no result paper.

The experiments and analysis are a little shallow (no pun intended). I really wanted to see more analysis on the Transformer model results. What do the comparative training curves look like? Can the model achieve better results given more time or is it just an improvement in training time? How does this effect inference time? You’ve suddenly made a highly parallel Transformer model more sequential, it seems like it must effect inference speed. In fact, how doe you measure speedup? Number of optimization steps or wall-time?

I’d hope that before this paper gets submitted to a conference these  issues could be addressed. Good luck with the ongoing research!",/r/MachineLearning/comments/fjy51p/r_128_layer_transformer_on_your_laptop_rezero/fkqpvai/
1584303765.0,15-Mar-2020 13:22:45,dojoteef,MachineLearning,"Hey, I understand that you're passionately defending your perspective, but sometimes it's important to take a step back and look at how your writing is being perceived by others.

While it is exceedingly rare that someone can publish without guidance in the field, (whether that's an undergrad, a grad student from a different discipline, or someone who is a hobbyist with no formal education in the subject), emphatically stating that it is not possible ignores the outliers that exist (and will likely elicit pushback). In this thread, I'd err on the side of questioning *how* they were able to succeed as an undergrad and what support they received, rather than assuming they might be disingenuous.

That said, I'm a CS PhD student in NLP and I agree with your general assessment that it is exceedingly hard to publish, let alone without any guidance or support. For applicants who are considering accepting an offer to become a PhD student in our department, we have a hand-raising event to give prospective students an idea of the demographics of the current student population. One of the standard questions is, ""Who here published in their first year?"" Out of around the 50-60 students PhD students who take part, a tiny fraction raise their hand (probably less than 10%). That's *with* the guidance and support of an advisor. I imagine it's even more difficult without any guidance or support.",/r/MachineLearning/comments/fiqcqt/r_is_it_possible_to_do_research_on_your_own/fkld6ie/
1582892803.0,28-Feb-2020 04:26:43,dojoteef,MachineLearning,"Yeah, it definitely depends on the University. I’m a PhD student at UMass Amherst and we have a [shared compute cluster](https://gypsum-docs.cs.umass.edu/) with over 1000 GPUS, 200 of which are 2080tis. At one point students could get up to [150 GPUs for up to 4 hours](https://ds.cs.umass.edu/news/gypsum-gpu-cluster-upgrade). I think it’s changed to allow a maximum of 80 GPUs (per GPU type, we have TitanX, 1080ti, and 2080ti GPUs), but that’s still plenty!",/r/MachineLearning/comments/fakjwm/d_thoughts_on_joint_academicindustry_phd_programs/fj089xj/
1582720768.0,26-Feb-2020 04:39:28,dojoteef,MachineLearning,"I wonder how much effort went into the acronyms to spell out AN(i)M(a)L PL(a)N. On the serious side, I believe other work has looked at a learned gating to prevent catastrophic forgetting (along with having a regularization effect). For example:

https://www.pnas.org/content/115/44/E10467.long",/r/MachineLearning/comments/f9gslr/learning_to_continually_learn_j_clune_et_al/fiti4bf/
1582647722.0,25-Feb-2020 08:22:02,dojoteef,MachineLearning,"Yes there are two morally comparable sides. Assistive devices such  [AIPoly](https://www.aipoly.com), which have legitimate real world helpful uses, must be able to identify objects. On the other hand, the ability to identify objects in general can be used for nefarious purposes, i.e. persecution of minority populations by tracking their every move. Hence, my reference to Feynman's essay and the associated Buddhist proverb.",/r/MachineLearning/comments/f8wsyg/nd_yolo_creator_joseph_redmon_stopped_cv_research/fiqmxcl/
1582585317.0,24-Feb-2020 15:01:57,dojoteef,MachineLearning,"There are no ""right"" answers to ethical dilemmas. They require answering tough philosophical questions and I applaud Redmon for taking a stand on what he feels is best.

I feel compelled to link to [comments](https://old.reddit.com/r/MachineLearning/comments/e1r0ou/d_chinese_government_uses_machine_learning_not/f8sa5vn/) I posted in a previous thread on ethical concerns of ML/CV research in China.

TL;DR this Buddhist proverb (repeated in Richard Feynman's short essay [_The Value of Science_](http://www.faculty.umassd.edu/j.wang/feynman.pdf)) succinctly highlights the dual nature of scientific advancement:
> To every man is given the key to the gates of heaven; the same key opens the gates of hell.",/r/MachineLearning/comments/f8wsyg/nd_yolo_creator_joseph_redmon_stopped_cv_research/fiol0qf/
1579698436.0,22-Jan-2020 05:07:16,dojoteef,MachineLearning,"Then don’t go for a PhD. Most programs require at least one semester as a teaching assistant. Additionally, if you only have a bachelors degree you will have to take courses, which can also be seen as grunt work, especially if it isn’t related to your area of research (which will happen).

Though without getting a PhD your options to pursue research are likely pretty limited, unless you are somehow able to publish papers as a first author that demonstrates your research ability (that’s usually part of a PhD). That could mean an AI residency program, though those can be even more difficult to get into than PhD programs and most often do not immediately lead to a research scientist position.

To be honest anytime you are looking to change career direction there is likely some amount of grunt work you must be willing to accept. The very rare exception is if you are extremely talented and do work on the side that clearly demonstrates your ability in a way others are willing to accept.",/r/MachineLearning/comments/es9uhw/d_how_can_i_become_a_research_scientist_as_a/ff8wjos/
1579104877.0,15-Jan-2020 08:14:37,dojoteef,MachineLearning,"I wholeheartedly agree with the idea of writing things down, then revisiting over time. This is especially useful when you are new in the field and your ideas might be more nebulous as you don’t know the scope of what is possible. It means you can be more creative, but sometimes do not have the ability to translate these ideas into actionable research ideas. At least, that’s what happened to me early on. Many ideas I looked back upon were not useful, but a handful were... unfortunately I haven’t had time to investigate them further, so having them written down allows me to have a pool of fresh ideas after I finish my current research.",/r/MachineLearning/comments/eoyc25/d_thinking_clearly_about_research_directions/fegxql9/
1578431686.0,07-Jan-2020 13:14:46,dojoteef,MachineLearning,"Here's one trick I use for my very first pass of a paper: I read the abstract and intro to understand what the paper is trying to solve, then think about how *I* would solve this problem *before* reading the approach outlined in the paper. I find it is *much* easier to creatively come up with a solution for next steps this way, because I come in unfettered by an in depth understanding of the approach taken by the authors. It also allows me to flex my creative problem solving ability, which is a skill like any other, and thus requires concerted effort to develop. Naturally, I adjust my thoughts on the problem space the more I read about the author's approach and relevant prior work.",/r/MachineLearning/comments/elde4z/deleted_by_user/fdhx0hr/
1578092226.0,03-Jan-2020 14:57:06,dojoteef,MachineLearning,"It seems your comment stirred up some heated debate. I’m unwilling to make statements impugning your character or development abilities, as it seems some of the commenters have done. Though, I want to state a counterpoint to your assertions.

I’m also a PhD student, but that is after spending roughly a decade in industry. I don’t have a notion of “cleaning up” my code for release. I incrementally improve it as I develop, such that it is releasable as soon as I’m done with the research. This includes writing comments, and easily readable code. I find it makes it MUCH easier to iterate without making silly mistakes, though looking around at my peers, it seems this approach is not very common in grad school. It is certainly a skill that takes time to develop, but I find it’s well worth the effort. In fact my advisor seems to be of the opinion that it helps others in the lab have a solid basis to build upon my work.

Additionally, when I have worked with other PhD students, we came up with a simple workflow of adding command line options and hyperparameters in a backwards compatible way, and used version control to track all of our experiments. That turned out to be a winning combination that allowed us to iterate quickly and test out a lot of ideas without needing multiple branches or accidentally repeating experiments. Not all research can be developed in this manner, but for the project we worked on this workflow was a major boon.

I completely understand that software engineering skills seems less pertinent to research, though I believe that is an issue of perception that you can “move faster” in your research that way. They way I look at it is that building a strong infrastructure allows you to work more quickly later. Though, if you never revisit the same code, maybe it isn’t worth the effort. Hopefully that isn’t super common in a person’s research career.",/r/MachineLearning/comments/ejf6o9/d_i_wish_conferences_required_an_accompanying/fcz2ahq/
1577478746.0,27-Dec-2019 12:32:26,dojoteef,MachineLearning,"I am certainly not willing to jump to conclusions, but I can't help but look at the public comments on the paper as well. One of the first public comment threads is from [Aurko Roy](https://openreview.net/profile?id=~Aurko_Roy1), who has previously published with [Łukasz Kaiser](https://openreview.net/profile?id=~Lukasz_Kaiser1). It just brought to mind this previous reddit post [ [D] A potential sneaky strategy to get your ICLR paper accepted [Not Recommended!] ](https://old.reddit.com/r/MachineLearning/comments/edat7l/d_a_potential_sneaky_strategy_to_get_your_iclr/), which I had initially discarded as nonsense. I'm still not sure how it would positively impact an official review score, even though the public comments would be visible to reviewers. Though, it is certainly weird to have that kind of behavior as part of the review process.",/r/MachineLearning/comments/eg1wr3/reformer_the_efficient_transformer_anonymous_et/fc6em2l/
1577458675.0,27-Dec-2019 06:57:55,dojoteef,MachineLearning,"The authors’ claim of fitting 64K tokens on a single GPU as a novel contribution seems disingenuous. The authors specifically call out using gradient checkpointing for their approach (see the final paragraph of section 3), but for the regular Transformer they conveniently state they are simply unable to train it as it won’t fit into memory.  They do not seem to make any mention that they tried to use gradient checkpointing for the regular Transformer to make it comparable.

I highly suspect that gradient checkpointing is their only reason for chunking some of the computations. It’s possible that since the feedforward layers have much higher dimension, you can’t rely on gradient checkpointing alone to keep a sequence of length 64K in memory, so they split the sequence into smaller independent chunks.

These two aspects alone are merely well known engineering techniques that likely account for the authors’ ability to train models on very long sequences. I (and many others) have certainly used gradient checkpointing on Transformers for the same reason, but since it is a well known technique at this point I’m not sure that the authors are contributing anything new here.

The the authors do not report absolute timings for training their approach compared to a regular Transformer either. Rather, they report what appears to be the forward pass of their LSH layer only. My guess is that their approach is much slower to train – it uses gradient checkpointing after all and splits matrix multiplies to fit everything in memory.

BTW, what type of GPU do they use? How much memory does it have? There’s a big difference fitting on a 2080 Ti with only 11Gb versus a V100 with 32Gb...",/r/MachineLearning/comments/eg1wr3/reformer_the_efficient_transformer_anonymous_et/fc5iox3/
1576763492.0,19-Dec-2019 05:51:32,dojoteef,MachineLearning,"Really great work! Focusing on areas of high uncertainty, along with adaptively upsampling seems to be a winning combination.",/r/MachineLearning/comments/ecrzsk/pointrend_image_segmentation_as_rendering/fbdixb4/
1576505096.0,16-Dec-2019 06:04:56,dojoteef,WoT,"Here’s one of the [publishing guides](https://www.biblio.com/publisher/tor-books) that /u/Phat_Strat mentioned. And here’s what [a first edition page looks like, along with Jordan’s real signature](https://m.imgur.com/a/xOWgTI0).

EDIT: That said, it’s not a first printing, just a first edition. No idea what printing it is. Though I bought my copy sometime in the 90s and got it signed around 2001 or 2002.",/r/WoT/comments/eb8uvs/how_to_tell_if_my_wot_books_are_first_print/fb4ahq2/
1576462499.0,15-Dec-2019 18:14:59,dojoteef,MachineLearning,"Again, I really appreciate you providing such an open retrospective! I think that takes courage and integrity and truly hope that if I was in your shoes, I would do the same.

Additionally, it sounds like maybe the way you framed things in your talk regarding Karen Hao's coverage of the research made her seem less competent than she actually is. I'm definitely willing to give her the benefit of the doubt.

Though, I am still concerned about her refusal to even mention the graduate students and do take issue with the fact that, despite being upset over it, you seem to be willing to accept that this is just the way tech journalism works. There are plenty of articles, even on MIT Tech Review that mention the graduate students, and that does not seem to detract from the articles. Here's smattering of examples related to our field:

[Exclusive: This is the most dextrous robot ever created](https://www.technologyreview.com/s/610587/robots-get-closer-to-human-like-dexterity/)

[Boosting AI's IQ](https://www.technologyreview.com/s/611229/boosting-ais-iq/)

[Storage System for 'Big Data' Dramatically Speeds Access to Information](https://www.technologyreview.com/s/524281/storage-system-for-big-data-dramatically-speeds-access-to-information/)

That said, I understand that you may feel that you do not have much power in this situation (it seems many in power feel they are powerless to push for positive change). Though, you likely have more power than you may be willing to exert. For example, you could threaten not to allow Karen Hao access to you for covering work from your lab in the future. While that can potentially harm your ability to have your research spotlighted, its something that is essentially happening to the students in your lab right now.",/r/MachineLearning/comments/eayp99/r_neuips_2019_david_duvenaud_bullsht_that_i_and/fb325gd/
1576427047.0,15-Dec-2019 08:24:07,dojoteef,MachineLearning,"Wow. I really appreciate the honesty, but couldn’t help being taken aback by the discussion of the MIT Tech Review article (starts around 7:05 in the video).

1) It seems like there is ZERO fact checking going on if the original article states that David Duvenaud invented ODEs or that the name ordinary differential equation needed rebranding because the word “ordinary” does not sound sexy enough.

2) The fact that the reporter flatly refused to even mention the grad students who actually conducted the work, is simply unethical. The fact that Duvenaud voiced agreement with the sentiment that it somehow dilutes the article is appalling.  This isn’t a high energy physics paper with thirty authors. This behavior simply perpetuates the status quo and appears to lead to infighting within the ML community (at the very least on this sub). It’s reminiscent of researchers who complain about not being cited, while in turn often not sharing credit with the grad students who actually conducted the research. It seems these researchers assume that simply coming up with an idea is enough to gain acclaim, and the difficult work of actually bringing an idea to fruition is seemingly less important/interesting.",/r/MachineLearning/comments/eayp99/r_neuips_2019_david_duvenaud_bullsht_that_i_and/fb0lblj/
1575895417.0,09-Dec-2019 04:43:37,dojoteef,MachineLearning,"You posted the exact same link nearly a month ago.

[https://old.reddit.com/r/MachineLearning/comments/dwnxp7/n_world_first_ever_computer_rpg_with_dungeon/](https://old.reddit.com/r/MachineLearning/comments/dwnxp7/n_world_first_ever_computer_rpg_with_dungeon/)",/r/MachineLearning/comments/e87fbs/n_open_world_rpg_with_dungeon_master_ai_and_story/fa9x5zm/
1575407410.0,03-Dec-2019 13:10:10,dojoteef,MachineLearning,"Have you looked at [pydantic](https://pydantic-docs.helpmanual.io/usage/settings/), [attrs](https://www.attrs.org/), or python 3.7's [dataclasses](https://docs.python.org/3.7/library/dataclasses.html) (which has been backported to 3.6)? They offer very similar functionality.",/r/MachineLearning/comments/e5fv65/p_anyfig_configuration_manager_argparse/f9kt8ji/
1575214832.0,01-Dec-2019 07:40:32,dojoteef,MachineLearning,"From the graphs, there are certainly some interesting cases that I’d love to know more about. Specifically the handful of scores that swung from 1 up to 8 and vice versa.


It’s also interesting that 1) reading a paper more thoroughly and 2) having more experience in the field are more likely to cause a score change. I wonder why. I’d love to know the correlation between those two groups.",/r/MachineLearning/comments/e4hp78/d_how_iclr_review_scores_changed_during_the/f9bovjt/
1574774915.0,26-Nov-2019 05:28:35,dojoteef,MachineLearning,"Many in science have questioned whether their discoveries will be used for good or evil and it can be helpful to learn from their wisdom. I think the late great Richard Feynman's short essay *[The Value of Science](http://www.faculty.umassd.edu/j.wang/feynman.pdf)* has some great insight that came from questioning the creation of the atomic bomb. Here are a few salient snippets from the essay:

> I believe that a scientist looking at nonscientific problems is just as dumb as the next guy - and when he talks about a nonscientific matter, he sounds as naive as anyone untrained in the matter. Since the question of the value of science is not a scientific subject, this talk is dedicated to proving my point - by example.

Which likely highlights why we are so poor at discussing these matters here on r/MachineLearning... Later he repeats a Buddhist proverb to highlight the dual nature of scientific advancement to be used for good or evil:

> To every man is given the key to the gates of heaven; the same key opens the gates of hell.

And he ends the essay by highlighting the persecution of scientists (like [Galileo](https://en.wikipedia.org/wiki/Galileo_affair)), and our duty to uphold the current freedoms to ask hard questions:

> It is our responsibility as scientists, knowing the great progress which comes from a satisfactory philosophy of ignorance, the great progress which is the fruit of freedom of thought, to proclaim the value of this freedom; to teach how doubt is not to be feared but welcomed and discussed; and to demand this freedom as our duty to all coming generations.",/r/MachineLearning/comments/e1r0ou/d_chinese_government_uses_machine_learning_not/f8sa5vn/
1574134446.0,18-Nov-2019 19:34:06,dojoteef,MachineLearning,"Having just written code to deploy NLP models as a web service, I would also second the use of an asyncio framework. I ended up using [FastAPI](https://fastapi.tiangolo.com), which is based on [Starlette](https://www.starlette.io). I additionally added caching via [aiocache](http://aiocache.readthedocs.io/) (since I have long running text generation that requires preprocessing) and wrote a simple publisher/consumer asyncio queue to automatically batch requests that are received within a specified timeframe. I got the idea to automagically batch requests from [these benchmarks](https://github.com/ShannonAI/service-streamer#benchmark) which highlight the importance of batching to best optimize use of the available GPUs.",/r/MachineLearning/comments/dy8hjh/p_cortex_deploy_models_from_any_framework_as/f80p0wr/
1573959368.0,16-Nov-2019 18:56:08,dojoteef,MachineLearning,"> Having worked on both the industry / academic sides of the fence, what do you think are currently the biggest hurdles to ML making it's way into games?

Fundamentally, ML amounts to a blackbox: feed data in during training and see the result afterward. Though, when using the trained system, it is not clear how to influence the decisions it makes. That means you have to encode all the domain knowledge in the training data and associated loss. It's hard to formulate many problems into this paradigm, especially for video games. What data do you train your model on when you are designing a brand new game? ML is notoriously bad at domain transfer, so learning from your previous game is unlikely to transfer to your new game. How then are you to train an AI DM when you haven't even finished making your game yet?

Both [EA](https://www.ea.com/seed/publications) and [Unity](https://unity3d.com/machine-learning) are trying to build on the hype of ML for games, but if you look at the publications they have, the only really applicable areas they are investigating that have potential are in areas of procedural content generation, e.g. texture synthesis and the like.

> Do you think reinforcement based learning is becoming increasingly relevant to architectures?

This is likely one of the few areas that applies well to video games, but it currently suffers from a few problems:
* It's a blackbox that you cannot influence after training (as I mentioned above), thus if it has some undesirable behavior, there isn't an easy way to correct that behavior.
* It is hard to actually encode the desired behavior in the form of a reward function. What you would really want is to encode the reward as the amount of ""fun"" a user has when playing against an ML agent, but that's a nebulous concept. Making the AI capable of easily beating all the players, i.e. optimizing for its ability to win, doesn't make a game more fun.

> Personally I have been dabbling in nn based speech recognition and speech synthesis, with the goal of exposing an API catered to the needs of game engines. Been toying with the idea of building a small compute cluster for training.

This certainly sounds like an interesting idea. Being able to convert text to speech, could lower the burden of content creation, such that you no longer need to hire voice actors for absolutely all of the roles. On the other hand, it'll be hard to match real voice actors. You'd need someway to encode how you want words spoken: emotion, inflection, accent, cadence, etc. I'm not sure that's a solved problem yet, though there has been some work on style transfer in audio, which could help.",/r/MachineLearning/comments/dwnxp7/n_world_first_ever_computer_rpg_with_dungeon/f7q67sc/
1573851930.0,15-Nov-2019 13:05:30,dojoteef,MachineLearning,"I am a former lead/AI developer in video games. The possibilities of writing dynamic AI, that reacts to players actions was one of the main reasons I got into video games. I'm sure that idea has spurred lots of individuals, including [Demis Hassabis, who was briefly lead AI developer on Black & White](https://www.theguardian.com/technology/shortcuts/2014/jan/28/demis-hassabis-15-facts-deepmind-technologies-founder-google).


While, I appreciate the fact these developers are trying to make the types of games that captivated me growing up, almost all of game AI is a house of cards, cleverly designed to look like a beautiful manor house if you squint from a distance without your glasses on, ready to tumble at the slightest breeze. Creating truly interactive stories has been a focus in research for a while (often funded by DARPA for training purposes). Almost all of the classical approaches to this have been planning based [[1](https://core.ac.uk/download/pdf/42489848.pdf)] , (including the most well known example [Façade](https://en.wikipedia.org/wiki/Façade_(video_game\))). The planning based approaches are typically quite labor intensive, because it's difficult to define meaningfully large stories with graph formalisms. Recently people have begun to consider NLP-based approaches [[2](https://www.aclweb.org/anthology/P19-1254/)], [[3](https://www.aaai.org/ojs/index.php/AAAI/article/view/4726)].


There are a number of prominent researchers that have essentially made it their life's work to create more engaging video game AI/storytelling, including [Julian Togelius](http://julian.togelius.com) and [Mark Riedl](http://eilab.gatech.edu/mark-riedl). So far, no one has really designed an approach that is truly feasible as a video game DM.


But, sometimes what really matters, isn't that the developers design a system that can truly create novel situations. Rather, it could be enough to develop lots of manually branching storylines that are hand crafted and high quality, ala Planescape: Torment. Hats off to the developers if they can make something fun and engaging, regardless of the approach they take to get there.


If you can't tell, I'm super passionate about this area of research myself, and is why I left the video game industry to pursue a PhD with a focus on NLP. My current research includes creating a platform that will allow advancements in this area to accelerate. I should probably go back to that now, since I have deadlines coming up...",/r/MachineLearning/comments/dwnxp7/n_world_first_ever_computer_rpg_with_dungeon/f7m0ogz/
1573578963.0,12-Nov-2019 09:16:03,dojoteef,MachineLearning,"I was initially super excited from the title, because I thought this was a Solid Snake AI... maybe it’s Solid Snake in disguise?",/r/MachineLearning/comments/dux7k2/p_playing_snake_with_reinforcement_learning/f7bw53p/
1572182744.0,27-Oct-2019 06:25:44,dojoteef,MachineLearning,"Depends on what your looking for. If you’re considering academia, I’d recommend perusing [csrankings.org](https://csrankings.org). You can even break down the analysis by conferences like NeurIPS. I found it invaluable when applying to universities for a PhD.


Disclaimer: a professor from my university created the site.",/r/MachineLearning/comments/dndfmi/d_top_academic_research_labs_deep_learning/f5fhzjp/
1572181801.0,27-Oct-2019 06:10:01,dojoteef,MachineLearning,"""Do not answer! Do not answer! Do not answer!""",/r/MachineLearning/comments/dnic1x/n_newton_vs_the_machine_solving_the_chaotic/f5ff02x/
1571884231.0,23-Oct-2019 19:30:31,dojoteef,MachineLearning,"Great twitter thread describing the methodology:
https://twitter.com/colinraffel/status/1187161460033458177",/r/MachineLearning/comments/dm9m33/r_exploring_the_limits_of_transfer_learning_with/f4ysl9f/
1571742617.0,22-Oct-2019 04:10:17,dojoteef,MachineLearning,"Is this paper similar to [COMET](https://www.aclweb.org/anthology/P19-1470/)? If not, how does it differ? A cursory look seems to indicate that they both extract knowledge automatically and both investigated their approach on ConceptNet. BTW, the demo for COMET is pretty great!",/r/MachineLearning/comments/dleoje/r_language_models_as_knowledge_bases/f4ptmzm/
1571004075.0,13-Oct-2019 15:01:15,dojoteef,MachineLearning,"FYI: The paper [Order Matters: Sequence to sequence for sets](https://arxiv.org/abs/1511.06391) from ICLR 2016 investigates unordered sets in sequence to sequence frameworks. They in fact use attention as their encoding mechanism, so it it seems natural that if you dropped the positional encoding from a Transformer it should work. One additional point they bring up is that if you want an output set, order actually does matter; finding the best output ordering for a set can actually produce better results. So they devise a way to sample different orderings during training by choosing an appropriate loss.",/r/MachineLearning/comments/dh9a66/d_transformer_for_nonnlp_tasks_unordered_sets/f3nyos7/
1570748419.0,10-Oct-2019 16:00:19,dojoteef,MachineLearning,"This is really great! Papers With Code is super helpful, so I have no doubt this well be too (kudos on using SacreBLEU for NLP). That said, I have some questions:


1. What aspects are being reproduced? Do you automatically run the preprocessing & training, or are you relying on loading pre-trained models?

2. I see there is a focus on inference speed in addition to test metrics. Right now, it seems there is only a single notion of inference speed (presumably using batching). It is also useful to look at inference speeds when only given a single input. It can highlight the difference between what a large cloud-based service might use (batching) versus running on a user's device (single input).",/r/MachineLearning/comments/dfxzjj/p_sotabench_benchmarking_every_open_source_model/f39i3kr/
1570654440.0,09-Oct-2019 13:54:00,dojoteef,MachineLearning,"~~For best performance, should finetuning be performed on the distilled version of GPT-2 or the small GPT-2 model followed by distilling the finetuned model?~~



~~Also, is the code for distilling available in the latest release of the huggingface transformers repo?~~

NVM: The [README](https://github.com/huggingface/transformers/blob/master/examples/distillation/README.md) explains it pretty well!",/r/MachineLearning/comments/df55ij/n_test_a_distilled_gpt2s_generative_capabilities/f34hky4/
1568289112.0,12-Sep-2019 04:51:52,dojoteef,MachineLearning,"Storytelling, with the goal of allowing games to dynamically adapt the story to the actions of the player(s).",/r/MachineLearning/comments/d2qkvf/d_how_do_you_feel_machine_learning_will_affect/ezzlxrn/
1568253294.0,11-Sep-2019 18:54:54,dojoteef,MachineLearning,"I second your point of view. I left the video game industry after nearly a decade to go back for a PhD. There is no way a publisher would fund the type of research I’m interested in, despite the potential it may have for future video games.",/r/MachineLearning/comments/d2qkvf/d_how_do_you_feel_machine_learning_will_affect/ezxz05c/
1568234715.0,11-Sep-2019 13:45:15,dojoteef,MachineLearning,"Section 8 has interesting discussion on their reasoning for releasing the full model and the steps they took before deciding to release. Nice to see the discussion making its way into the actual literature, rather than being relegated to blog posts. It also gives much clearer guidance than I have seen previously (though admittedly I do not follow the ethics debate closely).

> Openness and replicability are central aspects of the scientific ethos that, prima facie, suggest the release of complete scientific research results. We reify these principles by releasing all trained CTRL models.",/r/MachineLearning/comments/d2uii7/r_ctrl_a_conditional_transformer_language_mode/ezx8ww0/
1567769290.0,06-Sep-2019 04:28:10,dojoteef,MachineLearning,"The USPTO asks one very salient question that addresses people’s general comments here about prior art:

> 9\. Are there any prior art considerations unique to AI inventions?",/r/MachineLearning/comments/d05wlv/d_shouldnt_we_be_doing_more_than_complaining/ez90zuk/
1567014859.0,28-Aug-2019 10:54:19,dojoteef,MachineLearning,"For more info, see [the previous discussion](https://old.reddit.com/r/MachineLearning/comments/bzka5r/r_weight_agnostic_neural_networks/).",/r/MachineLearning/comments/cwmbco/r_google_ai_blog_exploring_weight_agnostic_neural/eydaoud/
1563498920.0,18-Jul-2019 18:15:20,dojoteef,MachineLearning,"I'll second this answer. I also worked in industry for over 10 years, starting in video game development. I started my PhD last year as well.

&nbsp;

FYI, in order to gain research experience I took time off, studied fundamentals like probability, and finally found a professor online who was looking for a research assistant (unpaid). Despite a strong background as a lead programmer & an engineering manger and having excellent industry recommendations (including the Head of Platform development at Riot Games), the six months I spent conducting research part-time turned out to be the most important for me getting into a PhD program. My advisor credited that professor's recommendation of my research ability over any of my industry recommendations (despite those recommendations clearly highlighting my ability to creatively tackle challenging problems, i.e. come up with clearly researched solutions to new problem domains). Your friend's luck may vary.",/r/MachineLearning/comments/cel71v/d_very_strong_cpp_programmer_research_engineer/eu6f6h5/
1563282027.0,16-Jul-2019 06:00:27,dojoteef,MachineLearning,Are you using [sparse tensors](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)?,/r/MachineLearning/comments/cdjhb6/d_why_is_tensorflow_so_slow_compared_to_ffm/etwrijw/
1563281532.0,16-Jul-2019 05:52:12,dojoteef,MachineLearning,"Wow. I’m ashamed that nearly everyone on this thread has slammed the research chip Intel created while simultaneously misunderstanding its fundamental purpose and design. The chip is a hardware implementation of [Spiking Neural Networks](https://en.m.wikipedia.org/wiki/Spiking_neural_network). One of the amazing aspects of the chip is its energy efficiency. While this Engadget article appears poorly written, there is no need to hate on a company trying to push the field forward. Personally, I don’t conduct research in evolutionary or biologically inspired methods, but certainly respect that others do.",/r/MachineLearning/comments/cdsnm2/n_intel_neuromorphic_chips_can_crunch_deep/etwqxsm/
1562386264.0,05-Jul-2019 21:11:04,dojoteef,MachineLearning,"If you aren't limiting yourself to images, take a look at the following NLP papers: http://arxiv.org/abs/1803.03382 & https://arxiv.org/abs/1805.11063. They also build off of the discrete latent codes (including VQ-VAE).",/r/MachineLearning/comments/c9ew68/d_learning_a_prior_on_the_latent_variables_for/et0u054/
1561219860.0,22-Jun-2019 09:11:00,dojoteef,MachineLearning,"This is a GREAT tool (as is Semantic Scholar). I’m starting a new research project and surprisingly, the Semantic Sanity search worked even better at finding relevant research than Semantic Scholar has done for me in the past. The live feed updates are super helpful. Please keep up the great work!

&nbsp;

One small suggestion is to make the default feed timeline configurable per feed (right now it seems to always default to one week).",/r/MachineLearning/comments/c3hkvt/p_semantic_sanity_an_arxiv_sanity_based_research/ersoeqi/
1560742632.0,16-Jun-2019 20:37:12,dojoteef,MachineLearning,"I had this same issue recently. I started working on a project around Sept of last year. We had a reasonably clear idea of what we wanted to try, but kept running into issues making it work. By February of this year I was seriously considering giving up on the project. Luckily I was able to get things figured out which resulted in [my first paper accepted to ACL](https://arxiv.org/abs/1906.02780)!",/r/MachineLearning/comments/c1gb4t/d_do_or_have_you_ever_work_on_a_project_for/erdhy3b/
1560437282.0,13-Jun-2019 07:48:02,dojoteef,MachineLearning,"I wish the paper went into mathematical detail to show the equivalence of the various frameworks. Section 4 of the [original GAN paper](https://arxiv.org/abs/1406.2661) discusses the theoretical underpinnings of GANs, including its relationship to the Jensen-Shannon divergence. Does the training criterion for Predictability Minimization (PM), or Adversarial Curiosity (AC), actually achieve a minimization of the Jensen-Shannon divergence at it's global minimum?

&nbsp;

I guess, I hoped the paper would have stronger theoretical underpinnings, such as the works unifying [exponential families](https://en.wikipedia.org/wiki/Exponential_family) or [alpha-divergences](https://en.wikipedia.org/wiki/Rényi_entropy). Considering this paper does not show a strong mathematical connection between PM/AC and GANs, this leads me to wonder if expecting the original GAN paper to do so was unreasonable to expect. It seems more likely that a clear mathematical relationship between GANs & PM/AC would be expected from a later analysis similar to the fact that the Binomial & Poisson distributions predate the notion of Exponential families. No one would fault early statisticians from not seeing a direct equivalence between those two distributions. Maybe a future paper will more clearly draw the equivalence.",/r/MachineLearning/comments/bzldq4/r_190604493_unsupervised_minimax_adversarial/er1w6m0/
1558930279.0,26-May-2019 21:11:19,dojoteef,MachineLearning,"It does not appear that it's on a held-out test set. Rather they are plotting the the test set performance over time during training, and choosing the peak performance as the test accuracy being reported. Instead, they should be either use a small validation set in order to determine when to stop training or cross-validation in order to assess the test set performance.


That's one issue I would correct in the paper before submitting to a conference or journal for publication. Otherwise, it's certainly interesting to see new approaches. I'd love to see if it generalizes to more complex datasets.",/r/MachineLearning/comments/bsd2nx/research_the_convolutional_tsetlin_machine_peaks/eoxx9e1/
1548370429.0,24-Jan-2019 14:53:49,dojoteef,MachineLearning,"1) Are there any plans to train an agent using only pixel inputs and physical mouse/keyboard actions now that you have demonstrated AlphaStar's current ability against professional players?
2) Have you gained any insights from this experience that you believe translates to other reinforcement learning problems where humans interact with AI-controlled agents?",/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/eevqw4r/
1534415680.0,16-Aug-2018 03:34:40,dojoteef,MachineLearning,"> Our proposed solution, sticky actions, leverages some of the main benefits of other approaches without most of their drawbacks. It is free from researcher bias, it does not interfere with agent action selection, and it discourages agents from relying on memorization. The new environment is stochastic for the whole episode, generated results are reproducible, and *our approach interacts naturally with frame skipping* and discounting.


The sticky actions paper advocates for not using frame skipping, but seems to take into account that frame skipping is prevalent, thus points out it is supported by their proposed method. See figure 3 as well.


My guess as to why you both are being severely downvoted (I didn’t take part in downvoting) is due to your tone. Emphatically stating someone’s research is worthy of being ignored is pretty demeaning, especially without clear justification. You could make your case with a well reasoned argument instead. I’d even advocate it as practice for actual conference reviews, considering the sometimes terse and aggressive reviews I’ve read on OpenReview.net",/r/MachineLearning/comments/97mz1a/curiositydriven_learning_learning_by_avoiding/e4a9wd0/
1534329739.0,15-Aug-2018 03:42:19,dojoteef,MachineLearning,"One important aspect of a good paper that is sometimes overlooked is the literature review. It can help you place the approach in context and also discover if the idea has already been published before. It may seem far-fetched to consider whether someone has already published your idea, but the field is large and there may be nomenclature you are unfamiliar with that can prevent you from discovering the previous work. It can take practice to do a thorough review.

Best of luck! It’s great that your company is willing to give you time to try the idea out and publish it.",/r/MachineLearning/comments/97c4zh/d_how_do_you_write_a_good_paper_to_spread_a_new/e484rql/
1534312362.0,14-Aug-2018 22:52:42,dojoteef,MachineLearning,Updated my comment. I know one off comments like mine probably aren’t super productive. Thanks for all the hard work you all do on Tensorflow!,/r/MachineLearning/comments/97b84a/d_tensorflow_20_is_coming/e47w82i/
1534276086.0,14-Aug-2018 12:48:06,dojoteef,MachineLearning,"A central focus for 2.0 is eager execution?! The success Pytorch has seen has probably prompted them to reconsider the default execution model.


~~Maybe they’ll also take a page from Pytorch’s book when it comes to defining custom gradient ops. It’s currently a bit of a pain in Tensorflow.~~

EDIT: I stand corrected on custom gradients! Sorry, I think I’ve not had to do it recently and was just remembering the pain points I had when I tried it a year ago.",/r/MachineLearning/comments/97b84a/d_tensorflow_20_is_coming/e46x22k/
1534151663.0,13-Aug-2018 02:14:23,dojoteef,MachineLearning,"Thanks for sharing! It astounds me that so many in the community can be deaf to those who point out their discomfort and feelings of otherness engendered by such episodes.

Industry and academia question why there is such a lack of representation from women and underrepresented minorities in the field. This episode stands as a reminder that the community disincentives those groups from taking part due to ignoring individuals that state specifically what leads to their reticence in engaging more fully with the community. (Let’s ignore the more general cultural/educational issues outside the CS/ML community’s control that precede entering the field).

Please stand up for those that have the courage to come forward! I know I will if/when the situation arises.",/r/MachineLearning/comments/96t5xs/d_informal_poll_for_new_name_of_nips_conference/e43rqij/
1533974531.0,11-Aug-2018 01:02:11,dojoteef,MachineLearning,"The problem with this back and forth is that it’s conflating two different issues. One is rightly expressing how astounding it is to train Imagenet on such a small budget in a small amount of time, while the other is rightly lamenting that this does not directly address the total costs of research. Both are valid issues, but are not directly comparable.


That is, if all you wanted to do was train the given model on a new image dataset from scratch (without transfer learning), then this technique could work wonders. Realistically that’s only something an applications focused company might need to do. This kind of training is likely not generally applicable to all forms of ML research.


If instead you care about reducing the cost of research, e.g. you want to apply the approach to training your novel GAN architecture, then this $40 is not the cost of research. Rather you’d be interested to know how many man-months of work and the total cost of compute needed to research the fast.ai technique.",/r/MachineLearning/comments/96bf22/p_now_anyone_can_train_imagenet_in_18_minutes/e3zymvy/
1533332975.0,03-Aug-2018 14:49:35,dojoteef,MachineLearning,"From my quick reading, an ALN is a NN approach to learn boolean logic, while the linked paper discusses learning generalized arithmetic operations. Saying the two approaches are similar seems a bit of a stretch.",/r/MachineLearning/comments/94833t/neural_arithmetic_logic_units/e3k85jr/
1533113375.0,01-Aug-2018 01:49:35,dojoteef,MachineLearning,"I have a strong suspicion that their new test set does not accurately reflect the original, despite the efforts they went to collect images using the same procedures as the original CIFAR-10 dataset. Image selection is subjective (and they note this subjectivity in their paper). Considering they are not the original researchers who selected the data, their internal notion of the criteria can easily differ from the true criteria used in the original dataset selection process. Heck, even if the original researchers who selected the images were to do so now, they might not truly have the same selection criteria internalized due to the length of time since they collected the dataset. This could explain the gap between test set results.

I do think their paper is a commendable first step to try this type of experiment. I believe if we truly want to conduct this sort of experiment with a much higher scientific bar, it would be done similar to a Kaggle competition, i.e. if researchers release a new dataset, they do not release the true test set (in fact they may have several hold out sets). Rather people report their results on a public test set, then possibly annually, the original researchers run an analysis over the reported models in the literature on a private hold out set. The reason to have multiple private hold out sets would be that it allows this analysis to be run more than once without the potential of people claiming overfitting. I think that would be a very interesting experiment, though quite long term considering the current ML research cadence.",/r/MachineLearning/comments/93gvaq/d_have_we_overfit_to_imagenet/e3ehntb/
1532979511.0,30-Jul-2018 12:38:31,dojoteef,MachineLearning,"A NYTimes article about the research that is plainly written, accurate, and devoid of unsubstantiated hype! Hooray!

https://nyti.ms/2mR9Wxw",/r/MachineLearning/comments/9362f0/n_learning_dexterity/e3b3u46/
1531421616.0,12-Jul-2018 11:53:36,dojoteef,MachineLearning,Without reparameterization you must resort to methods such as Monte Carlo or the straight through estimator which make different bias/variance tradeoffs.,/r/MachineLearning/comments/8ya63k/r_jointcontrastive_inference_and_modelbased_deep/e29sgyo/
1531169525.0,09-Jul-2018 13:52:05,dojoteef,MachineLearning,"While this blog post raises a number of valid points, it misses the similarities between ML and [applied math](https://en.m.wikipedia.org/wiki/Applied_mathematics).

&nbsp;

> Applied mathematics is the application of mathematical methods by different fields such as science, engineering, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models. In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.",/r/MachineLearning/comments/8x8cgx/d_how_should_we_evaluate_progress_in_ai/e22zfix/
1529597087.0,21-Jun-2018 09:04:47,dojoteef,MachineLearning,"It sounds like there is a perception that this practice is widespread, but neither of the papers specifically state that tuning the random seed was actually performed by any of the authors (though there was an anecdotal case mentioned in the thread u/Morninglow brought up).

That said, I see a push to make the experimental results more reproducible by trying to get authors to publish error bars and confidence intervals for multiple runs. Those runs *should* include different randomization. Additionally there are quite a few researchers who are publishing the hyperparameters used in their papers, usually in an appendix. So if researchers do try to optimize the random seed (ugh!), they really should publish that as a hyperparameter.",/r/MachineLearning/comments/8slsiv/d_method_order_of_hyper_parameter_tuning_with_the/e11zkon/
1529592052.0,21-Jun-2018 07:40:52,dojoteef,MachineLearning,"If for example your generative model is a VAE, you simply calculate the ELBO (reconstruction cost + KLD) on the hold out set. As for actually generating a sequence of contiguous primes, that was a toy example that demonstrates the difficulty. The model could simply generate a smooth approximation to the actual prime number.",/r/MachineLearning/comments/8sjh7p/d_what_is_a_relationship_between_entropy_of_data/e11tpjm/
1529528957.0,20-Jun-2018 14:09:17,dojoteef,MachineLearning,"What thread were you reading that gave you the idea that the seed is a hyperparameter that should be tuned? I'd be highly skeptical that choosing a random seed as a hyperparameter would result in a model that generalizes well, which is a crucial aspect of most real world usecases, i.e. not a toy example.",/r/MachineLearning/comments/8slsiv/d_method_order_of_hyper_parameter_tuning_with_the/e10guem/
1529513655.0,20-Jun-2018 09:54:15,dojoteef,MachineLearning,"Just got out of the shower (where the best thinking happens), and realized an even better example of a deterministic data generating process that highlights the difficulty.

Imagine your dataset is a sequence of contiguous primes. It will be very difficult to make a generative model for this data that isn't simply memorization, especially if you test against a hold out set of unseen numbers...",/r/MachineLearning/comments/8sjh7p/d_what_is_a_relationship_between_entropy_of_data/e0zy182/
1529512636.0,20-Jun-2018 09:37:16,dojoteef,MachineLearning,"In a generative model, typically we are trying to match the data *generating* process, which is unknown and has an unknown distribution. Therefore, we do not know what the entropy of the underlying process that generated the data is (except for toy examples).

The difficulty is not in the entropy itself, which is just a measure of how much surprise (or randomness) is exhibited in the distribution. The problem is in matching the data distribution. Even if there is zero entropy in the data generating distribution, you have to somehow discover the process that generated the data, i.e. if there was a deterministic process that generated the data, you would still need to figure out what that process is. A simple example might be a polynomial of degree *n*, with a large *n*.",/r/MachineLearning/comments/8sjh7p/d_what_is_a_relationship_between_entropy_of_data/e0zwrsu/
1529437150.0,19-Jun-2018 12:39:10,dojoteef,umass,"I have also worked in software development for over 10 years. Much of that time was on Windows (though recently my work development environment has been on macOS). You can definitely do most anything you need to on a Windows machine. I often worked in Cygwin with vim, even at work, but personally at home I use a Macbook Pro (with a Linux server for heavy lifting).

If you really want to you can always dual boot the laptop with [Windows and Linux](https://opensource.com/article/18/5/dual-boot-linux), or even [Windows and macOS](https://support.apple.com/en-us/HT201468).",/r/umass/comments/8sas6z/is_there_a_clear_preference_in_the_cs_department/e0y46hh/
1528903648.0,13-Jun-2018 08:27:28,dojoteef,MachineLearning,"A good approach to this problem is quite straightforward. Do not replace report B immediately. Rather have your system operate side-by-side with report B. Then you can gather data on how well your system compares to report B by using a small sample of staff that rely on your system, with report B as a backup. After this second phase of data gathering, try training your model with the new signals. How aggressive you are in removing report B, should be based on the risk involved with incorrectly predicting. Best of luck!",/r/MachineLearning/comments/8qqseg/d_tangled_mess_machine_learning_where_operator/e0lt0fu/
1528421875.0,07-Jun-2018 18:37:55,dojoteef,MachineLearning,"This is best explained by noticing that when the predicted probability matches the true probability the cross entropy is equal to the entropy of the underlying distribution.

That is if you have two distributions, *P* & *Q*, then the cross entropy is defined as H(*P*, *Q*) = -sum(*p*(*x*) * log *q*(*x*)). When *P* == *Q* then this is simply H(*P*, *P*) = -sum(*p*(*x*) * log *p*(*x*)). What you're really doing when you minimize the cross entropy, is minimizing the KL divergence which is KL(*P*||*Q*) = H(*P*, *Q*) - H(*P*, *P*).

So even though the number reported as the cross entropy is greater than zero, the two distributions are equal by virtue of having zero divergence.

Also note, that if it is equally likely that a coin flip will turn out heads as it will turn out tails, i.e. probability 0.5, then that is the least informative result, thus has the highest entropy.",/r/MachineLearning/comments/8pfcne/d_binary_cross_entropy_is_not_symmetric_for_soft/e0ax35r/
1528394295.0,07-Jun-2018 10:58:15,dojoteef,MachineLearning,"This is great! Rather than simply writing a description of the paper, you lead individuals to read and comprehend the salient information needed to understand the paper along with exercises. This approach leads to a much more useful way of gaining a deep understanding. Thanks for sharing!",/r/MachineLearning/comments/8p62mt/d_infogan_depth_first_learning/e0a3yag/
1528393463.0,07-Jun-2018 10:44:23,dojoteef,MachineLearning,"I'm not sure if this addresses your questions, but I did recently read a post that described [continuous approximations of logical functions](http://www.calebh.io/Continuous-Approximations-of-Logical-Functions/) which I found interesting.",/r/MachineLearning/comments/8p6u86/d_formal_logic_in_ml/e0a2xeh/
1527884380.0,01-Jun-2018 13:19:40,dojoteef,MachineLearning,"Interesting disclaimer at the bottom of the page:


> We are not official society and neither associated with NIPS 2018. We provide information independently related to the event and accommodation to the participant.
For More Information Visit NIPS 2018 Official Website. [Click Here](https://nips.cc/Conferences/2018)",/r/MachineLearning/comments/8nu1zh/fake_nips_2018_website/dzykgo7/
1527520317.0,28-May-2018 08:11:57,dojoteef,MachineLearning,"I am all for not being tied to a single strategy. Evolutionary algorithms are useful constructs. My main problem with the post is his line of reasoning. It is not logically consistent and holds a specific biased view that is being argued. Here is one example (of many):


> In this case, the representation (mathematical expressions represented as trees) is distinctly non-differentiable, so could not even in principle be learned through gradient descent.


While strictly true, in practice this is not a barrier (no more so than saying EA is just random search, so why expect it to work for real-world problems in finite time). For example, gradient descent based RL is able to learn policies for discrete action spaces. It all depends on how you formulate the problem. For the particular case considered of finding an optimal mathematical expression for physical systems, I have little doubt it can be formulated and solved in terms of gradient descent. Is that the best way to solve the problem, I don't know, but I do know have a strong suspicion that it is possible.


Fundamentally, this is the problem I have with the dogma surrounding certain approaches to machine learning. It devolves into a passionate philosophical debate seemingly run by emotion, rather than sound judgement. If the article more aptly described the pros and cons of both approaches instead of judging one approach to be intrinsically less-powerful, due to an analogy that is a bit of a stretch, I would have enjoyed it considerably more.",/r/MachineLearning/comments/8mo9t0/n_empiricism_and_the_limits_of_gradient_descent/dzpltay/
1527191840.0,24-May-2018 12:57:20,dojoteef,announcements,"If you want to see the new reddit, click on 'preferences' in the top right. Scroll to the bottom and enable beta features. Then you can go to https://new.reddit.com.

https://i.imgur.com/h2rqjtz.png",/r/announcements/comments/8luiie/fear_is_the_path_to_the_dark_side_introducing/dziqxg3/
1526849132.0,20-May-2018 13:45:32,dojoteef,umass,"I think it will heavily depend on your preferences and what you value, e.g. do you want to live with roommates or alone, amongst peers from your field of study or those who share your background, etc.


For a general overview you can see the trend amongst grad students in the recent [Campus Climate Survey](https://www.umass.edu/diversity/campus-climate), which surveyed the student body. [Figure 7 of the Graduate Student Overview (page 8)](https://www.umass.edu/diversity/sites/default/files/campus-climate-overview-reports/graduate_student.pdf) highlights housing trends for graduate students. The vast majority live off campus. Though you can live on campus in the [graduate apartments](http://www.umass.edu/living/residence/graduate-apartments).


Best of luck!",/r/umass/comments/8kuxzs/what_is_the_best_housing_option_for_an_incoming/dzasg08/
1526659982.0,18-May-2018 09:13:02,dojoteef,MachineLearning,"Many text books are written in a way to cover more topics than a typical one semester course. They often give guidance in the introduction as to what sections to read, and in what order for a given type of course. You can use that as a helpful guide.",/r/MachineLearning/comments/8kdrd2/d_how_do_you_study_from_textbooks/dz6xwox/
1526524577.0,16-May-2018 19:36:17,dojoteef,MachineLearning,That helped make my day! Thanks for sharing.,/r/MachineLearning/comments/8jyw1g/d_against_selective_conferences/dz3zcga/
1526353648.0,14-May-2018 20:07:28,dojoteef,MachineLearning,"The ban makes an exception for cryptocurrency mining, so their current uses are valid. Though, opening it up to ML would violate the EULA.


I wonder if you use an older version of the drivers that does not have the updated EULA if that would circumvent the issue?",/r/MachineLearning/comments/8ji0qy/p_managedhosted_1080ti_gpu_rentals_for_ml/dyzw4rj/
1526338120.0,14-May-2018 15:48:40,dojoteef,MachineLearning,"I'm not quite sure I understand your second point about images from videos, but one way to use all the data for training and still have a decent idea of generalization is to use [k-fold cross validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html).",/r/MachineLearning/comments/8jghbo/d_object_detection_using_cnn_how_can_i_estimate/dyzhjb4/
1525748658.0,07-May-2018 20:04:18,dojoteef,MachineLearning,Yes,/r/MachineLearning/comments/8hs3at/d_how_is_variable_reassignment_handled_in_reverse/dymfh8l/
1525741897.0,07-May-2018 18:11:37,dojoteef,MachineLearning,"The variable name does not matter. That is, in terms of the graph describing the operations, the variable names make no difference. In this case, the value you are actually returning in terms of `a` and `x` is:

`(a*(a*5+10+a)+a)*(x+10)*9`

That is what the autodiff would see and differentiate based on.",/r/MachineLearning/comments/8hs3at/d_how_is_variable_reassignment_handled_in_reverse/dym8l26/
1525640188.0,06-May-2018 13:56:28,dojoteef,MachineLearning,"> I feel like I don't understand why we model X as a random variable in the first place. I think we construct the function f not based on the distribution of X but rather on actual values of X. Why should we care if X is stochastic or not?

X in fact does not need to be modeled as a random variable for image classification. Supervised learning can be done in a non-probabilistic domain with a simple regression model for example.

The reason to model X as a random variable is to allow for probabilistic reasoning. To what degree does the model believe the image is a cat, i.e. what probability is it a cat rather than a dog.

If I showed you a random picture of a primate, you may only be 70% confident that the image is of a bonobo rather than a chimpanzee. By treating the input to the model as coming from a particular probability distribution we confer the model with similar probabilistic reasoning ability.",/r/MachineLearning/comments/8hekzr/d_bayes_error_in_the_context_of_supervised/dyjxctu/
1525486469.0,04-May-2018 19:14:29,dojoteef,MachineLearning,"> The posterior depends on the loss, and the loss depends on the prior. So the finding the posterior requires knowing the prior. So you cannot define the prior in terms of the posterior. Or, that is what I am thinking.

I hope this explains it better. The prior does not need to be fixed as a particular distribution ahead of time. While the stereotypical VAE prior is simply a diagonal gaussian with mean 0, and variance of 1, you can instead learn a prior, just like you learn the posterior. The insight from the paper is that you can try to make the prior distribution look more like the posterior. That is what the linked paper refers to when it states:

> In future work it may prove fruitful to investigate alternative, multimodal priors that can “meet in the middle” with the encoder and decoder networks.

I think your mistake is thinking of the VAE as a directed graph. That the posterior depends on the loss, and the loss depends on the prior such that one must be determined before the other can be determined. Rather the optimization problem can learn both the prior and the posterior jointly.",/r/MachineLearning/comments/8h1peq/d_elbo_surgery_matching_the_prior_to_the/dygzyxc/
1525462373.0,04-May-2018 12:32:53,dojoteef,MachineLearning,"> In the VAE, the only things that are given are the data, and the prior (and the DNN architecture).


I think here is your fundamental misunderstanding. The paper is stating, that assuming a fixed diagonal gaussian is detrimental to minimizing the ELBO. So the prior should not be a fixed given distribution as you've stated. Instead learning a better prior can help minimize the ELBO, as you want the marginal KL to be small (so small in fact as to be zero leading to p(z) = q(z)) if possible.


This fact has been born out by subsequent papers that look to learn better priors (for example [VLAE](https://arxiv.org/abs/1611.02731) learns an autoregressive prior).",/r/MachineLearning/comments/8h1peq/d_elbo_surgery_matching_the_prior_to_the/dyge09t/
1525385255.0,03-May-2018 15:07:35,dojoteef,MachineLearning,"Hmm... so from the wording in the paper they state they empirically chose those particular cutoffs to produce a uniform binning. That implies the actual histogram of depth from the Z-buffer is not uniform (which makes sense considering any particular view of a maze-like environment is unlikely to see off to the edge of the horizon).

So, while you are correct that the resultant values are very small, they are not implying that all potential numbers in the range will be uniform. Only that if they choose to bin them based on the heuristic they gave, it empirically produces a uniform binning. Hope that helps!",/r/MachineLearning/comments/8gtp25/r_nava3c_question/dyenjl6/
1525383633.0,03-May-2018 14:40:33,dojoteef,MachineLearning,"The paper is interesting, in that it reads like a debate over the merits of the approach of using debating AIs to solve AI safety concerns.

I definitely found the arguments they present, both for and against the approach, quite fun to read. The approach is to have AIs debate to ensure that resultant AI decisions align with the intended goals of human judges.

Honestly, this is the first paper on AI safety that I actually found provided some concrete steps rather than resorting to a magician's handwaving on specifics.",/r/MachineLearning/comments/8gssnr/r_ai_safety_via_debate/dyelv07/
1525381471.0,03-May-2018 14:04:31,dojoteef,MachineLearning,"To be honest, typically the easiest part of a job is learning a new language as many languages share a common set of features. Though low-level langauges like C/C++ can be a bit more challenging to master, especially when trying to eek out performance.


> So for right now it seems learning python at a minimum and maybe some basic understanding of tangent languages is the shortest and most obvious path to working with ML.
> Am I showing some misunderstanding of how software development works when I ask questions like this?

Unless you are doing something highly specialized (writing performance optimizers for low-level machine learning libraries), the most important thing you can do is know and understand the foundations of machine learning. I cannot imagine any company or research lab caring how well you know one particular programming language. Rather, the reason to learn Python would be to do some open source projects in PyTorch or Tensorflow that can help demonstrate your abilities.

This comes from years as a lead engineer and an engineering manager (though admittedly not in ML). I've definitely written code in more languages than I care to remember at this point.",/r/MachineLearning/comments/8gsr9c/d_what_development_languages_would_give_you_the/dyejke4/
1525036037.0,29-Apr-2018 14:07:17,dojoteef,MachineLearning,"If you are randomly selecting some youtube videos to train on, then you may not actually have enough structured data to produce meaningful results. It might be better to start with a much simpler dataset, like the [Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/) dataset.


The approach you are utilizing of generating sequences of images from the latent space of a VAE was recently successfully used as part of reinforcement learning for training an agent. See the [World Models](https://worldmodels.github.io) site for details.


If you want even better results, you can look at [Video Pixel Networks](https://arxiv.org/abs/1610.00527), which achieve very impressive results on Moving MNIST.


Best of luck!",/r/MachineLearning/comments/8ftfhw/d_image_sequence_generation_with_vae_and_rnn/dy6iijl/
1524967822.0,28-Apr-2018 19:10:22,dojoteef,MachineLearning,"I think the key is that there are LOTS of free resources online. Video lectures by top researchers from prominent universities, excellent books, etc that you can use to get the basic understanding of the field. Then try your hand at simple projects, and move up to reading research papers and try to either replicate their results or find some small improvement or area of investigation that shows an understanding of the fundamentals of research.


Considering I'm not on an admissions committee, I can't say what exactly helped me gain admission.  I continually hear research is one of the the single most important factors, though. And entering a ML PhD program is HIGHLY competitive by any measure, so demonstrating research aptitude seems key. I applied to 15 of the top US universities and was only accepted at one, while I waitlisted at another. I'm a fairly non-traditional applicant, as I've worked for over a decade in the industry (unrelated to ML), so my admissions results may vary from yours. But from my perspective, it was pretty brutal trying to get in.

Best of luck to you!",/r/MachineLearning/comments/8fmtr9/d_statement_on_nature_machine_intelligence/dy56i6y/
1524952390.0,28-Apr-2018 14:53:10,dojoteef,MachineLearning,"As an outsider that's breaking into ML, open access research has been a key factor that allowed me to conduct research outside of an industry/academic setting. Without which, I'm not certain I would have been accepted into a PhD program to study ML starting this Fall.",/r/MachineLearning/comments/8fmtr9/d_statement_on_nature_machine_intelligence/dy4tnoe/
1524333945.0,21-Apr-2018 11:05:45,dojoteef,MachineLearning,"You certainly don't need a degree to publish, though it can certainly help (especially with making connections). Without connections though, it can be mighty difficult to publish. Good news is that you can always make connections!

If you demonstrate your abilities through open source projects (whether starting your own, or working on an established project) you can make connections.

I had no connections in the ML field, and was trying to break in. I did a number of open source ML projects and sent out my work to people looking for researchers. It worked for me, though YMMV as I have quite a bit of industry experience in software engineering to bolster my resume.",/r/MachineLearning/comments/8dwkme/d_anyone_having_trouble_writing_a_ml_paper_post_a/dxqp3o8/
1523900985.0,16-Apr-2018 10:49:45,dojoteef,MachineLearning,"I've actually tried this with Tensorflow recently (just this past weekend in fact!)


I ran into an issue where the gradient was defined for the entire variable rather than the truncated portion I wanted (which I collected using [tf.gather](https://www.tensorflow.org/api_docs/python/tf/gather). I ended up needing to do a [tf.scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) to ensure the gradients were backpropagated correctly. So I was unable to cut down on  my training time that way. Long story short, while it might theoretically cut down on training time, my experience using Tensorflow did not easily allow for that usecase. YMMV with other frameworks.",/r/MachineLearning/comments/8cdlao/d_eliminating_useless_variables_in_deep_neural/dxgpp8b/
1523851813.0,15-Apr-2018 21:10:13,dojoteef,MachineLearning,What leads you to believe that these weights are resulting in optimizing towards a local minima? I am unaware of research implying  pruning may improve model accuracy or speed training. Typically pruning is done for efficiency reasons and results in minimal performance loss for the network.,/r/MachineLearning/comments/8cdlao/d_eliminating_useless_variables_in_deep_neural/dxfr3bi/
1523776081.0,15-Apr-2018 00:08:01,dojoteef,MachineLearning,"There are a number of papers that discuss this topic. It general falls under the idea of ""sparsity"". See [Sparse Feature Learning for Deep Belief Networks](https://papers.nips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks.pdf), [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149), and [To prune, or not to prune: exploring the efficacy of pruning for model compression](https://arxiv.org/abs/1710.01878) for some examples.


If you use Tensorflow, there is even a [module that performs pruning](https://www.tensorflow.org/api_docs/python/tf/contrib/model_pruning).",/r/MachineLearning/comments/8cdlao/d_eliminating_useless_variables_in_deep_neural/dxe4lzt/
1523736427.0,14-Apr-2018 13:07:07,dojoteef,MachineLearning,"I don't normally jump in the middle of internet disagreements, but in this case for the sake of the OP I think it's important to point out that the typical breakdown is:

&nbsp;

* **training** set is for training the model
* **validation** set is for estimating the model's performance on a hold out set to prevent over/underfitting
* **test** set is for testing the final model

&nbsp;

See this [wikipedia entry](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets) and the associated citation, the book: [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/). And here is the [full first edition PDF of the book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf) in case your interested in verifying the definitions yourself (see page 176 in Chapter 5). I can verify that Bishop's [Pattern Recognition and Machine Learning](http://www.springer.com/gb/book/9780387310732) also uses the same definitions (see page 11 in the Introduction).",/r/MachineLearning/comments/8c3gly/d_should_i_stop_the_training_when_the_loss_began/dxd97ur/
1523484268.0,11-Apr-2018 15:04:28,dojoteef,MachineLearning,"Hmm... this is not affiliated with PyTorch, correct? If so, I wonder if the logo may infringe on PyTorch's trademark.

Also, what's the advantage of using this over PyTorch's own [torchtext](https://github.com/pytorch/text)?",/r/MachineLearning/comments/8bkp89/p_pytorchnlp_lightweight_deep_learning_toolkit/dx7jxjq/
1523225636.0,08-Apr-2018 15:13:56,dojoteef,AskNetsec,Thanks! Good find.,/r/AskNetsec/comments/8asn3n/weird_bank_of_america_website_behavior/dx1dh09/
1523049615.0,06-Apr-2018 14:20:15,dojoteef,MachineLearning,"Admittedly, I just came up with this in the shower (literally) and I only skimmed the paper quickly to answer your question as I was truly interested in the claim of a true posterior.

That said, I'm wondering if this paper suggests that an interesting research avenue is trying to train VAEs with an EM-like algorithm. Maybe the success of GANs can partially be attributed to the notion of the discriminator working off of a fixed representation of the generator at any given time step, thus allowing to better fit the generating distribution... or not, like I said, I thought of this in the shower!",/r/MachineLearning/comments/8a6ody/d_cremer_et_al_inference_suboptimality_in/dwxp2vb/
1523047444.0,06-Apr-2018 13:44:04,dojoteef,MachineLearning,"I think section 5.3 explains their notion of a true posterior:


> To investigate this, we trained a VAE on MNIST, discarded the encoder, then retrained encoders on the fixed decoder: one with a factorized Gaussian distribution and the other with a parameterized expressive distribution. **We fixed the decoder so that the true posterior is constant for both en- coders**. In order to highlight a large amortization gap, we the encoder trained on the fixed decoder had no hidden layers (ie. a linear transformation), which greatly impoverishes its ability and results in a large amortization gap.",/r/MachineLearning/comments/8a6ody/d_cremer_et_al_inference_suboptimality_in/dwxmujv/
1522800837.0,03-Apr-2018 17:13:57,dojoteef,MachineLearning,Thanks!,/r/MachineLearning/comments/89a2jn/p_data_version_control_machine_learning_time/dwriiy8/
1522798577.0,03-Apr-2018 16:36:17,dojoteef,MachineLearning,"Thanks for pointing out the Yup'ik project; that's awesome that they are trying to translate such a low-resource language! I happen to live in Nome, AK so it's nice to see people working on projects related to the local area.",/r/MachineLearning/comments/89i9h8/ps_the_2018_stanford_cs224n_nlp_course_projects/dwrfz66/
1522794661.0,03-Apr-2018 15:31:01,dojoteef,MachineLearning,"Why do you need disentangled representations for the tasks you've specified? Considering the specific tasks, I would consider a VAE with a PixelCNN decoder such as [PixelVAE](https://arxiv.org/abs/1611.05013) or [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731). Both achieve excellent NLL bounds and VLAE additionally makes use of normalizing flows.

Though if you care about disentangled representations, then the papers you cited do indeed provide that capability.",/r/MachineLearning/comments/89f17m/d_current_best_practices_for_vaes/dwrbyj7/
1522790595.0,03-Apr-2018 14:23:15,dojoteef,MachineLearning,"Yes, a ""current"" version of the files (potentially optimized for reading by transforming them into ProtoBufs in Tensorflow for example) are put on a small fast SSD. The files are on the order of 10s of GB. I am currently working with images, but will likely begin doing more nlp as well.",/r/MachineLearning/comments/89a2jn/p_data_version_control_machine_learning_time/dwr7bcb/
1522774643.0,03-Apr-2018 09:57:23,dojoteef,MachineLearning,"I have a separation between my workspace where my code repository exists versus where the actual experiments are run. There may even be a copy phase where data is copied from a slow backing store like a HDD to an SSD before running. That way I can have access to large datasets, but when the experiment is reading data, the data is cached for quick retrieval.",/r/MachineLearning/comments/89a2jn/p_data_version_control_machine_learning_time/dwqmpjr/
1522767931.0,03-Apr-2018 08:05:31,dojoteef,MachineLearning,"I started reading the tutorial and it mentions the use of hardlinks. Unfortunately, hardlinks can only be used if the link is on the same file system as the file being linked. I typically have multiple partitions on multiple hard drives for segregating data and optimizing loading on a fast SSDs. Are there any future plans to support symlinks rather than hardlinks?",/r/MachineLearning/comments/89a2jn/p_data_version_control_machine_learning_time/dwqek0n/
1522549695.0,31-Mar-2018 19:28:15,dojoteef,MachineLearning,You could always try to apply the technique from the following paper [Fast Generation for Convolutional Autoregressive Models](https://arxiv.org/abs/1704.06001). They have [code on github](https://github.com/PrajitR/fast-pixel-cnn) built off of OpenAI's [PixelCNN++](https://github.com/openai/pixel-cnn).,/r/MachineLearning/comments/85oauo/d_faster_pixelvae/dwly611/
1521932575.0,24-Mar-2018 16:02:55,dojoteef,gradadmissions,Not satisfied with the answers [here](https://www.reddit.com/r/gradadmissions/comments/7zx4bc/not_going_to_a_top_ten_school/dus12t6/)?,/r/gradadmissions/comments/86wnon/rankings/dw8dzcv/
1521827152.0,23-Mar-2018 10:45:52,dojoteef,gradadmissions,"I just kept it short, to the point, and professional, e.g. I'm inquiring about my status. Heard from school A, but interested in your school before making a final decision.",/r/gradadmissions/comments/86mbtn/should_i_ask_poi_for_update_on_phd_application/dw66kgn/
1521825516.0,23-Mar-2018 10:18:36,dojoteef,gradadmissions,YRMV: I was in a similar boat (but in computer science) and emailed the POI I was interested in. The POI got back to me within an hour or two and said I was on a short waitlist.,/r/gradadmissions/comments/86mbtn/should_i_ask_poi_for_update_on_phd_application/dw64pdw/
1521825235.0,23-Mar-2018 10:13:55,dojoteef,gradadmissions,"Disclaimer: I was accepted to UMass Amherst for a PhD in CS, but was rejected from UW.

&nbsp;

Rankings are [not absolute](http://www.theexclusive.org/2017/11/cs-rankings.html) and can [vary](http://occamstypewriter.org/scurry/2017/05/16/university-rankings-are-fake-news/) depending on [methodology](https://cra.org/cra-statement-us-news-world-report-rankings-computer-science-universities/). [CSRankings.org](http://csrankings.org/#/fromyear/2007/toyear/2018/index?mlmining) (created by a professor at UMass Amherst) for example tells a different story by only considering research publications in [top conferences and journals](http://www.guide2research.com/topconf/). While there is no category for Data Science, it can probably be seen as having aspects related to data mining.

I do not know if there is an analog of [CSRankings.org](http://csrankings.org) for Statistics programs, but from my investigation of universities during this admissions cycle, UW has a great Statistics program.

Both are strong programs and I think taking a holistic approach by considering potential research opportunities, job prospects, academic culture, etc is best. It also makes the decision more personal based on the attributes you consider most important.",/r/gradadmissions/comments/86j37i/is_the_data_science_program_better_at_umass/dw64dtz/
1520546920.0,08-Mar-2018 14:08:40,dojoteef,gradadmissions,"I applied to a school at the beginning of December and had not heard anything until I emailed the POI I was interested in working with last weekend (beginning of March). He said I'm on a short waitlist. While anecdotal, this could very well be what's going on for you too. Doesn't hurt to ask. I'd just keep the email short and professional.",/r/gradadmissions/comments/830kl7/unofficially_waitlisted/dvedu1q/
1520479643.0,07-Mar-2018 19:27:23,dojoteef,gradadmissions,"I'm guessing it really depends on the program. For example, some schools admit individuals into a PhD program without a masters, but they earn a masters on the way to earning a PhD.

That said, just ask the program to which you've been accepted these questions. They are not going to rescind an acceptance just because you asked if you will need to have completed your masters before starting the PhD. You do not need to explain why you are asking.",/r/gradadmissions/comments/82sz4o/current_ma_accepted_for_phd_should_i_graduate/dvcwgti/
1519506888.0,24-Feb-2018 13:14:48,dojoteef,gradadmissions,"Just read this [great post](http://www.theexclusive.org/2017/11/cs-rankings.html) talking about rankings (talks specifically about CS, but could apply to rankings in general).

&nbsp;
> Rankings are not life. The distinctions that we are talking about here are small distinctions at the very top. The school ranked #100 — I haven’t looked up what it is — is a fine university with brilliant researchers where you will learn a lot. Here’s an analogy. The weakest football player in the English Premier League, who spends most of his time on the bench, is still a prodigiously talented football player who would run circles around anyone who you and I have ever met. The difference between Lionel Messi and that guy — that’s the level of difference we’re talking about here.",/r/gradadmissions/comments/7zx4bc/not_going_to_a_top_ten_school/durut01/
1519123931.0,20-Feb-2018 02:52:11,dojoteef,gradadmissions,"I'm part of the 30+ crowd. I did get one acceptance, and quite a few rejections. Still awaiting a response from half the programs I applied to. Hang in there!",/r/gradadmissions/comments/7ysvbh/where_my_old_people_at/dujbzxn/
1517075159.0,27-Jan-2018 09:45:59,dojoteef,gradadmissions,"Presumably you are going for a PhD? What I understand is that no single criteria will ruin your chances. Rather, they look at the whole application. If your application is strong enough in other areas it may compensate for the failing grade.",/r/gradadmissions/comments/7tdn7w/will_a_failing_grade_in_a_single_graduate_course/dtbt2hi/
1516846657.0,24-Jan-2018 18:17:37,dojoteef,gradadmissions,Thanks for sharing! I remember reading another [analysis conducted back in 2015](http://debarghyadas.com/writes/the-grad-school-statistics-we-never-had/).,/r/gradadmissions/comments/7srxxy/decision_timelines_for_particular_universities/dt76lev/
1516774957.0,23-Jan-2018 22:22:37,dojoteef,gradadmissions,"Some sage advice that helped me calm down (a bit):
https://twitter.com/goodfellow_ian/status/950773136701603840",/r/gradadmissions/comments/7sgbgi/havent_heard_back_yet_dont_lose_hope/dt5lsyv/
1516661642.0,22-Jan-2018 14:54:02,dojoteef,gradadmissions,"Are these specific to all departments in SCS (CS, LTI, RI) or just for the ML program?",/r/gradadmissions/comments/7rqogk/cs_phd_admission_no_interviews_yet_really_worried/dt32rbd/
1516230857.0,17-Jan-2018 15:14:17,dojoteef,gradadmissions,"Not entirely sure where you got the impression that you should frame your interest in graduate school as fulfilling your childhood fantasies...

Take a look at [this blog post](http://www.cs.cmu.edu/~pavlo/blog/2015/10/how-to-write-a-bad-statement-for-a-computer-science-phd-admissions-application.html) from a CS professor discussing what not to do in a statement of purpose. While your field may not be in the STEM fields, I'm sure some of what he states likely holds true for your profession as well.",/r/gradadmissions/comments/7r55qe/what_i_hate_about_the_statement_of_purpose_is_how/dsuahum/
1516027209.0,15-Jan-2018 06:40:09,dojoteef,gradadmissions,"One approach is to make a personal website that links to your research. You can put the PDF of the paper on your website. In your application, include the link to the paper on your personal website. Once you have completed the arXiv submission you can update the link to redirect to the arXiv paper.
That said, I think it is fine to include the paper in the additional docs section of an application as well.",/r/gradadmissions/comments/7qhwq2/sending_a_writing_sample_to_admissions_that_hasnt/dspp58u/
1516025419.0,15-Jan-2018 06:10:19,dojoteef,gradadmissions,"Are you applying for an MS or PhD? While you state you eventually want a PhD, it's not entirely clear if that is the degree you are applying for with this SOP. If you are indeed applying for a PhD, then I would remove the following:

> At graduate school, I plan on taking advanced courses pertaining to artificial intelligence, machine learning, computer vision and image processing, developing my knowledge and skills on related topics, and move on to conducting research in associated fields.

My understanding is that for a PhD they only care about the research you are going to conduct. Talking about courses you plan on taking is superfluous. Explicitly mentioning that you plan on conducting research is unneeded; it's a given considering you are applying for a PhD. Rather I would replace the sentence with some deeper understanding of your motivation/interest in those areas. Why are you fascinated by those problems?

Good luck!",/r/gradadmissions/comments/7qjk3i/please_help_me_find_out_if_this_is_a_good_intro/dspnvpo/
1515963432.0,14-Jan-2018 12:57:12,dojoteef,gradadmissions,"This beginning to sound like a comical progression of increasing # of applications, but I applied to 15 CS/ML departments. I know the feeling. It is very hard not to feel the anxiety despite knowing that I realistically need to wait until the end of January before I am likely to hear anything back.

BTW, good luck to both of you!",/r/gradadmissions/comments/7qcpng/no_word_from_any_program/dsoj1wm/
1515959096.0,14-Jan-2018 11:44:56,dojoteef,gradadmissions,"I don't think they necessarily know how ""hard"" a course was given your transcript. Rather they seem to use some rubrics to decide how impressive the GPA is. Admittedly, [this blog post](http://da-data.blogspot.com/2015/03/reflecting-on-cs-graduate-admissions.html) is from someone chairing graduate admissions for a CS department at CMU, but it seems to lead credence to the fact that they do not use external sources to calibrate the difficulty of a particular grade:

> ***Why ""at a top school""?***  Because it's a very easy way of being calibrated.  We admit a lot of students from CMU, MIT, Berkeley, and Stanford, and a lot of our faculty went to these institutions.  We know the courses, because we also know and collaborate with the faculty who teach them.  We know that an A in 15-410 at CMU is a pretty impressive indicator of systems hacking ability;  that for an undergrad to get an A in 6.856 at MIT is a ""wow"" indicator for undergrad theory ability.  We know what the hard courses are at these schools, and how to read through that to get an accurate estimate of what a student is good at.",/r/gradadmissions/comments/7q94a6/stupid_question_how_do_institutions_know_how_hard/dsoepms/
