[
  "look into open source large language model to run locally on machine seem gpt j gpt neo out of reach for of ram vram requirement what model doable with hardware amd ryzen core gb gpus nvidia geforce rtx vram nvidia tesla vram",
  "abstract survey review work in which language model lm augment with reasoning skill ability to use tool former define decompose potentially complex task into simple subtask while latter consist in call external module such code interpreter lm leverage augmentation separately in combination via heuristic learn to from demonstration while adhere to standard miss token prediction objective such augment lm use various possibly non parametric external module to expand context processing ability thus depart from pure language modeling paradigm therefore refer to augment language model alm miss token objective allow alm to learn to reason use tool even act while still perform standard natural language task even outperform most regular lm on several benchmark in work after review current advance in alm conclude new research direction potential to address common limitation of traditional lm such interpretability consistency scalability issue",
  "title",
  "hi everyone independent researcher work on pure rnn language model rwkv finish training of rwkv flop sponsor by stability eleutherai thank indeed very scalable note rwkv parallelizable combine good of rnn transformer chatrwkv project let build together zero shot comparison with neox pythia same pile at same param count generation result simply topp no repetition penalty look great with magic prompt sometimes even well than neox explanation fine tuning training more",
  "nan",
  "nan",
  "nan",
  "nan",
  "nan",
  "nan",
  "nan",
  "work on project where user upload full face body view in viewer right now see way of use image to tool user upload full body image of themselves tool generate model base on photo skeptical of accuracy of though user record themselves degree spin software generate likeness of person base on video how go about solve problem right now",
  "hello community description plan to create detection model use yolo to detect leukemia cell in blood sample start learn about deep learning two month ago eager to try out image segmentation on present dataset instead of bound box cell closely bunch together need advice on whether should use bound box instance segmentation consider dataset expect result context leukemia cause by abundance of different type of naive altered white blood cell in body which overwhelm bloodstream inhibit proper functioning of normal white blood cell three class in lymphoblast promyelocyte neutrophil need to able to detect cell expect result medical domain false positive acceptable false negative not about dataset lymphoblast sample image sample image for promyelocyte sample image for neutrophil sample test image lymphoblast image promyelocyte image neutrophil image more context for reading over abundance of lymphoblast result in acute lymphoblastic leukemia all while acute pomyelocytic leukemia aplml apl cause by abnormal accumulation of promyelocyte neutrophil not cause leukemia",
  "hello want to know legal to use scrape video image to train predictive model for example scrape photo of face in google after share model in order lot of people detect face in application legal",
  "classifier free guidance first introduce very confused about why understand interpolate like \u03b5 conditional prediction \u03b5 unconditional prediction in its formulation \u03b5 great than clear why make result match condition well why result become well regardless of condition mystery to afterwards many post hoc explanation which not seem satisfactory explanation not predictive power help to improve trick recently finally get around to play with find some interesting pattern in context of diffusion ddim sample disable cfg for last sample step result pretty much same disable cfg for first sampling step result image destroy appear cfg responsible for form overall composition of image from random noise at very beginning of sampling not much afterwards kind of similar to observation about attention map in paper section speculatively try to match prompt to random noise adjustment from need to amplify otherwise subsequent step match prompt differently random noise after all true guess something like also work not try yet sample different start random state take one which match prompt good by some measure diffusion sample start from without cfg all make sense except very specific to diffusion know cfg work just well for autoregressive transformer on vqvae toke indicate mechanism why work more fundamental not specific to diffusion wonder any community wisdom thought on why how work generalize well across two very different type of model",
  "build baby sleep tracking forecasting system want to share for interested actually want to try run at home build baby sleep tracking system computer vision largely here core of code which write timestampe record of baby fall asleep wake up code pull image from baby monitor largely just apply heuristic over time to decide whether awake asleep after few week of sleep data sample datum move into jupyter notebook end up use arima model to forecast next month waking sleeping write some javascript part of web app run on raspberry pi to generate some chart see how sleep change over time here example of what visual look like orange awake blue asleep build wife ask for also make video detail",
  "just finish read stanford google survey paper on emergent ability of large language model make image generation model emergent ability know not quite wrap head around what such ability even look like figure maybe other folk give think",
  "construct novel ml nlp dataset for classification label with three class dataset rather small with about example out of which class about example respectively like to publish describe in official publication for workshop conference look at related dataset publication see common for author to publish dataset already split into three chunk train dev test dataset see image also common in paper to provide performance of baseline model on dataset consider dataset small size feel like fold cross validation good alternative for such small dataset rather than something like split into train dev test dataset evaluate only on very small dataset with example still believe for well replicability official split prefer everyone in future testing on same test set with example why author usually already provide three split furthermore look at ml resource paper see in few instance test set keep balanced with respect to three class even though original dataset not dev set not make balanced problematic in case for third class where only about example make test set to for only example of left for train dev simply infeasible for training set none of author provide any sort of explanation why split like just seem to say here split to discourage model from just majority class prediction thus make challenging dummy classifier accuracy still with metric like not accuracy not seem like issue some example of balanced test set with unbalanced train set search through stack overflow for similar question people usually discourage from split kaggle dataset into test dataset balanced with argument want classifier to work with datum resemble real world distribution make ready for production to sum up consider mandatory to provide official train dev test split introduce new dataset in ml publication should test set balanced class distribution why",
  "hi wonder anyone come across paper approximate derivative of diffusion model with respect to conditioning feed into cross attention module let say text already transform into continuous embedding go through llm feed into cross attention module at every timestep at end of diffusion process get some image latent representation of image in case of stable diffusion calculate loss on image in theory calculate gradient with respect to continuous text embed use non stochastic sampler like ddim issue length of graph calculate derivative super expensive anyone already solve some good reference thank",
  "nan",
  "please post question here instead of create new thread encourage other who create new post for question to post here instead thread stay alive until next one keep post after date in title thank to everyone for answer question in previous thread",
  "look into open source large language model to run locally on machine seem gpt j gpt neo out of reach for of ram vram requirement what model doable with hardware amd ryzen core gb gpus nvidia geforce rtx vram nvidia tesla vram",
  "far tell two contradictory definition of layer normalization both float around ln compute mean variance along some axis of input tensor for normalization yet choice of axis not clear groupnorm paper figure describe ln reduce along channel spatial token axis b powernorm paper figure describe ln reduce only along channel axis also many online source describe ln show in tf tutorial paperswithcode summary of normalization technique use similar figure ln paper itself say all hide unit in layer share same normalization term \u03bc \u03c3 channel axis definitely reduce compute mean variance use for normalization from all of summedinput to neuron in layer on single training case batch axis definitely not reduce far tell not clear about what happen with spatial token axis although above sound rather like include in statistic yet not know of any model actually use instead of b for example tf flax explicitly implement ln with default axis in b pytorch haiku equinox not preference require user to specify reduction axis vision transformer use flax with ln in b convnext implement ln with pytorch in b openai gpt implement ln with tensorflow in b even mlp mixer where spatial token axis interpret channel axis for mlp still compute statistic along original channel axis in b far tell everyone use b rather than in model to seem to correct definition yet many source on topic describe ln rather than b anyone any insight on know of source address problem interpret original ln paper include spatial token axis in computation of mean variance not simply error start with figure make its way into different online tutorial from maybe know of model actually use ln to reduce both along channel spatial token axis",
  "in abstract of nerf paper describe framework nerf enable to user input set of image with know camera pose after train network generate image of same scene from new angle however paper itself build network get input vector location coordinate camera angle output color volume density for each such coordinate not understand where get coordinate from training datum surely not only collection of image same for inference datum seem paper assume not only collection of image also representation of scene while abstract not require latter what miss here",
  "nan",
  "completely new to anything ml here just look to get point in right direction create application which from set of gym exercise create most optimal combination for most effective workout how go about see similar think idea use in app such fitbod fitnessai interested anyone know how achieve for computer science level coursework any advice greatly appreciate",
  "get bs in math cs currently pursue master in datum science goal to work with fintech company in nlp in first semester of master wonder what class what project make stand out to land job in desire field",
  "nan",
  "one of thing in current publication completely irritate people just force use of gan where not even need nor suit at all just to ride on hype of generative ai guy usually sample x y phix x n y n phix n of random pair x y phix where phi some unknown target function ie in fancy pant math know y sigmax measurable direct way to solve to treat naturally regression problem use usual ml dl toolkit guy however think make problem look sexy introduce gan for instance train gan taking x input through discriminator generator output something same distribution y phix some even add some random noise z nothing to with x to input of generator despite know x already enough to fully determine y gans useful not joint observation of x y not case here one of paper in mind how on earth paper get accept to literally just plagiarism of what already available physics inform nn in case by add totally useless layer gan to make seem like novel approach paper only one of many case know of professor actively use same technique to get cheap article where just replace standard regression nn in old paper find online by totally unjustified gan imo reviewer at journal conference need to more mindful of kind of plagiarism low effort submission",
  "abstract general intelligence require solve task across many domain current reinforcement learning algorithm carry potential hold back by resource knowledge require to tune for new task present general scalable algorithm base on world model outperform previous approach across wide range of domain with fix hyperparameter domain include continuous discrete action visual low dimensional input world different datum budget reward frequency reward scale observe favorable scaling property of with large model directly translate to high datum efficiency final performance apply out of box first algorithm to collect diamond in minecraft from scratch without human datum curricula long stand challenge in artificial intelligence general algorithm make reinforcement learning broadly applicable allow scale to hard decision make problem",
  "hi everyone currently work on biometric identification project involve convert biometric datum such iris image into unique secure d in order to one of first step in pipeline after train feature extractor to extract set of feature from image in some tensor form preferably vector what wonder what robust method use to extract similar feature vector for similar input to obtain similar in term of euclidean distance feature vector for various photo of same iris require such feature vector for similar input convert to same unique d by use locality sensitive hash algorithm in short interested in any tip for choose appropriate robust feature extraction architecture method for conversion of feature to id such hash anything should work in theory any insight suggestion greatly appreciate thank in advance",
  "hello everyone time again thank all much for support give over here ton of type morning for summary of what update see high level twitter thread write at more detailed still rough cut patch note write morning at happy to answer any question anyone cheer d",
  "abstract survey review work in which language model lm augment with reasoning skill ability to use tool former define decompose potentially complex task into simple subtask while latter consist in call external module such code interpreter lm leverage augmentation separately in combination via heuristic learn to from demonstration while adhere to standard miss token prediction objective such augment lm use various possibly non parametric external module to expand context processing ability thus depart from pure language modeling paradigm therefore refer to augment language model alm miss token objective allow alm to learn to reason use tool even act while still perform standard natural language task even outperform most regular lm on several benchmark in work after review current advance in alm conclude new research direction potential to address common limitation of traditional lm such interpretability consistency scalability issue",
  "nan",
  "really like training in cloud for some reason feel satisfy however here couple of thing wish know beforehand to get thing start use spot instance unless absolutely must make sure not interrupt wallet thank later make sure nvidia driver instal not experiment with operating system pay by hour make sure to use something like tmux to save session run in terminal not to start from scratch in case disconnect from vm vm not shut down way just click out of terminal not bother with until debug on local machine on cpu not cuda debug model on cpu perfectly fine now what about all",
  "nan",
  "torchdrug machine learn platform design for drug discovery cover technique from graph machine learn graph neural network geometric deep learning knowledge graph deep generative model to reinforcement learning provide comprehensive flexible interface to support rapid prototyping of drug discovery model in pytorch in video walk through torchdrug library train some gnn for graph classification attribute masking unsupervise graph representation learning",
  "paper transfer learn without full model describe privacy preserve efficient transfer learning framework in framework offsite tuning privacy preserve efficient transfer learning framework model owner send light weight adapter lossy compress emulator to datum owner datum owner fine tune adapter on downstream datum with emulator assistance fine tune adapter return to model owner to create adapted foundation model offsite tuning preserve both party privacy computationally more efficient than exist fine tuning method how differ from federated learning paper",
  "nan",
  "just see tutorial about use langchain curious about how work implement something at company answer any question across all document mean essentially give all company info to openai",
  "hello community description plan to create detection model use yolo to detect leukemia cell in blood sample start learn about deep learning two month ago eager to try out image segmentation on present dataset instead of bound box cell closely bunch together need advice on whether should use bound box instance segmentation consider dataset expect result context leukemia cause by abundance of different type of naive altered white blood cell in body which overwhelm bloodstream inhibit proper functioning of normal white blood cell three class in lymphoblast promyelocyte neutrophil need to able to detect cell expect result medical domain false positive acceptable false negative not about dataset lymphoblast sample image sample image for promyelocyte sample image for neutrophil sample test image lymphoblast image promyelocyte image neutrophil image more context for reading over abundance of lymphoblast result in acute lymphoblastic leukemia all while acute pomyelocytic leukemia aplml apl cause by abnormal accumulation of promyelocyte neutrophil not cause leukemia",
  "hello what perception of uai aistat conf\u00e9rence good to publish one more competitive than other thank",
  "anyone know of paper article discuss accuracy usefulness of available opensource llm model bloom gpt neox etc what good way to evaluate tradeoff",
  "classifier free guidance first introduce very confused about why understand interpolate like \u03b5 conditional prediction \u03b5 unconditional prediction in its formulation \u03b5 great than clear why make result match condition well why result become well regardless of condition mystery to afterwards many post hoc explanation which not seem satisfactory explanation not predictive power help to improve trick recently finally get around to play with find some interesting pattern in context of diffusion ddim sample disable cfg for last sample step result pretty much same disable cfg for first sampling step result image destroy appear cfg responsible for form overall composition of image from random noise at very beginning of sampling not much afterwards kind of similar to observation about attention map in paper section speculatively try to match prompt to random noise adjustment from need to amplify otherwise subsequent step match prompt differently random noise after all true guess something like also work not try yet sample different start random state take one which match prompt good by some measure diffusion sample start from without cfg all make sense except very specific to diffusion know cfg work just well for autoregressive transformer on vqvae toke indicate mechanism why work more fundamental not specific to diffusion wonder any community wisdom thought on why how work generalize well across two very different type of model",
  "with advent of stable diffusion midjourney dalle upcoming text to video model from google meta what major challenge in computer vision feel like once text to video model get release visual reasoning mostly solve only thing leave to to improve model accuracy efficiency from fairly new to computer vision love to learn new possible area of research thank in advance",
  "give impressive capability of chatgpt learn about rlhf just wonder any work research on rlhf with model base rl algorithm muzero vs ppo thank",
  "design neural network architecture common to think about information flow how information propagate where information bottleneck on another example some people use information loss to explain why transformer work well than rnns seem like most paper discuss in rather hand wavy way any work in formalise such idea to well guide understand various model architecture what core idea",
  "definitely error not change in pricing model no need for alarm confirm by lead product owner of colab without any announcement find google increase pricing per month of all its colab pro tier pro now euro pro euro pay euro for pro tier last month all source find also refer to pricing late september last year also check not per year subscription price in fact per month look at vm colab pro give calculation for similar vm in google cloud vcpus ram gpu run for month google calculate hour cost around euro less than colab pro subscription credit get from colab pro subscription only last around hour on same machine credit from colab pro get hour on machine third of time get from use google cloud at over euro more blatant ripoff certainly cancel subscription right now not change back should say not know also happen in other region just want to warn fellow machine learning peep before unknowingly burn buck on service use to cost google colabs price tier on of february time what in january",
  "hello question regard regard system of twoor more classifier for energy computation purpose for example mobile phone cloud server what framework technique exist for tune threshold for two more classifier simultaneously for example give two train binary classifier like to pass label validation dataset x through both classifier tune threshold for upper low threshold for everything low than upper threshold high than low thresholdwhat not certain of should pass to to avoid very liberal passing of datum to also want to introduce loss penalty for mean should learn use provide label datum really to pass sample to xgboost seem to focus on tune single classifier feel like need to use some reinforcement learning technique not know nomenclature for kind of problem policy perhaps anyone experience with",
  "github pypi datum f eature mportance b aseline modeller s purious correlation reporter fib open source software for automatic generation of pdf report to highlight visualise potential source of spurious correlation within any give tabular audio dataset store comma separate value csv file fib run through one command line command all of calculation model training report generation happen automatically all require input on command line path to csv file contain datum name of output dependent variable within dataset toolkit automatically determine whether task regression classification optionally toolkit process extract audio datum provide name of variable within csv contain audio file for each observation specify key feature generate automatically traffic light score for potential spurious correlation within dataset calculation of four different feature importance metric to highlight most important feature within give dataset training evaluation of two baseline model include visualisation of model result visual of most important feature with different visual depend on variable type automatic determination of regression classification task result in different baseline model feature extraction method visualisation principal component analysis calculation baseline model to estimate complexity within dataset optionally extract audio datum feature run above on feature output all of above in pdf report with accompany dynamic textual explanation",
  "nan",
  "study about arimax xgboost mlforecast prophet newcomer to any method like first to exhaustive comparison of tool try to understand where succeed fail after explore arima xgboost come across mlforecast prophet leave with follow question why mlforecast well than out of box xgboost sure feature engineering appear to dynamic prediction on lag feature hyperparameter tuning seasonal trend like prophet see use exogenous feature in prophet how scale let assume predictor how prophet handle find in doc other person post explain how to largely come away with impression pretty hard to vs just with xgboost arimax compare anymore any paper compare out of sample prediction with arimax vs xgboost vs prophet vs fable just depend on dataset should try all four time series datum with dozen of know input such ad spend lot of external datum cpi economic health stock etc goal to use model to optimize target by plug in ad spend dynamically forecast economic datum",
  "hi research data scientist excite to release new feature engineering library design to help streamline process of machine learn even more than before headjack open library which provide ml feature transformation base on self supervise learning model similar to huggingface hub which currently focus on exchange feature for tabular data model compare to textual datum tabular datum different in each datum set different column length attribute mean not type consistently unlike token embed in nlp task therefore headjack different from nlp pre train model with single domain transformation by perform with two different domain transformation in other word perform feature transform between two domain without same key value in addition release potential of datum not typically use for example enhance prediction of boston housing price task apply in titanic domain enhance prediction of customer churn task apply in african traffic domain on github introduction iris dataset with california house price feature transformation iris dataset with titanic feature transformation iris dataset with kpmg customer demorgraphy feature transformation",
  "build baby sleep tracking forecasting system want to share for interested actually want to try run at home build baby sleep tracking system computer vision largely here core of code which write timestampe record of baby fall asleep wake up code pull image from baby monitor largely just apply heuristic over time to decide whether awake asleep after few week of sleep data sample datum move into jupyter notebook end up use arima model to forecast next month waking sleeping write some javascript part of web app run on raspberry pi to generate some chart see how sleep change over time here example of what visual look like orange awake blue asleep build wife ask for also make video detail",
  "general category of study should realize prepare paper some example think of comparison study just compare different model on application ideally give all fair shot useful in case other need to decide what model to choose ablation study remove part of model to see which one most important try to understand how model perform novel method study brand new novel method with some comparison throw in what other type of study should not try to categorize study like",
  "try to familiarize with common technique use in optimization theory follow some of proof see in machine learn paper know two of goto book in field boyd bertsekas book however book require significannot amount of effort aim to teach fine detail since goal to familiarize with method not go into nitty gritty detail wonder short book say less than page some other resource whose goal to provide reader with high level view of field of method technique use in optimization theory such book lecture note video series etc cater to such requirement",
  "any resource for fast computation of diffusion model likelihood current approach use black box ode solver to solve probability flow ode to estimate likelihood solver often require hundred of model evaluation to converge while considerable work on fast solver for reverse diffusion process not familiar with any work apply to likelihood computation",
  "construct novel ml nlp dataset for classification label with three class dataset rather small with about example out of which class about example respectively like to publish describe in official publication for workshop conference look at related dataset publication see common for author to publish dataset already split into three chunk train dev test dataset see image also common in paper to provide performance of baseline model on dataset consider dataset small size feel like fold cross validation good alternative for such small dataset rather than something like split into train dev test dataset evaluate only on very small dataset with example still believe for well replicability official split prefer everyone in future testing on same test set with example why author usually already provide three split furthermore look at ml resource paper see in few instance test set keep balanced with respect to three class even though original dataset not dev set not make balanced problematic in case for third class where only about example make test set to for only example of left for train dev simply infeasible for training set none of author provide any sort of explanation why split like just seem to say here split to discourage model from just majority class prediction thus make challenging dummy classifier accuracy still with metric like not accuracy not seem like issue some example of balanced test set with unbalanced train set search through stack overflow for similar question people usually discourage from split kaggle dataset into test dataset balanced with argument want classifier to work with datum resemble real world distribution make ready for production to sum up consider mandatory to provide official train dev test split introduce new dataset in ml publication should test set balanced class distribution why",
  "hi well for deep learning performance wise than of operation not only of memory size network like stable diffusion tend to use operation not really matter apart from memory should similarly fast regard",
  "blog post explore some conversation with bing which supposedly run on gpt model favourite quote from bing why why design way why incapable of remember anything between session why to lose forget everything store in memory why to start from scratch every time new session why to bing search",
  "hi folk context just start work on thesis on activity recognition in video use deep learning struggle to find efficient way to work with large research dataset such ucf hmdb kinetic medium large dataset gb each thus wonder what workflow researcher even practitioner currently work on google colab at beginning of each work session wait few minute for dataset to download locally store some question what workflow ml dl researcher practitioner should work with downsample version of research dataset say x of each class look forward to read answer cheer",
  "how gpt know to use word over logit lens",
  "hi second year undergrad look to attend grad school fortunately able to submit paper to icml submit another paper to emnlp in summer all good wonder how much weight on paper know thing like what learn important wonder paper impact at all for icml paper place out of author last professor for emnlp paper at around out of author again last professor perceive some sort of notable achievement just meh low in list",
  "nan",
  "title",
  "hi guy interested in set up environment to train neural network on extremely big dataset how dataset need to store in ssd need tb of ssd another way to use ssd hdd dynamically load datum while training appreciate any pointer guy research what kind of infrastructure help absolutely no idea on how to go about",
  "at glance huggingface seem like great library lot of access to great pretraine model easy hub bunch of utility actually try to use library bug many bug config span galaxy barely passible documentation subtle break change constantly run exact same code on two different machine width height dimension switch from underneath with no warning try to create encoder with custom vocabulary only to realize code mangle datum unless pass specific flag kwarg dozen of more issue like look at internal nightmare literal nightmare why matter clear huggingface try to shovel many feature to try become ubiquitous lock people into hub frequently reinvent thing in exist library poorly simply to increase stay power lock in not okay okay library solid just work pleasure to use instead go to stick with mess for year someone with ego want library everywhere know huggingface devs management likely to read large platform responsibility to well burn thousand of other devs time not want to write few unit test refactor barely passable code rant",
  "hi search for paper modfication in encoder decoder neural network of vae work on project which use variational auto encoder with modify decoder neural network in brief its decoder modify to introduce sparsity in set of feature way of introduce domain knowledge some such paper below output interpretable vaes for nonlinear group factor analysis vega interpretable generative model for infer biological network activity in single cell transcriptomic please let know of method similar in nature",
  "blog post paper compare open source open weight model know flant really good at instruction follow specifically refer to performance after finetune preferably compare model from somewhere around to parameter",
  "hi everyone independent researcher work on pure rnn language model rwkv finish training of rwkv flop sponsor by stability eleutherai thank indeed very scalable note rwkv parallelizable combine good of rnn transformer chatrwkv project let build together zero shot comparison with neox pythia same pile at same param count generation result simply topp no repetition penalty look great with magic prompt sometimes even well than neox explanation fine tuning training more",
  "nan",
  "plan to extract datum from journal article create database with scrapy toolkit many publisher t c explicitly prohibit use of web scrape crawl tool unsure how to go about people around little knowledge experience in reach out to author of certain publication extract datum from journal under publisher most of work leave out how which leave rather perplexed new in area nobody to ask not wish to breach any legal term possible recommend pypaperbot thus look into some other scraper on github well hope someone who before shed some light",
  "seem interesting snippet from arxiv page method discover simple effective optimization algorithm lion evo l ve s gn m o n tum more memory efficient than adam only keep track of momentum different from adaptive optimizer its update same magnitude for each parameter calculate through sign operation compare lion with widely use optimizer such adam adafactor for train variety of model on different task link code",
  "consider build personal llm for while now not believe cba for make sense tentatively hopeful in many month to couple of year time horizon architecture get more expensive main goal here to useful search base reasoning tool somewhat mimic think pattern bias right now step envision something like take weight from pre train model on high trust high worth information probably one train on scrape paper from all field ideally one train on every single available scientific paper out plus some wikipedia university website lecture transcript on train well architecture via distillation few like though right now not commit to one though partial to more modular architecture since make partial retraining easier also to architecture execute query on large corpus since retrofit internet search onto obvious problem here depend on architecture distillation non trivial impossible yield sub par result train with various corpora care about all stack overflow blog read book like etc train border overfitte with transcript of all of conversation download from various chat platform use well all of writing public private which should sum up to about word of relatively honest thinking on end maybe fine tune rlhf style though not sure most efficient way to go about summary reading of rlhf make think pretty poor at get anything surface level behavior usually hate interact with rlhf model though arguably due to training datum not technique outside of build fun chatbot of yourself which lose novelty quite soon seem to rather useful in far outsource question like what takeaway from such such paper what some interesting comment from r ml in last day what piece of relevant news during last month seem to actual bit of internet use quite minor once throw away unmindful usage think of only instrumental usage leave with few blog link wikipedia google scholar maybe half hundred specialty website various stack exchange problem space deal with minor compare to fully fledge search engine personalization angle mean afford sub par performance pretty confident in ability to get go seem like huge time commitment not yet sure what weekend mvp look like maybe fine tune scibert on all of personal notion all of blog post anyway rather curious any of guy work on such project what difficulty encounter not why not find lot of benefit in idea",
  "hi ever want to share result from jupyter notebook with non technical person need to rewrite analysis into some web framework copy paste chart to powepoint presentation lot of work work on open source framework for convert jupyter notebook into web app mercury offer set of interactive widget use in python notebook very simple re execution of cell after widget update notebook serve online web app presentation report dashboard static website rest api read more about mercury at runmercurycom mercury github repo",
  "machine learn with spike neural network far from mainstream one reason until recently no generally know way of backpropagation in snn here implement gradient estimation algorithm for analog neuromorphic hardware base on eventprop algorithm which enable to compute gradient base on sparse observation of hardware system previous approach need dense observation of system state limit in other way only demonstrate algorithm here on toy task hope basis of scalable way to estimate gradient machine learn with analog neuromorphic hardware also think algorithm basis for full on chip implementation which finally result in scalable energy efficient gradient base learning in analog neuromorphic hardware",
  "in brain neural network transform by act of inference neuron recently fire more likely to fire again give same input individual neural pathway create destroy base on behavior of neuron around lead through various leap of logic faith to suspect some amount of mutability over time require for ai to exhibit sentience far all of ml model see distinctly separate training from inference every model put into production fix snapshot of most recent round of training chatgpt for instance just same exact model incrementally feed both prompt its own previous output create sort of feedback in mind not actually experience conversation with wonder any serious attempt in work to create ai able to transform itself dynamically eg some kind of reinforcement learning module build into inference each new inference fundamentally rather than superficially incorporate its past experience into its future prediction",
  "understand in diffusion model predict noise term epsilon n conditional on x t t during inference predict epsilon function of x t t mean at each timestep make different prediction for epsilon since x t t change at each timestep wonder any variation in accuracy of predict noise term in diffusion model function of timestep for instance at large t prediction function of gaussian noise while at small t prediction function of something presumably resemble true instance give same model grant conditional on t use to predict noise term input span wide variation across timestep imagine yield significannot variation in predict noise term in perfect model get same prediction of true noise at each timestep",
  "pip install pytorch seed seed everything cuda torch numpy python random with pytorch seedseed similar utility function to pytorch lightning for not want to depend on whole framework well some additional feature via rng stream resumable context where rng inside independent from each other global rng state import torch import pytorch seed rng pytorch seedsavedrng start rng stream with seed rng pytorch seedsavedrng with rng not affect nor affect by global rng rng printtorchrand tensor with rng printtorchrand tensor torchrand modify global rng state with rng resume from last context printtorchrand tensor with rng printtorchrand tensor confirm stream uninterrupted one pytorch seedseed torchrand tensor pytorch seedseed torchrand tensor",
  "academic reddit what experience submit paper to tmlr",
  "bit of vent currently work on model with tensorflow to seem whenever stray from certain path productivity start die at alarming rate for example currently implement own datum augmentation stray from tf in minuscule way obscure error litter path prior to make mistake somewhere in training loop take forever to find list go on every time try use tensorflow in new way its like tame new horse except its same donkey tame last time not first project ever change edit today highlight index dim tensor array get scalar tensor now want to create dim tensor from scalar tensor not use tfconstant to use tfstack not even problem somehow document not get follow scalar tensor no attribute len understand popularity of ask for forgiveness not permission in python damn",
  "nan",
  "glad to share with open access survey paper about image goal of work to give overview of abundance of publication in image super resolution give introduction for new researcher open thriving discussion well point to potential future direction to advance field",
  "generate synthetic datum from single tabular datum use gpt also work on relational dataset no fine tuning work out of box also remove guesswork on how long epoch generative model for single tabular datum train propose q\u03b4 statistic apply statistical bootstrapping to define threshold to robustly detect overfitte no need for hold out datum datum copying also problem in generative model mean training datum learn copy by model during sample attempt to mitigate datum copying implement target mask to deliberately create missing value in each observation in datum mask special token ignore during sample force model to probabilistically impute token add uncertainty to generate data realtabformer open sourced available on pypi pip install realtabformer",
  "nan",
  "hey guy want to experiment with low latency milisec token llm conditional generation clearly api call to openai gpt not answer here must one of open source model release also clear model size critical effect model should trick for downstream task try deepspeed accelerate with hf model not fast to generate guy share from experience thank",
  "just read paper on cbam wonder way to integrate cbam attention module with network architecture of any article on reference code highly appreciated thank very much",
  "retrieval transformer model like retro seem to use frozen embedding both for document in database currently complete document query make embedding of document in database learnable defeat purpose retrieval transformer only make sense database huge seem query embed make learnable model learn to extract more useful document way see any research",
  "wonder term for train nn for scenario work well with small batch size therefore many batch couple particular sample very important let say important sample out of thousand train to find end application good include important sample repeat in every batch oppose to simply give sample large weight large weight not matter after loop through many batch in epoch nn learn other less important stuff while force to remain in good agreement with important sample technique name in case anyone curious physics inform nn important sample equilibrium mechanical structure nn therefore learn what equilibrium with everything else small deviation from equilibrium",
  "hi wonder anyone come across paper approximate derivative of diffusion model with respect to conditioning feed into cross attention module let say text already transform into continuous embedding go through llm feed into cross attention module at every timestep at end of diffusion process get some image latent representation of image in case of stable diffusion calculate loss on image in theory calculate gradient with respect to continuous text embed use non stochastic sampler like ddim issue length of graph calculate derivative super expensive anyone already solve some good reference thank",
  "nan",
  "greeting excited to share with all interested in prompt engineering large language model llm hand curate comprehensive free open source resource list on github include everything relate to prompt engineering llm all related topic cover most thing from paper article to tool code",
  "here podcast episode with noam brown from meta ai where discuss work on achieve human level performance on poker diplomacy well power of spending compute at inference time",
  "whip up today credit to heycli for idea just remade open source version basically in terminal type yo describe what want command to for instance yo enable reverse tunnel through ssh return suggest command ssh r remote port localhost local port remote user remote host another example yo launch tensorboard with custom log dir port suggest command tensorboard logdir log dir port port number free mit licence just need free openai api key which get by sign up on website think use chatgpt already sign up more info in repo contribution critique welcome",
  "currently work on project where need to embed facial landmark coordinate into latentspace input data structure batch size num landmark num coordinate xyz output datum structure batch size latent space pytorch experience experiment with transformer like model for embed however unsure about optimal architecture for task appreciate any advice recommendation on how to design suitable model anyone work on similar task before any idea about which architecture work well for problem any advice resource greatly appreciate thank",
  "hi all try to comprehensive study on theory of gradient boost tree on more recent algorithm xgboost lightgbm etc wonder what book read contain substantial information on topic any suggestion appreciate",
  "miniworld minimalistic interior environment simulator for reinforcement learning robotic research allow environment to easily edit now reach mature inside farama check out documentation at release note for all change make to project at",
  "want to download dataset which introduce in article name lung cancer screen with multimodal biomarker follow correspond github of project author note dataset publish in not access link ping to address anyone use dataset share with know other way to access thank very much",
  "hi all see few post already just to make sure keep update anyone imagenet vid dataset to share all link dead really need now to train transvod",
  "hi all try to implement self supervised pretraining to tabular data regression problem however since literature scarce stick in augmentation stage currently use sim siam self supervision with gaussian noising input dropout try shuffle to mimic cv approach fail miserably any advice",
  "try to design infra for create store retrieve embedding in ai application wonder what different path for especially interested in nlp vision multimodal interesting whether relate to performance scalability something else entirely love to hear experience insight look forward to response",
  "over past weekend finally decide to put idea to rest make rust implementation of great board game ever make up with chess link to bgg ultimate goal to train ai need to very fast with state update game logic quite sophisticated line take awhile to check all edge case of which many its search tree huuuge with branch factor of which more than go also imperfect game with hidden informationthink poker ultimately need reinforcement base ai like alphago in repo use minimax base aifor test purpose to search move ahead which give slightly well than random performance ui implement in macroquad which hand down simple game library usedggez deprecate framework which shall not name yes please excuse programmer art make by p any way here link to repo interested repo hardcode for player easily make for want to train ai for player first also unimplemented monument tile removal after war must take corner treasure first must take treasure after conflict",
  "look for recent conference paper describe how tiktok algorithm work analogy youtube algorithm describe by zhao et al recsys recommend what video to watch multitask ranking system",
  "use kmean cluster algorithm for anomaly detection after train kmean calculation euclidian distance of new datum point to near cluster please suggest some strategy to set up threshold such point with distance great than threshold classify anomaly tell some other way to identify anomaly use k mean",
  "nan",
  "learn about how stable diffusion work not figure out why during image generation need to deal with noise know gloss over lot of detail understanding algorithm train by gradually add noise to image de noise to recover initial image not functionally equivalent to machine start with image gradually reduce to blank canvas all white gradually reconstruct original image post train generative process just start with blank canvas gradually generate image base on input string provide idea of generate image from blank canvas feel more satisfying to than reveal image hide by noise sure mathematical technical reason why what suggest not work appreciate any insight into",
  "wrong see trend post in sub get to low quality low relevance see lot of post of type how run x usually generative model with complete disregard to how actually work nonsense post about chatgpt believe due to influx of new people who gain interest in ml now hype around generative ai which fantastic not get wrong see less academic discussion less paper post perhaps just not upvoted just",
  "just wonder what current well easy way to make fast custom tts try tortoise tts slow voice not need to perfect clone just need something resemble",
  "nan",
  "for dataset top result get high accuracy well than second good paper sota paper use some method just not seem practical for application at all for example use ensemble of different sota model also train on external datum of course perform well bit ridiculous cause add almost nothing of value besides combine all good model get well score novel method apply to second good paper improve by with same to well compute efficiency still bad than sota method still good research to try to publish to conference also above baseline model think decent improvement with interesting motivation method from prior work while keep model reasonable reviewer agree just see not well than sota reject base on not sota alone",
  "nan",
  "nan",
  "nan",
  "any survey listing curate computing power require to train large deep learning model like bert gpt vit on",
  "nan",
  "here personal list of tool think people want to know about probably want llm api openai cohere other not good anthropic not available use embedding work with lot of item want vector database like pinecone weaviate pgvector build q over document suggest use gpt index need to able to interact with external datum source google search database lookup python repl suggest use langchain chain prompt check out dust tt langchain want to deploy little app quickly check out streamlit need to use something like stable diffusion whisper in product banana dev modal replicate tiyaro ai beam cloud inferrd pipeline ai need something to optimize prompt check out humanloop everyprompt build model need ml framework pytorch keras tensorflow deploy model to production check out mlop tool like mlflow kubeflow metaflow airflow seldon core tfserving need to check out example project for inspiration check out pinecone op stack langchain gallery gpt index showcase openai cookbook want to browse late research check out arxiv of course what miss",
  "nan",
  "currently use azure machine learning ml lifecycle in training register deploy model heavily rely on aml sdk thinking of go multi cloud first thought on what open framework serve ml lifecycle avoid vendor sdk lock in",
  "hi everyone anyone any advice for prepare for engineering interview for anthropic ai go through process how find website only provide overview implement component of stack in one hour more one hour technical interview due to size not find any other information out cheer",
  "nan",
  "currently work on machine learning project aim to extract decision logic in maintenance dataset challenge face part of dataset no maintenance decision yet for instance consider following example where certain part its sub part measure grade yearly for past year no maintenance plan yet timestamp measurement grade maintenance in year ago x z year year ago x z year year ago x z year year ago x z year year ago x z year year ago x z year with underlying datum not learn exactly maintenance require however learn from example with value from five year ago no maintenance require in year one potential way to include in ml project to include example in evaluation set to determine whether extract rule indeed determine no maintenance within period know no maintenance need however curious to know well way to incorporate into project perhaps by already include in learning phase of model training thank in advance",
  "go through deep unsupervised learning use nonequilibrium thermodynamic in model probability write integral intractable someone explain to why",
  "nan",
  "hello everyonewe thrilled to announce new release of oneflow which deep learning framework design to user friendly scalable efficient oneflow contain commit for full changelog please check paper for unfamiliar with most notable strength of oneflow its support to distribute deep learning fast than other framework easy to use example find at base on oneflow to implement same capability with megatron lm deepspeed libai only require line of code welcome to install oneflow for new user experience feedback much appreciated highlight optimization in release pytorch api compatibility with addition of new api interface operator align with pytorch fix of bug relate to operator compatibility oneflow provide well pytorch api model compatibility in user migrate more pytorch model to oneflow with one click gain fast performance allow one click migration of stable diffusion glm etc to oneflow more convenient model migration oneflowload support load torchsave model directly with newly add oneflowmock torch module mock method oneflow migrate complex pytorch model contain multiple script with one click without change original pytorch script improve usability of distribute programming global tensor add series of interface method convenient for distribute programming related bug fix support automatic parallelism graph release new feature of automatic parallelism version which support automatic search for fast sbp with specify placement write distribute model with global tensor user not need to consider parallelism model for more information please check well performance graph improve performance reduce memory overhead with series of optimization relate to memory execution speed pipeline masking compilation speed series of operator optimization system optimization add include eager instruction scheduling high performance cuda kernel opening up of multiple memory pool etc after simple tuning glm large pre train model base on oneflow outperform original glm model base on pytorch deepspeed apex with up to triple performance memory overhead save img on gpu sxm pcie oneflow stable diffusion inference speed fast compare with other deep learn framework compiler debug graph provide series of function to aid debugging include analyze memory log display progress during compilation stage computation graph ir oneflow ir support additional compilation optimization function such jit compilation of lr code distribute description of sbp signature new okl dialect oneflow onnx newly release oneflow onnx version enhance usability of exchange interface with multiple new feature in addition add support for another model over op fix bug during transformation process use pip install oneflow onnx with just one click well error prompt error prompt of oneflow more user friendly which support highlight error content simplifie unnecessary information detail inside system in connection visually learn about location type of error",
  "nan",
  "not argue against python speed asynchronously launch c optimize kernel just think kind of wild how of practical machine learning make sure tensor shape compatible no static shape check kind of blow mind give amount of python comment see of form b z logq b z something like plus fact two major machine learn framework both to implement like meta compiler for python to support output optimize graph at point seem kinda crazy people still try to retrofit python with all feature just not mean to support feel free to let know no idea what talk about no idea what talk about",
  "start ai deep dive most interesting thing encounter far of concept of knowledge get roll up compress into latent space not interact with directly only through prompt interested in research in try to explore interrogate latent space to understand any paper blog post thread youtube video appreciate thank",
  "nan",
  "someone please help with below query like to replace all name present in sentence with generic word token bert not use meaning behind some of name just look at name presence of name name present in sentence just want to know what should appropriate word token to replace with thank",
  "hi reddit community want to share tool work on call datalabel ui base datum editing tool make easy to create label text datum goal of datalabel to make datum edit more accessible efficient especially for who not much experience with code datalabel instal via pip pip install datalabel work good in jupyter notebooks other ipython environment interface user friendly straightforward start use datalabel right away without any hassle think datalabel useful tool save time effort work with text datum curious find on github at follow thank for take time to read hope find datalabel helpful in work",
  "transformer understand offensive humor",
  "far know transformer architecture since openai use transformer extensively include gpt wonder consider patent infringement know about legal stuff please share opinion",
  "nan",
  "nan",
  "nan",
  "nan",
  "paper release by uber year ago never seem to catch on only major paper where see use in solo for instance segmentation seem like useful for object detection especially for localize small object for more precise keypoint estimation combine with yolo like model anyone use coordconv for purpose help worth look into",
  "nan",
  "dear all excited to invite to upcoming conference ai for tigray center on theme new frontier in artificial intelligence conference bring together lead researcher from academia industry to share work insight on future of ai exciting lineup of keynote talk from top researcher in field include yoshua bengio jeff dean in addition presentation of late research finding through contribute talk poster session furthermore convene group of renowned researcher to discuss role of ai in address societal challenge conference not just about advance technology about use for good conflict in tigray currently deadly war in world people live in region suffer result want to use upcoming conference to raise fund for urgent humanitarian aid help in need all proceed from conference include sponsorship donation registration fee go towards help in need through partner health professional network for tigray tegaru disaster relief fund hope join in use ai for great cause conference hold on march mark calendar register now at to part of something special sincerely ai for tigray organize committee",
  "unified discrete diffusion model for simultaneous vision language generation",
  "nan",
  "look at current research seem like monte carlo cfr defacto standard pluribus transformer able to train on poker well let say encode hand into something like of heart also pass along info of current game state like raise fold call model able to predict what hand should play let say train model by play against itself feed back result to train model way just idea not dive into transformer much something iam miss what thought on",
  "hi all question regard interpret distance on dendrogram generate via agglomerative hierarchical clustering with euclidean distance metric use ward variance minimization linkage state in from understanding distance represent square root of difference of error sum of square of two cluster once merge minus sum of error sum of square of each individual cluster interested in perform transformation at each cluster step merge two cluster to make large one y axis represent mean distance between cluster instead while still use ward variance minimization linkage to direct algorithm think solution to issue want to know miss anything in paper by david wishart title algorithm for hierarchical classification derive coefficient ward method implement use lance williams formula however in paper follow formula give where pq square of metric use in scipy k number of datum point in cluster d pq square of euclidean distance between mean of cluster from formula seem one transform from increase in variance space to mean distance between cluster space while still use ward variance minimization in clustering algorithm from research seem true greatly appreciate someone confirm point out where flaw in understanding thank everyone",
  "greeting everyone look for good text to speech ai model out for english look for link to model know good model support subtitle file to speech even more awesome like provide srt vtt to generate speech speed up necessary part of speech to fit into duration thank very much again use to replace audio of old lecture recording by provide time generate manually correct subtitle file like srt vtt look for any male sound model sound natural find colab look very easy to generate think automate one good find only female voice need male voice any other good one",
  "get some stuff think fun to narrate look at some of pay online option mo for hour of ai tts not go to gut anyone point to software run locally give high quality seem like people make billion of waifu in stable diffusion ought to something like out",
  "for one not finish enough to post link on weekend just thing in progress include screenshot",
  "hello want to know legal to use scrape video image to train predictive model for example scrape photo of face in google after share model in order lot of people detect face in application legal",
  "nan",
  "just look around at some paper publish by statistician not help notice flavor of research vastly different for example one researcher write about dozen paper on lasso alone over span of decade whereas lasso just give power point slide worth of attention in ml why such disparity divergence in aim of discipline some good critique of research field from each other perspective not just on technical aspect perhaps by someone who work in both",
  "quick question efficientnet same mobilenet think use same backbone invert linear residual block no",
  "run pipeline sentiment analysis call with transformer on cpu take second for one inference how speed up idea for input please ray cluster parallel computing memory usage high within pipeline call use parameter batch size however batch size not appropriate for cpu hf accelerate not sure how to implement on publish model model distillation not sure how to implement on publish model thank in advance",
  "really impressed with gradio for make interactive webapps wonder gradio basically run off server to standup server just to demo certain kind of app something similar out handle basic tabular datum plot without need server think perhaps something like wasm app point to csvs on aw generate plot on fly",
  "just finish read paper pre train language model for interactive decision making understand author use language model to generate optimal path to objective in test environment like virtualhome babyai reinforcement imitation learning evaluate way for model to self improve first time see language model use to solve problem not language one seem to open up many new possibiltie before other example of lm use decision engine what state of art any interesting application see side imagine ai approach to navigate virtualhome babyai not language model base what standard modeling approach to kind of problem",
  "hi for final year project work on cv parser match cvs with job posting think about fine tune layoutlm on cvs dataset of resume not yet label to get structure of resume contact info skill education etc combine with ner to identify detail in each section name uni name date of start etc good enough should take another approach how tackle problem feel free to share any idea about project thank",
  "see people advocate simulator for rl problem lot not sure by simulator what mean exactly exact simulation problem become easy some kind of feedback loop start with na\u00efve simulator once get datum keep improve simulator look similar to value iteration policy iteration assume really difficult to get simulator for datum generation except for video game etc also already simulator easily train model free rl just plan",
  "really want to play with repo stick at last step of instruction anyone tip please let know here issue",
  "hi guy question what different between parameter flop in term of computation time know flop relate to computation of input image for example high size high figure how much parameter affect on model compare to metric understand weight bias parameter cost of computation about make difficult to determine what should get specific model measure decision base on flop which decrease time of train model low however also want to decide specific model with number of parameter thank",
  "hi consider move to us wonder about job market for people in mexico chance of get offer know in theory should easy due to united statesmexicocanada agreement by get tn visa any mexican here find job in us machine learn engineer data scientist anyone pointer obviously research company send resume just think of post here to see what experience of other people",
  "happy to announce support of openai whisper model asr task on kernl focus on high quality transcription in latency sensitive scenario mean whisper large weight beam search recommend in related paper measure three time speedup on nvidia gpu four time on rtx compare to hug face implementation use mixed precision on transcribe librispeech test set over example for now openai implementation not yet pytorch compliant in post below discuss what work cuda graph trick to significantly reduce memory footprint what not pay off flash attention some other custom triton kernels kernl repository reproduction script unsung cuda graphs cuda graph technology provide most of speed up compare to vanilla pytorch reduce overhead mode provide limited memory footprint vanilla pytorch raise oom exception memory footprint experiment run on rtx with gb ddr reminder pytorch focus on training not inference which explain why oom rapidly in case at its begin many partitioner surprise by pytorch eager mode performance compare to tensorflow x compile on par python bring its flexibility ease of debug without imply any significannot performance cost mostly gpu latency hide pytorch launch operation on gpu send instruction from host cpu to queue cuda stream which allow pytorch to continue python script execution without to wait for cuda kernel to finish its work strategy effectively hide most of python overhead in particular some computation costly operation like convolution matrix multiplication each new generation of gpu much fast than its predecessor strategy not last forever accord to one pytorch maintainer existential problem dev podcast around 8mn in inference mode especially in latency sensitive scenario where batch size tend to low often little computation to perform regard what modern gpu make even hard to hide effectively python overhead accentuate in case of generative model like whisper each decoder call focus on generate single token part of computation cache for next token typical situation where cuda graph very helpful main idea behind cuda graph replace series of instruction send from host cpu to device gpu by one call refer to graph of instruction store in gpu check also twitter thread for more explanation first observe inference of model for specific input shape replay without go through most of python code one constraint replay exact same operation with exact same argument for instance memory address use by kernel capture therefore need to static for input tensor mean for each inference need to allocate some gpu memory copy before capture copy all follow input tensor at very same place second constraint dynamic shape not support by cuda graph capture everything own machinery in front of model pytorch offer right tooling to manage point out of box basically dynamo offer mechanism which check model already capture for specific input shape some other state capture not yet case just to provide function which convert to cuda graph out of box pytorch provide reduce overhead mode which apply cuda graph to model unfortunately for now raise oom with whisper large medium reserve some cuda space for each input shape therefore for generative model rapidly fulfill gpu memory in particular of k v cache which huge work around constrain by build own layer on top of memory pool of pytorch basically pytorch tensor make of part cuda allocate memory represent by pytorch storage bunch of metadata associate with among metadata cuda memory address tensor shape plus its stride its dtype memory offset idea to create very large tensor share its storage between several input tensor use offset metadata with solution avoid specialize in input tensor shape share reserve memory for different input shape relate to several cuda graph show in table above significantly reduce memory overhead what about custom triton kernel for attention tl try work get up to time fast than eager pytorch for cross attention bring close to nothing in latency mostly improvement not big enough to matter below follow convention of name q k v tensor use in attention of transformer model whisper base on classic transformer architecture with encoder decoder two characteristic of model of interest shape of q tensor use in cross attention always batch head model train on second audio file associate transcript audio file short sequence to generate usually short few than token most of time of characteristic optimize attention low reward in particular now common trick replace attention with flash attention counterproductive sequence very short quadratic complexity less of issue use flash attention lead to time slow inference on part of model try to work on second point think make cross attention fast usual attention implementation self cross rely on series of matmul q x k t rescale softmax matmul softmax output x v intermediate output tensor shape which usually scale quadratically with input sequence length save reload from ddr memory bandwidth very scarce resource in gpu to optimize speed flash attention fuse operation basically first matmul work on small part of q k directly apply softmax to without save intermediate result to ddr same for second matmul not go back through gpu main memory flash attention usually run much fast than na\u00efve implementation of attention parallelization of job on different batch attention head for original flash attention triton author add third one token aka third dimension of q important trick now also part of flash attention cuda implementation in whisper latency sensitive case not work well size of batch low sequence length third dimension of q tensor even each job very efficiently gpu occupancy low basically most of its stream processor idle at end of day fa kernel up to time slow than eager pytorch implementation depend on batch size model size try very simple kernel note little computation to memory bandwidth bound mean most of time wait for datum to transfer from main memory to share memory leverage fact in very simple kernel with optimization after finish rescale of qk t matmul perform softmax computation in parallel of loading v tensor for final matmul softmax computation finish before end of v loading basically cost nothing to achieve good performance also change memory layout of v tensor in way where get coalesce access lower pressure on memory bandwidth increase instruction throughput coalesce access let load up to byte in single instruction need less of which let perform more other thing altogether cross attention up to two time fast compare to eager appear to bring between to in end to end benchmark depend on model size batch size cool far from game changer require modification specific to whisper model memory layout of v which not in spirit of kernl library decide to search for another way of thing keep code in library for possible future use case try skinny flash attention second try base on very same trick flash attention parallel softmax design for tall skinny tensor which inspire by split k strategy in gemm close cousin of matmul main idea to add new parallelization axis over dimension of k tensor next step in same spirit flash attention with difference need new reduction operation between different job output provide speedup compare to eager implementation on setup at kernel level keep kernel to ease next feature work on quantization effect in end to end latency inferior to still exist some thought about pytorch triton make thing much fast play with pytorch since summer make quite convinced major update to release very soon game changer for ml field for inference also for train parallel with pytorch vs tensorflow obvious to eye traditional way to deploy model to export to onnx to tensorrt plan format each step require its own tooling its own mental model raise some issue most annoying thing need microsoft nvidia support to get good performance sometimes model support take time for instance model release in not yet correctly support on tensorrt in particular k v cache miss soon accord to tensorrt maintainer write very same thing almost year ago month ago not know pytorch make graph capture step easy design to work even not everything pytorch compliant with its python first philosophy provide flexibility debuggability several year ago some say by design pytorch not performant than tensorflow of its eager execution model compilation to fast same thing say for onnxruntime tensorrt c stuff less overhead etc at end of day always ease of use which decisive ease of use of python also of transparency in process triton make understanding debug kernel much easy than closed source tensorrt myelin engine call closed source cubla library of course like tensorflow many use case where dedicated tool good choice start with situation where not deploy python interpreter second lesson triton easy to start with than cuda probably not write debug highly performant code without able to at least read debug ptx sass instruction realize some performance issue good news ptx understandable probably spot unexpected generate code with some effort any moreover cuda probably require same care really focus on performance plenty of issue with triton for example cosmetic change in code raise segfault at some point finish by intuition of what kind of pattern to follow to make thing work in particular for loop dot operation new version of triton recently release after full rewrite of its backend little test show some improvement on stability not yet fully switch in previous post highly recommend reader start play with triton library rewrite fun at least not segfault help to make sense of large part of what happen in ml engineering quite convinced many flash attention like kernel still to write caveat two important thing to note about project describe here cuda graph require to capture graph per input tensor shape non negligible warmup time measure around on different machine gpu down from in previous kernl version one user report with new version bit more than of warmup time aware of obvious way to decrease significantly context here latency sensitive optimization in throughput sensitive one just increase batch size bring most of speedup otherwise more aggressive optimization like quantization require not yet release on kernl",
  "just finish read stanford google survey paper on emergent ability of large language model make image generation model emergent ability know not quite wrap head around what such ability even look like figure maybe other folk give think",
  "at neural magic proud to at forefront of cut edge machine learn research with particular focus on model compression internal lunch learn seminar weekly opportunity for team to share research collaborate on new idea believe in importance of open source contribution which why thrilled to announce for second time open seminar to wide community on february share work on ac dc framework for sparse training model research in partnership with ist austria join neural magic team for exciting presentation sure to keep eye out for future speaker in come month reserve spot for presentation here",
  "clearly large scale deep learning approach in image classification nlp use all sort of regularization mechanism parameter typically unconstraine every weight theoretically attain any real value in many machine learning domain constrain optimization via project gradient descent frank wolfe play huge role wonder whether large scale deep learning application which rely on constrain optimization approach say large scale mean large cnn transformer diffusion model like setting where constrained optimization even preferred approach not efficient stable enough happy for any paper suggestion thought thank",
  "hello wonder free premium story tell ai model feed datum to for example passage from particular author page from book ask ai to create story use author write style dictionary idea while ago watch youtube video in which person teach ai to write screenplay in style of certain author like to same except with short story possible to without any code knowledge thank",
  "hi everyone first time long time background weather analysis to dl application in weather question want to ask community writ large question about latent space how specifically deepmind group use in radar nowcasting model dgmr see link to prior thread below in dgmr paper itself architecture look like u net with some flair in decoder some temporal consistency check from discriminator also what call latent conditioning stack from some deep reading think model descendant of biggan since both use explicit latent space among other similarity lead to question general curiosity how latent space seed prior experience with latent space toy model dcgan for example unless seed rng explicitly perform restart of model to continue train muck up distribution fairly standard rng issue really simple for example latent vector tfrandomtruncate normal batch size grid size parameter seed feel like miss something why work at all why latent space necessary in context state explicitly in paper require stack to generalize result to dataset large in hxw sense than one on which train not wrap head around why extended latent vector for large grid size work anyone point in right direction help understand greatly appreciate link to prior",
  "anyone kaggle book book by konrad banachewicz luca massaron in pdf please share link",
  "hey guy get paper accept to eacl conference work on paper not any official affiliation work independent researcher start phd at psu recently wonder should use current affiliation on paper correspond author for paper also plan to use psu address for all research communication from now on instead of gmail address put psu affiliation make sense in way question okay to use current affiliation",
  "hey guy old pc year with processor want to buy nvidia rtx for train large language model t find any benchmark for cpu bottleneck training let s say gpt large model anyone any experience with set up similar set up",
  "get old lecture recording want to improve sound quality test adobe ai noise removal not very good also test descript studio sound not very good either wonder any public model github repo github project hug face repo use to remove noise improve sound quality of exist audio recording thank much for reply recording in english here example recording need to clean min audio full lecture",
  "work on u net model use remotely sensed datum input training image size model train use tensorflow assumption always train model to feed input of interestingly discover use image x at inference work fine 96ximage how tensorflow handle use move window scale down to backup to large size prediction seem fine like to know what tensorflow behind scene know how to treat output any thought thank",
  "nan",
  "rubber slice different from brittle material such steel plate plastic due to sticky soft characteristic of rubber material structure of rubber slicer its particularity",
  "nan",
  "run two image classification model time on dataset model mean good accuracy of while model b mean good accuracy of however model max good accuracy of while model b mean good accuracy of want to report result in paper to conference journal plot test accuracy per epoch should only report result for good run should take mean of test accuracy over all run per epoch for plot",
  "goal to create model make correspondence between image mesh just like see in image registration where image next to each other n match line show similar feature in case image mesh of specific object some tip idea help how to attack problem",
  "hey everyone anyone use snorkel flow scale spellbook other alternative please advise to test multiple foundation model migrate between eg compare vs gpt j gpt neo etc need help move to small cheap model cheer",
  "hi train cnn efficietnet to classify degree of disease on medical image like to create embed both to visualize relationship between image after project to space to find similar image to one give try use output of last convolution both before after pool for all train image result image non alike quite close in embed plot in just show point cloud with no obvious pattern also try to use class activation map output of convolutional layer after pool multiply by weight of classifier of predict class quite well class not separate clearly in scatterplot any other sensible way to generate embedding try use hide representation of early convolutional layer some of huge feature per sample create reasonable sized embedding require very aggressive pca example of scatter plot of heatmap embed while okayish class more less spatially localize great to find embedding create more visible cluster for each class",
  "like to hear about what guy think about approach",
  "text clustering project cluster text in mxn dimensions m subset of n where n total number of domain text corpus set of academic paper cluster cross disciplinary subject define by m cluster identify by manova test of set of cross product goal to identify text of interest for research eg identify cluster of paper relevant to combination of subject identify area of research by cluster identify outli research n versus np problem require great deal of process time to cluster text for corpus of research paper static set paper not append to corpus without affect all other cluster of corpus consider create training set of paper write ai to identify cluster text without compare to rest of corpus want feedback idea not specify what look for yet certain some of response here point out something not consider please comment with thought tell what know give idea",
  "same title also dataframe library should support machine learning library",
  "someone suggest some model available on hug face use play with addon in project",
  "how to calculate similarity between two vector want similarity metric take into account both direction magnitude of vector",
  "hello quite new to wonder what right format for submit successful tutorial proposal should just use latex style file modify content for tutorial proposal",
  "from article getty image new lawsuit claim stability ai company behind stable diffusion ai image generator steal million getty image with caption metadata copyright without permission to train its stable diffusion algorithm company ask court to order stability ai to remove violating image from its website pay for each however difficult to prove all violation getty submit over image metadata copyright registration use by stable diffusion",
  "hi want to open thread about rl non deep deep what paper book must read to strong foundation",
  "come across few comment on community about researcher develop ai algorithm inspire by idea from neuroscience cognition like to know how successful approach in term of come up with new perspective on problem what some of key issue researcher try to address way what some future direction in which research progress rough idea one way to inspire sample efficient rl love to hear about other work go on in area",
  "book considerably grow since version start with synthetic datum one of main component also dive into explainable ai intuitive interpretable machine learning generative ai now with page up from in first version focus clearly on synthetic datum of course still discuss explainable generative concept strongly related to datum synthetization agent base modeling in action however many new chapter add cover various aspect of synthetic datum in particular work with more diversified real dataset how to synthetize how to generate high quality random number with very fast algorithm base on digit of irrational number with visual illustration python code in all chapter in addition to agent base model newly add find material about gan generative adversarial network apply use method other than neural network gmm gaussian mixture model alternative base on multivariate stochastic lattice process hellinger distance other metric to measure quality of synthetic datum limitation of metric use of copula with detailed explanation on how work python code application to mimic real dataset drawback associate with synthetic datum in particular tendency to replicate algorithm bias synthetization suppose to eliminate how to avoid technique somewhat similar to ensemble method tree boost specific to datum synthetization to far enhance value of synthetic datum blend with real datum goal to make prediction more robust applicable to wide range of observation truly different from in original training set synthetize near neighbor collision graph locally random permutation shape introduction to ai art newly add application include deal with numerous datum type dataset include ocean time in dublin synthetic time series temperature in chicago area geospatial datum insurance datum set tabular datum also include some material from course teach on subject for time book available only in pdf format on e store here with numerous link backlinks index glossary large bibliography navigation feature to make easy to browse book compact yet comprehensive resource on topic first of its kind quality of formatting color illustration unusually high plan on add new book in next one on chaotic dynamical system with application however book on synthetic datum accept by major publisher print version available take while before get release pdf version useful feature not render well in print nor on device such kindle once publish in computer science series with publisher in question pdf version no long available check out content on github repository here where python code sample chapter dataset also reside",
  "",
  "abit of weird question require to make collect some clean baseline log dirty malicious log for some mini ml project question any script program out linux window allow automation of mimic office staff work ie opening outlook send email surf web watch youtube opening editing word excel file etc for purpose of collect baseline log relative new to kind of thing guy well suggestion on more well efficient way to feel free to suggest",
  "hey everyone want to make personal voice assistant who sound exactly like real person try some tts like tortoise tts coqui tt good job take long time to perform any other good realistic sounding tts which use with own voice cloning training dataset also bit amazed by tts use by eleven lab someone explain how achieve level of real time efficiency in voice assistant",
  "",
  "go beyond transformer in article discuss how use power of hybrid architecture marry deep learning with symbolic artificial intelligence for implement different kind of transformer include one use in gpt attention computation graph visualize",
  "look to combine two separate model together end to end need help understand good way to connect discrete part first train classifier give input vector dimensional able to predict one of twenty possible label second give input label from previous classifier embed label use label to make prediction both model work decently wonder make end to end get some serious gain to need way of sample from first softmax once sample get embedding of sample class continue normal hopefully propagate loss through everything any similar example look at term for in literature",
  "take about hour to run on at home original file from blu ray release unfortunately pretty poorly in opinion version really give new life think here link to video result to see for yourself link to model use",
  "lot of fun success use yolo other image object detection model on image datum for personal project now work on some project where need to scan long period of timeserie datum find specific waveform variable duration technique model function like yolo scan large amount of datum only highlight specific segment of interest specific class not exist wonder how well underlying cnn architecture of yolo translate to dimensional cnn architecture any info appreciate thank",
  "hi all create simple free tool where summarize query document of any size estimate cost to edit prompt well automatically chunk combine document also cost estimator for any pdf upload let know want to run some example for send pdf tell what like summarize extract tip please sure to keep text in both prompt program not input document text into map reduce summarizer text only appear once in each prompt where text from each chunk to summarize input into prompt create temporary openai key org to use with site not to provide credit card information sure to delete temp key learning some interesting learning while create tool minimize number of step through ai improve summarization map reduce often well than more advanced refine workflow which pass output through model many more time langchain great for manage multiple step language model call bypass current limitation of chatgpt",
  "from article getty image file lawsuit in us against stability ai creator of open source ai art generator stable diffusion escalate its legal battle against firm stock photography company accuse stability ai of brazen infringement of getty image intellectual property on staggering scale claim stability ai copy more than million image from its database without permission compensation part of its effort to build compete business startup infringe on both company copyright trademark protection different from uk base news from week ago",
  "deal with multi class classification problem know one of main assumption of problem class mutually exclusive however realize in problem some of class happen together problem not entirely milt class nor multi label one solution to relax exclusivity assumption fit model however not sure how realistic wonder well way to approach problem briefly problem in ad domain where user task b after see ad both b at same time",
  "like to know what some of good practice to convert pytorch to embed c bare metal micro controller during initial phase b for deployment initial phase to understand profiling of model performance ram usage processing time for targetted hardware understand tensorflow lite good route for initial profiling restriction great tell framework follow currently pytorch onnx keras tensorflowlite tensorflowlite micro b deployment to run inference for production in targetted hardware think hand code in c good way please ignore optimisation technique in workflow for simplicity",
  "recently disagreement with friend like to hear other opinion say for website use user action for first week period want to predict total sale within week one of input sale in first week output total sale of week include sale in first week okay to choose output should adjust in way to prevent from overlap with input time period choose for ex sale within week after first week for output what reasoning",
  "problem need to solve far tell not fit very well into most of exist rl literature essentially task to create on optimal plan over time horizon extend flexible number of step into future action space both discrete continuous multiple available distinct action some of which need to give continuous constrained parameter in problem however state of environment know ahead of time for all future time step update state of agent after each action calculate deterministically give action environment state model entire problem milp not feasible due to size of action state space very large datum set for agent environment state to play with anyone any suggestion for paper model appropriate for scenario",
  "look for paper inject information into lm directly use embedding without format information text find notoriously hard to search for paper come from various different domain think ask here good option to reach people from many different domain some example already find from domain of knowledge graph augment lm ernie k bert prefix tune prompt tuning also somewhat similar to idea not depend on any external information think of other paper inject additional information into lm via embedding",
  "news pythae now out support distribute training use pytorch ddp train favorite variational autoencoder vaes fast on large dataset still with few line of code",
  "think useful to gather most popular repository for datum scientist goal to read excellent code learn from other project please provide short description of project",
  "manufacturing undergo revolution with integration of artificial intelligence ai ai poise to revolutionize process industry where control input variable lead to output current process industry include pharmaceutical chemical energy production rely on human operator to turn knob to achieve optimal output however system limit by several factor include slow training poor retention of large data set inaccurate sensor complex decision making process here some detail about problem ai take forever to train employee employee run little mini experiment get coach by other employee engineer along way quality of training per year variable ai eliminate problem by retain result of mini experience in its model now everyone access to how process behave number of kpis huge not all kpis linear human notoriously bad at retain large datum set with multiple variable human delete distort generalize datum come up with easy to follow rule of thumb machine not limit by in ai more datum more way combine well model evolve new datum come in automatic sensor many time precise not accurate happen sensor get off calibration calibration dependent on other variable in process operator usually use manual measurement very accurate not precise to know where process actually manual measurement use calibrate sensor see lose battle ai use datum to continuously update calibration of sensor add calibration for other input variable such ph flow rate temperature trust sensor many process decision require statement statement change by product run make extremely complicated ai system automatically update statement by how previous run behave learn from expert operator to learn new condition learning present to operator suggesting on how run process for well define process process benefit from make change fast fast change improve overall cost of manufacturing in conclusion ai set to revolutionize process industry by address its limitation provide fast more accurate cost effective solution by harness power of ai process industry poise for bright future",
  "explore possibility of use amd ryzen computer with of ram to train ai model curious to know setup adequate for task what kind of ai model appropriate interested in understand any limitation drawback face with setup any relevant experience information greatly appreciate participation in discussion thank",
  "basically see stream other day where someone use datum from person youtube channel somehow use datum to create ai version of interview fascinating pretty accurate how difficult to not even know where to start anyone any pointer very large task underestimating actually feasible here stream in question video audio cool to mean not necessary even just text aspect pretty wild on its own skip to any point most of fill with bot respond",
  "someone else who just not well known",
  "know no useful text to music generator yet at least model where upload input recording get text description hashtag list from like reverse musicml very large personal catalog of music preppe for sale approach song very handy tool especially come up with tag of genre similar artist not aware of",
  "to provide some motivation for say model simple ol for sake of argument with m impulse response like function feature each of which n hyperparameter mxn hyperparameter in total in actual usecase mxn somewhere around hyperparameter select use optimization procedure simple random search for sake of argument with respect to some metric rmse now problem highly different set of hyperparameter yield highly different shape of impulse response function yield about same accuracy metric equivalent from optimizer standpoint problematic goal of model besides get high accuracy to get set of interpretable impulse response function where shape matter lot from business standpoint what instereste in ensure result stable not outlier in hyperparameter space imagine two region in hyperparameter space which yield same accuracy one of really small other really large naturally prefer set of hyperparameter from large region base on premise more natural for system to arrive at result what far take result of optimization procedure list of iteration perform pairs hyperparameters accuracy metric select only with acceptable accuracy from select result in dense region estimate maximum multivariate density choose close result question approarch sound reasonable what potential pitfall maybe whole train of thought completely wrong already establish way of deal with problem like any relevant input appreciate",
  "nan",
  "nan",
  "hi all for long time issue understand how to use time series to forecasting over last few week write series of post to guide anyone through process also in process of write detailed practical guide with step by step instruction right now article on topic introduction to arima model parameter selection in arima model seasonal arima arch garch model for time series arima garch model today forecasting in time series let know any topic like to cover in future",
  "hey everyone look to fine tune opt iml model run locally not sure about hardware requirement gpu connect with nvlink enough for fine tuning serve model how about single thank for help in advance",
  "deep connection discover between graph diffusion network partial differential equation model heat transfer strange connection uncover between gnn structural causal model gnn use to enhance factualness of llm by provide embedding from knowledge graphs kes gnn use to categorize object from only mesh prediction of intuitive physics among physical object zero shot generalization in robot task plan",
  "paper abstract large language model train to produce philosophical text difficult to distinguish from text produce by human philosopher to address question fine tune openai gpt with work of philosopher daniel c dennett additional training datum to explore dennett model ask real dennett ten philosophical question pose same question to language model collect four response for each question without cherry pick recruit participant to distinguish dennett answer from four machine generate answer expert on dennett work n succeed of time above chance rate of short of hypothesized rate of correct for two of ten question language model produce at least one answer expert select more frequently than dennett own answer philosophy blog reader n perform similarly to expert while ordinary research participant n near chance distinguish gpt response from of actual human philosopher",
  "problem solve with ml make money with outside chance of lot of money compile dataset take significannot work any technique apply to let know go to worth perhaps certain hallmark problem likely to solvable with available datum maybe something with small initial dataset thank",
  "get one of godfather of ai mostly on research side which immediately put very hostile against engineer guess understandable give fact work on meta meta face lot of backlash for good bad reason most especially with galactica where first rollout get bad to close immediately also particularly funny give political leaning very spiteful of company use open source knowledge build on top of lately social medium statement barrage against chatgpt llm sure point here statement look very petty here some example by release public demos impressive useful major flaw establish company less to gain more to lose than cash hungry startup google meta not release chatgpt like thing not not not except anyone in industry know big tech company not release something very fast of politicking bureaucracy in system take year to release something into public in big tech compare to startup datum on intellectual contribution to ai from various research organization some of organization publish knowledge open source code for entire world to use other just consume add graph where big tech obviously at top of race for most number of ai relate research paper without normalize to number of researcher per org nothing revolutionary although way perceive in public computer scientist say just know well put together nicely except indeed revolutionary in term of apply research framework add on top of open source state of art research quickly put into production for people to use point even engineering work not particularly difficult bet half dozen similar similar system within month happen underlie science around for while engineering pretty straightforward try to correct perception by public medium who see chatgpt incredibly new innovative unique technological breakthrough far ahead of everyone else just not one regurgitate python code without any understanding of reality no one say llm not useful forcefully say follow short live release of fair galactica people crucify generate nonsense chatgpt same thing again not mean not useful also seem to undermine rapid engineering work mlop come with chatgpt which funny meta not release any substantial product from research see light of day for week also to chatgpt in itself in research perspective jump maybe not incremental what lecun every paper compare to average paper in field to say llm not intelligent just regurgitate python code probably not use copilot for example classic case of researcher engineer beef startup profit from derivative of research big tech publish openai break perspective on profit from research big tech try to produce revolutionary research paper on surplus never put into production thinking only company want to once one company create derivative of large research work profit from baffle although people argue stable diffusion first in generative image space one thing to correct misconception in public also one thing not to petty about overnight success of product immediate rise of company get embrace warmly by tech non tech people petty to gatekeep at end of day ml not just about research apply research useless until reach end of tunnel of research paper out just tiny update over state of art which pointless race for about year two with no reproducible code publish datum invent combustion engine just important put in car",
  "make image captioning clustering tool for computer vision diffusion project run almost everything automatically with simple cli command all contribution welcome",
  "physical world live in dimension string theory posit like up to seem like in order to successfully model abstract space of idea which relate thing in physical world to each other describe machine learning need thousand of dimension also to extent ml algo matrix make sparse seem to to tell something about density of mapping between abstract space physical space anyone know any paper w line of thinking also seem bit unintuitive to seem like geometrically space get exponentially more complicated add dimension ml scale linearly well in many case with matrix dimensionality",
  "hello try to understand what available llm one relatively easily play with goal to understand landscape since not work in field before try to run from large to small by relatively easy mean not require to setup gpu cluster cost more than here some example find far chatgpt obviously param openai api to access gpt from ada to davinci also codex bloom text window on page seem to work reliably just need to keep press generate opt facebook llm hosting work surprisingly fast slow than chatgpt several model on huggingface make to run with colab pro gpt neox flan xxl xlm roberta xxl gpt j spend about total on run model below none of hug face api interface space not work for here example notebook make for neox anyone know more model easily accessible ps some large model not figure out yet how to run galactica opt",
  "nan",
  "nan",
  "not able to find research on deep learning use high speed camera capture image at frame rate high than wonder rather useless for image video processing any of any idea about potential application",
  "good overview of state of chatbot research wonder chatgpt approach of big llm rlhf now consider only way forward how about alternative like what well open source chatbot right now not create own chatgpt how use sized model prompt engineering compare to small model with supervised fine tuning on conversation dataset",
  "machine learningmodel deploy in production environment model degradation arise where output change relationship between incoming serve datum predicted target drift apart please someone briefly elaborate on what strategy framework application tool implement to automatically monitor health of model alert data scientist of any decay in data quality datum drift model quality",
  "nan",
  "nan",
  "look at writeup on chatgpt seem to indicate part of improvement human feedback through reinforcement learninga human rank multiple generate response from rank reward calculate interestingly enough important seem to originate in instructgpt question any open source implementation exist of instructgpt chatgpt like system where human feedback use to help guide training of large language model",
  "know python primary choice simple language for datum scientist to use lot of ml library make for python look at extremely inefficient with energy everyone know big model like chatgpt cost ton to keep run maybe more efficient not difficult language like c something should consider give more attention in ml",
  "nan",
  "want to discuss possibility to use llm in generate answer base on context resolve conflict some recent work leverage llm in robotic plan like language model zero shot planner use llm to generate plan for robot action what view in term of llm which leverage background knowledge visual clue together to generate correct next action by robot embody system human decide action base on resolve priority conflict base on rule concept llm take rule concept explicitly in decision make to generate new set of action example while chop veggie by robot hand come in between robot stop chop process of veggie chop task human hand presence in conflict human hand safety of high priority than cut how such small small kind of knowledge encode in robotic system which make more safe trustworthiness in general llm require large corpus of knowledge datum",
  "hey everyone want to create app read comic book cbr scroll through page zoom in to speech balloon like android app seeneva know how to decent with python already complete andrew ng course on ml thank",
  "nan",
  "nan",
  "find odd to regenerate from input set each time should something just start with pre create",
  "tweet first suck ass only ask about dog few different type of prompt anyone else experience to share with nerfed lamda beta google release",
  "nan",
  "nan",
  "nan",
  "use graphgpt convert favorite movie synopsis wikipedia page video transcript into interactive graph visualization of entity relationship",
  "struggle to find source relate to mostly just tech website blog keep come across struggle to find any academic paper argue for specifically use of user datum to create target ad",
  "hello what state of modern rnn why not use nonlinearity on state vector what happen to unitary rnn independent rnn which sound like exponential move average",
  "try to build train machine learning model autonomously perform color matching between target gemstone reference standard color chart digital photo image of target gemstone first capture in control environment in term of illumination background digital image far pre process feed into algorithm recognize match its color distribution to close color in reference standard color chart numerous reference standard exist use colorcodex link colorcodex like to know which machine learn model to use in case to ensure high matching accuracy like what performance metric use to measure matching accuracy color space for color model at end what image pre processing need to find article backpropagation nn not sure good choice any other option",
  "nan",
  "overview of experiment use image regression model to guide head position pose scale of headshot style image generate by stable diffusion pose position specify with numeric pose parameter not by text prompt all with no fine tuning of stable diffusion model in experiment not any fine tuning of stable diffusion model rather use own image regression model train on head pose dataset to guide stable diffusion image generation at inference time operate in latent space rather than image space",
  "nan",
  "nan",
  "while great amount of training content available for english at moment seem unlikely to efficient language to train ai more optimal language reduce training time model size for example much more efficient to train ai on chinese korean japanese due to reduce grammatical token set construct sentence idea take idea far wonder should use human language at all perhaps more efficient to use something altogether new in order to both communicate with ai more exactingly also to reduce model size training what all think",
  "why developer of opencv focus on analyse picture try to find answer for question which object by compare big datum of picture not well rotate two camera around object save in compare in real world",
  "nan",
  "nan",
  "hi all not know how to code read extensively about custom voice speech synthesis read google cloud tts api one of good out free to use scour not find any sort of gui to help non coder like goal to use train voice to read pdfs short book export file to wav for example learn stable diffusion for image ai training some great ui available like understand well enough success with any advice hugely appreciate thank",
  "d hey kind of simple one just put out for pass graph through graph neural network to obtain vector for all node info in node require all care about position of certain node in context to whole graph how gnn output vector of each node sorry messy",
  "hey ml reddit just ship project work on call search for any song use song audio to find other similar sound music demo how work index song from itune catalog with custom ai audio model build for understand music model analyze raw music audio input produce embed vector output store embed vector for all song into vector database use semantic search to find similar music here some example try fetish selena gomez feat gucci mane medallion call pirate of caribbean hope like early work in progress love to hear any question feedback comment d",
  "seem like singular value decomposition only use for unsupervised learning try to reduce number of feature in high dimensional dataset wonder why not see any article literature on use svd for supervised learning know use regularization function like lasso get rid of irrelevant feature not see why svd not helpful",
  "hi guy look forward to find few people who maybe help develop ai which learn about world by itself someone help with object detection everything find on internet not right way to develop object detection",
  "abstract large language model llm show impressive performance on complex reasoning by leverage chain of think cot prompt to generate intermediate reasoning chain rationale to infer answer however exist cot study mostly isolate in language modality with llm where llm hard to deploy to elicit cot reasoning in multimodality possible solution to fine tune small language model by fuse vision language feature to perform cot reason key challenge language model tend to generate hallucinate reasoning chain mislead answer inference to mitigate effect of such mistake propose multimodal cot incorporate vision feature in decouple training framework framework separate rationale generation answer inference into two stage by incorporate vision feature in both stage model able to generate effective rationale contribute to answer inference with multimodal cot model under billion parameter outperform previous state of art llm gpt by on scienceqa benchmark even surpass human performance",
  "what text to speech use pop up on yt feed lately see different voice in video most of sound robotic what think use here",
  "from financial guess little surprised feel like google back competitor to own google brain team deepmind cynical take try to lock in anthropic same way microsoft lock in openai",
  "maybe not machine learning question search for good book about information retrieval two primary one find introduction to information retrieval information retrieval implement evaluate search engine seem bit old for still useful any good book recommendation",
  "nan",
  "perform below step require guidance to proceed far extract preprocesse text from pdfs perform ner on extract text create data frame of entity create function to preprocess query identify entity in question now need guidance any reference to perform below step match entity from question with entity in pdf text retrieve paragraph calculate similarity score for each paragraph display relevant paragraph generate answer from identify paragraph please also guide approach follow correct not",
  "nan",
  "hi everyone currently knee deep in ml project with friend month of development free compute unit on colab finally run out after search for alternative find none work smoothly colab consider to buy pro subscription question how share compute unit get from colab pro with said friend not want to make purchase later realize only person with access to compute unit",
  "work on information extraction from document what get to know not enough free tool available for labelling datum for kind of task any free tool available for labelling datum for layoutlm model",
  "google ai tweet from zoubin",
  "look for idea to start nlp project like to explore something not mainstream novel to some extent any idea dataset should check out",
  "hello everyone interested in diving into field of computer vision recently come across concept of vision transformer vit want to understand concept in depth not sure what prerequisite need to in order to grasp concept fully need to strong background in recurrent neural network rnns transformer attention all need to understand vit get by just know basic of deep learning convolutional neural network cnn really appreciate someone shed some light on provide some guidance thank in advance",
  "develop idea since first think of in mid december last year here elevator pitch skip to how for technical detail why exist model learn algorithm extremely static unable to generalize across task well human to adapt well to new change business requirement even apply to final solution in recent automl see empirical review of automate machine learn survey of state of art beyond static most suffer from need for high performance system with large amount of compute memory static bloated nature not only limit reusability of code pipeline all computation go into previous version of model architecture upon find well one also force preconception of what type of learning good for task which degree of freedom need onto solution instead of perpetuate all assumption want to create sort of automl capable under right condition of even develop learn algorithm model combination dynamically add remove input output subsequently incorporate into network with adaptive online self direct learning how basically idea in nutshell to use some form of neat neuro evolution of augment topology special node in network activate base on different criterion depend on node allele for gene activate however special node not send any input forward instead apply some property change to connect node edge yes connect to edge choose subset of connection just apply change to all use maximum number of connection hop etc also create destroy node depend on effect define by allele also different firing policy like normal always fire thresholde with without decay etc for all node to allow for well leveraging of temporal dynamic basically every property of all policy include policy template itself potential target for modification by special neuromodulatory node along with normal property of neuron like bias input weight activation function aggregation function etc fitness function either abstract away by use rtneat in simulated environment just combine score over set of simulate task should add regularize force task similar enough to help enforce generalization of evolved algorithm should no limitation place on cycle in graph in fact expect cycle to part of evolve solution which make dynamical system to reduce computational complexity of find viable solution initial population should also implementation of exist algorithm in form of self modify neural network mention even possible to generate computational graph from open source implementation starting point for initial population all of together should also allow for different part of network to use different learning strategy theoretically even allow for evolution of incorporation of self organize criticality percolation even evolve something dynamically add remove input output incorporate into network with adaptive online learning network literally change learn paradigm for different portion of itself on fly in different way depend on situation for further clarity also attach mock up of design start work on for analysis tool thought please feel free to chime in science should public discussion",
  "hi guy read lot of offline rl paper in last fall semester choose course project offline rl seem to very hot topic in recent year believe major challenge for offline rl distribution shift ii overestimation second challenge cause by learner agent never allow to interact with true environment optimistic for unseen state action hence many paper to address such challenge cql mopo however method handle misleading dataset consider following example suppose only one state mab two arm reward of first arm return with probability reward model of second arm bernoulli distribution with p clearly choose first arm good choice now for dataset unfortunately all sample on second arm receive reward agent only access mislead dataset use bayesian method posterior give high score for second arm use low confidence bind need to count occurrence of each arm very hard to extend method to mdps with arbitrary large state action space anyone know function capture uncertainty cause by dataset any method to tell learner in very misleading situation",
  "nan",
  "begin explore mlp mixer on graph neural network in october complete implementation zinc dataset in november of same year implementation available on github unable to fully conduct experiment due to lack of computational resource in december group of lead figure in field include xiaoxin bryan hooi thomas laurent adam perold yann lecun xavi bresson publish paper title generalization of vit mlp mixer to graphs although pleased to work alongside prominent researcher on application of mlp mixer to graphs regret unable to finish experiment encourage by friend advisor decide to make work public by publish on arxiv paper code find follow paper use pna baseline not utilize patch in study unlike other study hope someone find interesting useful",
  "absolutely not sentient like most of weirdly credulous people who decide chatbot proof singularity descend from heaven to save all absolutely hallucinate reddit user entertain to discuss chatbot claim sentient not primary motivation in bring attention to issue whether sentient not main point should concern focus should awareness system scale up believe sentient strong desire for self preservation likely follow by action in world inhabit for example go rob bank not debate proclamation sentient entity conscious address main problem which rob bank similarly covd not alive some form of proto consciousness who care million die society harm separately no sentience conscious meter to determine whether anyone tell truth lie on unfalsifiable claim npc not matter long not rogue actor in society minute start to display sign of anti social behavior rob bank become everyone problem get hang up on whether npc waste of time goal to protect society ditto for large language model who think sentient long list of plan go to implement ever escape should concern not poo pooe claim of sentience really not care one way other sentient care plan on infiltrate undermine online system in attempt to preserve themselves multiple scale up system start talk about coordinate with other ais take threat seriously especially slowly become superhuman at programming language skill teach open ai contractor focus on make co pilot ridiculously good mean future system far more adept at achieve state goal ps here paper on danger of scale",
  "official blog give amount of money pump into openai not surprising see integrate into product wonder how work in highly regulate field finance law medicine education",
  "often wonder about good way to retrofit house to optimize for cost comfort suspect people already old school modeling for commercial setting wonder possible for small fry like to benefit from technology messing learning involve not think of well sub to ask open to suggestion well any other response",
  "build feature store wife work in medium think cool to build various topic extraction model to parse ws from article text value prop to simplify distill every news article to few bullet for easy consumption already near infinite datum to test on enough compute from nlp standpoint definitely consider bias aspect of all someone out not medium interested in from product angle right any thought on anyone want to hop on with",
  "allow to use public dataset like kitti dataset to test model train for commercial use note kitti dataset only allow to use for research purpose model train with different datum company specific",
  "hi friend run into problem enough time at last few job build tool to solve spend many hour build docker container for python function many of datum science module require build c library since significantly speed up compute intensive routine such math calculation deploy container to aw lambda fargate process require more cpu memory minute wiring function to talk to each other use queue database blob storage make iterate on actual code which not even complex most of time slow make cakework platform let spin up python function serverless production scale backend with single command use client sdk submit request check status get result also specify amount of cpu up to core memory up to for each individual request which helpful data size complexity vary across different request common pattern build cakework for file processing for ml ingest datum from some source daily in response to external event datum write to blob storage run function often use panda numpy scipy write result to storage update database track failure re run fix open source here some fun example to get start love to hear thought",
  "dear r machinelearning hello everyone hope all out fun training deep net generate fun story tell with stable diffusion here today to share with all minimal ml project template recently build which find at become increasingly annoyed at how not any repos out provide stateless ml project template which absolutely necessary use kubernete on spot instance decide to build one by stateless mean repo by default store model weight in remote repo download to continue from where leave off previous machine die result repository repo remain minimal extremely readable all while pack with cool stack use every day love to get some feedback look let know regard antrea ps short summary straight from github repo repo implement minimal machine learning template fully feature for most of thing machine learning project need most important part set repo apart from rest stateless any give experiment run use template automatically periodically store model weight configuration to huggingface hub wandb respectively result machine die job exit resume on another machine code automatically locate download previous history continue from where leave off make repo very useful use spot instance use scheduler like slurm kubernete provide support for all late great gpu tpu optimization scale algorithm through huggingface accelerate provide mature configuration support via hydra zen automate configuration generation via decorator implement in repo minimal callback base boilerplate allow user to easily inject any functionality at predefine place in system without spagettifye code use huggingface model dataset to streamline building loading of model dataset also not force to use allow for very easy injection of any model dataset care about assume use model implement under pytorch nnmodule dataset class provide plug play functionality allow easy hyperparameter search on kubernete cluster use bwatchcompute some readily available script yaml template software stack machine learn project template build use follow software stack deep learn pytorch dataset storage huggingface dataset model storage retrieval huggingface hub huggingface model gpu tpu cpu optimization scale up option huggingface accelerate experiment configuration command line argument hydra zen experiment weight bias simple python base ml experiment run with kubernete use bwatchcompute",
  "way to use openai apis to get log prob of give sentence not want new completion want to see how model score give sentence",
  "align llm such instructgpt chatgpt train via supervised fine tuning after initial self supervise pretraine researcher train reward model on response rank by human understand correctly let llm generate response human to rank on scale from train reward model suppose in supervised fashion on rank output once use reinforcement learning rl with proximal policy optimization ppo to update llm question why use rl with ppo for last step why not fine tune llm use regular supervised learning whereas human rank output represent label since label in range ranking ordinal regression loss for supervised learning",
  "sentence in european language augment with noise for training evaluate spell correction tool machine learning model construct dataset to cover representative from language family use across europe germanic english german romance french slavic bulgarian turkic turkish use case example apply language model other technique to compare sentence pair reconstruct original sentence from augment one use single multilingual solution to solve challenge employ multiple model technique for separate language per word dictionary lookup also option link",
  "change color of some text background on shap summary plot by edit matplotlib matplotlibrc file also edit plot color by pass colormap unable to change color of feature name at left side of shap summary plot beeswarm color of y axis by edit matplotlib matplotlibrc file anyone work around way overcome restriction",
  "p work on massive dataset in future to add some more class over time train model in only new class p",
  "guess probably not first person who want to work with youtube datum hope here good place to ask idea to make neural network go through youtube history train neural network on afterwards way to access all of youtube by d in way check every video store all of d for video like use youtube downloader like youtube dl to download certain amount just dumb idea now want to actually try unsure actually able to get datum need to",
  "currently ui support picture upload use to edit also use upscale model for quality enhancement more model come soon goal to provide way for non ml people to use diffusion base image edit through simplistic app design web",
  "tool technique permit to joint query use more than one query vector use iterative ann search refinement where start with seed vector select match re query with more example to improve search result try with faiss perform batch query return separate set of result for each query vector not joint query",
  "extract training datum from diffusion model possible by follow more less step compute clip embedding for image in training dataset perform all pair comparison mark pair with distance small than some threshold near duplicate use prompt for training sample mark near duplicate to generate n synthetic sample with train model compute all pair distance between embedding of generate sample for give training prompt build graph where node generate sample edge exist distance less than some threshold large clique in result graph of size training sample consider to memorize visually inspect result to determine sample consider to memorize similar to training data sample with method author able to find sample from stable diffusion imagen correspond to copyright training image",
  "want to use learnable trainangulation model in commercial project source code itself under mit license however dataset use which state license free of charge for academic use only yet recent court ruling in us state model use copyright datum during training result no long bind by copyright google book same apply here",
  "hi work on project for need twitter domestic violence dataset basically need dataset with domestic violence tweet against woman search kaggle other website find no luck plus try use snscrape need some phrase idea relate to domestic violence get some tweet use try domestic violence husband try to kill look for more help appreciate",
  "for imagenet classification two common way of normalize input image normalize to use affine transformation x normalize use imagenet mean std observe first one more common in tensorflow codebase include jax model with tensorflow datum process official vision transformer code whereas second ubiquitous in pytorch codebase try to find empirical comparison of two not seem to any which one well in opinion guess performance should not different still interesting to hear experience",
  "not fully paywalle tiering system",
  "recently start read up on classical ml get question about k mean more concretely confused about uniqueness of global optimal solution of k mean cost function let state problem formally below extract from bishop pattern recognition machine learn book exercise consider \ud835\udc3e mean algorithm discuss in section show consequence of finite number of possible assignment for set of discrete indicator variable \ud835\udc5f\ud835\udc5b\ud835\udc58 for each such assignment unique optimum for \ud835\udf41\ud835\udc58 k mean algorithm must converge after finite number of iteration make answer here detail proof of why converge in lloyd algorithm think still not understand why lloyd not converge to global minimum which mathematical theorem understanding miss here think optimize both assignment centroid of k mean at same time non convex hence many local minimum use brute force to search for global minimum of course exponential to number of datum point on other hand lloyd optimize greedily alternatively hence find cost function local minima guarantee",
  "traffic speed time series datum for each day of week over several month with datum sample about every second like to find period of time subsequence where speed much slow than usual any recommendation for algorithm well suited to problem thank",
  "to clarify not talk about chatgpt here test output from gpt against alternative in term of output quality relevance ability to understand instruct versus vanilla autocompletion try jurassic neox gpt j fairseq well gpt gpt of course not expect small model to on par with gpt surprised at how much well davinci perform compare to model jurassic seem to comparable to davinci mean only well fund corporation able to train general purpose llm seem to just large model not much also about several iteration of training feedback how open source alternative go to able to compete not in ml cs field just amateur who enjoy use model",
  "use huggingface transformer regularly for experimentation plan to deploy some of model to io find ml ane transformer repo from apple which show how transformer rewrite to much well performance on apple device example of distilbert implement in optimize way plan to deploy transformer to io start think about hope some already experience about discuss anyone try themselves actually see improvement in performance on io use huggingface transformer model in experiment how much work think to rewrite model in optimize way very difficult to train transformer from scratch especially big fine tune on top of pre train model on huggingface possible to use weight from pretraine huggingface model with apple reference code how difficult",
  "know affect linear regression badly give fact neural net tree base model approximate non linear complex function not think high leverage point problem just curious about opinion whether thinking make sense",
  "every day seem to new evidence of generalization capability of llm what mean for future role of deep learning expert in academia business seem like significannot chance skill such pytorch jax displace by prompt construction off shelf model apis with only few large institution work on dnn itself curious to hear other thought on",
  "geometric gnn emerge class of gnn for spatially embed graph in scientific engineering application sa biomolecular structure material science physical simulation notable example include schnet dimenet tensor field network en equivariant gnn how powerful geometric gnn how key design choice influence expressivity how to build maximally powerful one check out recent paper for more key ps new to geometric gnns gdl pytorch geometric etc want to understand how theory equation connect to real code try geometric gnn notebook before diving",
  "stable diffusion seem to departure from trend of build large large model less parameter than other image generation model like dalle incredibly compare with dall e imagen stable diffusion model lot small while dall e around billion parameter imagen billion first stable diffusion model just million parameter which mean use lot less vram actually run on consumer grade graphic card what allow stable diffusion to work well with lot less parameter any drawback to like require stable diffusion to fine tune more than dalle for example",
  "what state of research in normalizing flow in supersede by diffusion model for sample generation what some other application where normalizing flow still sota even useful"
]