[
  "I've been looking into open source large language models to run locally on my machine.\n\nSeems GPT-J and GPT-Neo are out of reach for me because of RAM / VRAM requirements.\n\nWhat models would be doable with this hardware?:\n\nCPU: AMD Ryzen 7 3700X 8-Core, 3600 MhzRAM: 32 GB\n\nGPUs:\n\n1. NVIDIA GeForce RTX 2070 8GB VRAM\n2. NVIDIA Tesla M40 24GB VRAM",
  "Paper: [https://arxiv.org/abs/2302.07842](https://arxiv.org/abs/2302.07842)\n\nAbstract:\n\n>This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows **ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks.** In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.       \n\nhttps://preview.redd.it/lyjdr1ozj6ja1.jpg?width=1281&format=pjpg&auto=webp&v=enabled&s=1b5db84ec5b38228fc794e3fd24e83e4e450cc57",
  "Title.",
  "Hi everyone. I am an independent researcher working on my pure RNN language model RWKV. I have finished the training of RWKV-4 14B (FLOPs sponsored by Stability EleutherAI - thank you!) and it is indeed very scalable. Note RWKV is parallelizable too, so it's combining the best of RNN and transformer.\n\nThe ChatRWKV project (let's build together):\n\n[https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nZero-shot comparison with NeoX / Pythia (same dataset: the Pile) at same params count (14.2B):\n\n&#x200B;\n\nhttps://preview.redd.it/f6lxnjgfceia1.png?width=1174&format=png&auto=webp&v=enabled&s=54de7568974fc187584bd6825d92935baa079e83\n\nGeneration results (simply topP=0.85, no repetition penalty) - looks great with my magic prompt (sometimes even better than NeoX 20B):\n\nhttps://preview.redd.it/99deuc17ceia1.png?width=1878&format=png&auto=webp&v=enabled&s=456c8d9bb2a968d73f44a0d3589cf6b893be31f4\n\n&#x200B;\n\nhttps://preview.redd.it/g62e4l48ceia1.png?width=1887&format=png&auto=webp&v=enabled&s=c997bf27692d7e53d07de19048b6cbf3d2c9ebff\n\n&#x200B;\n\nhttps://preview.redd.it/379egq09ceia1.png?width=1808&format=png&auto=webp&v=enabled&s=895f05fe14e2a3a41863802858114f3096d0ed77\n\n&#x200B;\n\nhttps://preview.redd.it/pcgq7gz9ceia1.png?width=1886&format=png&auto=webp&v=enabled&s=138b0aec404b8f7f49f585d00284edbac791ffaf\n\n&#x200B;\n\nhttps://preview.redd.it/rn743etbceia1.png?width=1715&format=png&auto=webp&v=enabled&s=6d83cc2a200bdd655b690f56559dda43490ed2b3\n\n&#x200B;\n\nhttps://preview.redd.it/uhal4dkcceia1.png?width=1879&format=png&auto=webp&v=enabled&s=3b3db0b96456df9590a8b38ebe7d58509ebccb20\n\nExplanation, fine-tuning, training and more:\n\n[https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)",
  "I'm working on a project where the user can \"upload\" their full face and body view it in a 3D viewer.\n\nRight now I see 2 ways of doing this:\n\n1. Use an image-to-3D tool. Have the user upload a full body image of themselves and the tool will generate a 3D model based on the photo. I'm skeptical of the accuracy of this though.\n2. Have the user record themselves doing a 360 degree spin and the software will generate a 3D likeness of the person based on the video.\n\nHow would you go about solving this problem right now?",
  " \n\nHello, community.\n\n**Description:**\n\nI am planning to create a detection model using YOLO v8 to detect leukemia cells in a blood sample. I started learning about deep learning two months ago and I am eager to try out image segmentation on my present dataset instead of bounding boxes, as the cells are closely bunched together. I need advice on whether I should use bounding boxes or instance segmentation, considering my dataset and expected results.\n\n**Context:**\n\nLeukemia is caused by an abundance of different types of naive or altered white blood cells in the body, which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells. There are three classes in my dataset: lymphoblasts, promyelocytes, and neutrophils, and I need to be able to detect these cells.\n\n**Expected Results:**\n\nAs this is a medical domain, false positives are acceptable, but false negatives are not.\n\n**About dataset:**\n\n[lymphoblast sample image](https://imagebank.hematology.org/getimagebyid/2201?size=3)\n\n[sample image for promyelocytes](https://medschool.co/images/detail/blood-film/promyelocyte.jpg)\n\n[sample image for neutrophils](https://imagebank.hematology.org/getimagebyid/3610?size=3)\n\n[sample test image](https://thumbs.dreamstime.com/z/picture-white-blood-cell-red-blood-cell-platelet-blood-film-analyze-microscope-picture-blood-cells-blood-film-161974012.jpg)\n\nlymphoblasts(101 images)\n\npromyelocytes(91 images)\n\nneutrophils(133 images)\n\n**more context for your reading:**\n\nAn over abundance of lymphoblasts results in acute lymphoblastic leukemia (ALL), while acute pomyelocytic leukemia (APLML/APL) is caused by an abnormal accumulation of promyelocytes. neutrophils do not cause leukemia.",
  "Hello, I want to know if it is legal to use scraped video or images to train a predictive model, for example, If I scrape photos of faces in google, and after that, I share that model in order that a lot of people can detect faces in their applications, is that legal?",
  "When the [classifier-free guidance](https://arxiv.org/abs/2207.12598) was first introduced, I was very confused about why it works: I'd understand if it was interpolating like `\u03b5 * conditional_prediction + (1 - \u03b5) * unconditional_prediction`, but in its formulation, \u03b5 is greater than 1. It is clear why it makes the result match the condition better, but why the result becomes better regardless of the condition was a mystery to me.\n\nAfterwards, there were many post-hoc explanations, which didn't seem satisfactory (e.g. these explanations didn't have predictive power helping to improve the trick). Recently, I finally got around to play with it, and found some interesting patterns (in context of diffusion, DDIM sampling):\n* If we disable CFG for 90% last sampling steps, results are pretty much the same;\n* If we disable CFG for the first 10% sampling steps, the resulting image is destroyed.\n\nIt appears that CFG it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling, and doesn't do much afterwards. This is kind of similar to the observation about attention maps in [this paper](https://arxiv.org/pdf/2208.01626.pdf) (section 3.1). Speculatively, it tries to \"match the prompt\" to the random noise, and the adjustments from it need to be amplified, otherwise subsequent steps will match the prompt differently (it is a random noise after all). If this is true, I guess something like this might also work (I haven't tried yet): sample 1000 different starting random states, take the one which \"matches the prompt\" the best by some measure, and do the diffusion sample starting from it without CFG.\n\nThis all might make sense, except that this is very specific to diffusion. But [it is known](https://arxiv.org/pdf/2206.10789.pdf) that CFG works just as well for autoregressive transformers on VQVAE tokes. This might indicate that the mechanism why it works is more fundamental and is not specific to diffusion.\n\nI wonder if there is any community wisdom/thoughts on why/how it works, and generalizes so well across two very different types of models.",
  "I built a (1) baby sleep tracking & (2) forecasting system, and wanted to share for those interested, or actually want to try running it at your home.\n\n(1) I built a baby sleep tracking system (computer vision largely, [here's the core of that code](https://github.com/calebolson123/BabySleepCoach/blob/924e7b55d3aa36acd706519c446c1172dbbda4a7/main.py#L322)) which writes timestamped records of when my baby fell asleep or wakes up. The code is pulling images from my baby monitor, and largely just applying heuristics over time to decide whether he's awake/asleep.\n\n(2) After I had a few weeks of sleep data ([sample data](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_logs.csv)), I moved it into a [jupyter notebook](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_forecast_arima.ipynb) and ended up using an ARIMA model to forecast the next month's wakings/sleepings. I wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so I can see how his sleep is changing over time. [Here's an example of what that visual looks like](https://imgur.com/BdwBoeG) (orange is awake, blue is asleep).\n\nI built it because my wife asked for it, but also made a video detailing the project: [https://youtu.be/r7Exc0sUt5E?t=209](https://youtu.be/r7Exc0sUt5E?t=209)",
  "Just finished reading the Stanford/Google survey paper ([https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)) on emergent abilities of large language models. It made me wonder: do image generation models have emergent abilities, too? Do we know?\n\nI can't quite wrap my head around what such an ability would even look like. Figured maybe other folks had given this a think.",
  "I have constructed a novel ML (NLP) dataset for classification and labeled it with three classes. The dataset is rather small with about 700 examples, out of which the classes have about 400, 200, and 100 examples respectively. I would like to publish it and describe it in an official publication for a workshop or a conference.\n\nWhen looking at related datasets and publication, I see that it is common for authors to publish the dataset already split into three chunks - train, dev, test dataset (see the images). It is also common in these papers to provide the performance of baseline models on the dataset. Considering the dataset's small size, I feel like doing a 5-fold cross-validation would be a good alternative for such a small dataset, rather than doing something like a split into 450-100-150 train-dev-test datasets and then evaluating only on the very small dataset with 150 examples. Still, I believe that for better replicability, doing an \"official\" split is preferred and then everyone in the future testing on the same test set with 150 examples? Why do the authors usually already provide the three splits?\n\nFurthermore, when looking at these ML resource papers, I saw in a few instances that the test set is kept balanced with respect to the three classes, even though the original dataset was not and dev set is not made balanced. This is problematic in my case for my third class where there are only about 100 examples. If I make my test set to be 50-50-50 for class1-class2-class3, then there is only 50 examples of class3 left for train+dev! That is simply infeasible for the training set. None of the authors provide any sort of explanation why they split it like this, they just seem to say \"here is our split\". Is this done to discourage the model from just doing a majority-class prediction and thus make it challenging? Or because a dummy classifier would have a 60% accuracy? Still, with a metric like F1 and not accuracy, this does not seem like an issue...\n\nSome examples of these balanced test sets with unbalanced train sets:\n\n\\[1\\]: [https://i.stack.imgur.com/RGRk3.png](https://i.stack.imgur.com/RGRk3.png)\n\n\\[2\\]: [https://i.stack.imgur.com/R39Oh.png](https://i.stack.imgur.com/R39Oh.png)\n\n\\[3\\]: [https://i.stack.imgur.com/6Vqaw.png](https://i.stack.imgur.com/6Vqaw.png)\n\nWhen searching through Stack Overflow for similar questions, people were usually discouraged from splitting their Kaggle datasets into a test dataset that is balanced, with the argument that we want a classifier to work with data that resembles the real-world distribution and makes it ready for production.\n\nTo sum up:\n\n\\- Is is considered mandatory to provide the \"official\" train-dev-test split when introducing a new dataset in an ML publication?\n\n\\- If so, should the test set have a balanced class distribution and why?",
  "Hi,  \n\n\nI was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross-attention module. So let's say we have a text that is already transformed into a continuous embedding. Then this goes through the llm and is fed into the cross-attention module at every timestep. At the end of the diffusion process, we get some image/a latent representation of an image in the case of stable diffusion. We can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non-stochastic sampler like DDIM. The issue is the length of the graph calculating that derivative is super expensive. I was if anyone already solved this or has some good references.  \n\n\nThanks :)",
  "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
  "I've been looking into open source large language models to run locally on my machine.\n\nSeems GPT-J and GPT-Neo are out of reach for me because of RAM / VRAM requirements.\n\nWhat models would be doable with this hardware?:\n\nCPU: AMD Ryzen 7 3700X 8-Core, 3600 MhzRAM: 32 GB\n\nGPUs:\n\n1. NVIDIA GeForce RTX 2070 8GB VRAM\n2. NVIDIA Tesla M40 24GB VRAM",
  "As far as I can tell, there are two contradictory definitions of Layer Normalization that are both floating around. LN computes the mean and variance along some axes of the input tensor for normalization, yet the choice of axes is not clear:\n\nA. The [GroupNorm paper (2018)](https://arxiv.org/pdf/1803.08494.pdf) has this figure that describes LN as reducing **along channel and spatial/token axes**.\n\nhttps://preview.redd.it/ui9adzzxgcja1.png?width=1353&format=png&auto=webp&v=enabled&s=f701d53a0992e3fe13bdac6ee022d352f965c893\n\nB. The [PowerNorm paper (2020)](https://arxiv.org/pdf/2003.07845.pdf) has this figure that describes LN as reducing **only along the channel axis**.\n\nhttps://preview.redd.it/e0qmp9sahcja1.png?width=1717&format=png&auto=webp&v=enabled&s=00126512760766783d88217b44377f5741290d9c\n\nThere are also many online sources that describe LN as shown in A (e.g. [TF tutorials](https://www.tensorflow.org/addons/tutorials/layers_normalizations), [PapersWithCode](https://paperswithcode.com/method/layer-normalization), [this summary of normalization techniques](https://theaisummer.com/normalization/)) using similar figures.\n\nThe [LN paper (2016)](https://arxiv.org/pdf/1607.06450.pdf) itself says\n\n>all the hidden units in a layer share the same normalization terms \u03bc and \u03c3\n\nso the channel axis is definitely reduced, and\n\n>computing the mean and variance used for normalization from all of the summedinputs to the neurons in a layer *on a single training case*\n\nso the batch axis is definitely not reduced. As far as I can tell it is not clear about what happens with spatial/token axes, although the above sounds rather like they might be included in the statistics.\n\nYet, I don't know of any model that actually uses A instead of B. For example, [TF](https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/normalization/layer_normalization.py#L158) and [Flax](https://github.com/google/flax/blob/main/flax/linen/normalization.py#L321) explicitly implement LN with default axes as in B ([PyTorch](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py#L142), [Haiku](https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/layer_norm.py#L78) and [Equinox](https://github.com/patrick-kidger/equinox/blob/1f5373f5905504bc5e7069ed6d458dbad5616495/equinox/nn/normalisation.py#L39) don't have a preference and require the user to specify the reduction axes). [Vision Transformer](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L133) uses Flax with LN as in B, [ConvNeXt](https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L135) implements LN with PyTorch as in B, [OpenAI GPT-2](https://github.com/openai/gpt-2/blob/master/src/model.py#L28) implements LN with Tensorflow as in B, even [MLP-Mixer](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_mixer.py#L41) where the spatial/token axes are interpreted as channel axis for an MLP still computes statistics along the original channel axis as in B.\n\nAs far as I can tell, everyone uses B rather than A in their models, so to me this seems to be the \"correct\" definition. Yet, many sources on this topic describe LN as doing A rather than B.\n\nDoes anyone have any insight on this or know of a source that has addressed this problem? Do you interpret the original LN paper as including spatial/token axes in their computation of mean and variance, or not? Is this simply an error that started with the figure A and made its way into different online tutorials from there? Or do you maybe know of a model that actually uses LN to reduce both along channel and spatial/token axes?",
  "In the abstract of the Nerf paper ([https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)), the described framework is that Nerf enable to do the following: the user inputs a set of images with known camera poses, and after training the network they can generate images of the same scene from new angles.\n\nHowever, the paper itself builds a network that gets as an input 5D vectors (3 location coordinates+2 camera angles) and outputs color and volume density for each such coordinate. I don't understand where do I get those 5D coordinates from? My training data surely doesn't have those - I only have a collection of images. Same for inference data. It seems that the paper assumes not only having a collection of images but also having a 3D representation of the scene, while the abstract doesn't require the latter. What am I missing here?",
  "Completely new to anything ML here, just looking to get pointed in the right direction.\n\nI'm creating an application which will, from a set of gym exercises, create the most optimal combination for the most effective workout. How would I go about this?\n\nI've seen similar, I think, ideas used in apps such as FitBod and FitnessAI so would be interested\u2014 if anyone knows \u2014 how they achieved this. \n\nThis is for computer science a-level coursework.\nAny advice would be greatly appreciated :)",
  " I got my BS in Math and CS and currently pursuing a master in data science. My goal is to work with a fintech company or in NLP. I'm in my first semester of my master and was wondering what classes or what projects will make me stand out to land a job in my desire field?",
  "One of the things in current publications that completely irritates me is people just forcing the use of GANs where they are not even needed nor suited at all, just to ride on the hype of *generative AI*.\n\nThese guys usually have samples `(x_1, y_1=phi(x_1)), ..., (x_n, y_n=phi(x_n))` of a random pair `(X, Y=phi(X))` where `phi` is some unknown target function (*ie* in fancy-pants math we know that `Y` is `sigma(X)`\\-measurable). A direct way to solve this is to treat it naturally as a regression problem and use your usual ML/DL toolkit. These guys however think that they can make the problem look sexier if they introduce GANs. For instance, they'd train a GAN taking `X` as an input and through the discriminator have the generator output something that has the same distribution as `Y=phi(X)`. Some will even add some random noise `z` , that has nothing to do with `X`, to the inputs of the generator despite knowing that `X` is already enough to fully determine `Y`. GANs would have been useful if we didn't have joint observations of `X` and `Y` but that is not the case here.\n\nOne of the papers I have in mind is this one: [https://openreview.net/pdf?id=SDD5n1888](https://openreview.net/pdf?id=SDD5n1888)\n\nHow on earth are these papers getting accepted? To me that is literally just plagiarism of what's already available (physics-informed NNs in that case) by adding a totally useless layer (the GAN) to make it seem like this is a novel approach. That paper is only one of many cases. I know of a professor actively using that same technique to get cheap articles where he just replaces a standard regression NN in an old paper found online by a totally unjustified GAN. IMO reviewers at these journals/conferences need to be more mindful of this kind of plagiarism/low-effort submission.",
  "Paper: [https://arxiv.org/abs/2301.04104#deepmind](https://arxiv.org/abs/2301.04104#deepmind) \n\nWebsite: [https://danijar.com/project/dreamerv3/](https://danijar.com/project/dreamerv3/) \n\nTwitter: [https://twitter.com/danijarh/status/1613161946223677441](https://twitter.com/danijarh/status/1613161946223677441) \n\nGithub: [https://github.com/danijar/dreamerv3](https://github.com/danijar/dreamerv3)  / [https://github.com/danijar/daydreamer](https://github.com/danijar/daydreamer) \n\nAbstract:\n\n>General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present **DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches** across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with **larger models directly translating to higher data-efficiency and final performance.** Applied out of the box, DreamerV3 is the **first algorithm to collect diamonds in Minecraft from scratch without human data or curricula,** a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems. \n\nhttps://preview.redd.it/h4hrfqwp57ja1.jpg?width=1320&format=pjpg&auto=webp&v=enabled&s=f3687d48c9b28efe184931cee62d7ff42b5d5655\n\nhttps://preview.redd.it/bl13kxwp57ja1.jpg?width=1399&format=pjpg&auto=webp&v=enabled&s=56c70c244e3ccf45b351e83791059d01a535299f\n\nhttps://preview.redd.it/b0kqa2xp57ja1.jpg?width=1286&format=pjpg&auto=webp&v=enabled&s=55fffc14ae68cb8ad60e395182c71c541c5f2005\n\nhttps://preview.redd.it/e61x5xwp57ja1.jpg?width=1291&format=pjpg&auto=webp&v=enabled&s=d23cfe3bab1114f543187c53d355488e4d3c8ffa",
  "Hi everyone,\n\nI'm currently working on a biometric identification project that involves converting biometric data (such as iris images) into a unique and secure ID. In order to do so, one of the first steps in the pipeline (after training a feature extractor) is to extract a set of features from an image in some tensor form (preferably a vector). What I'm wondering is what robust method could be used to extract similar feature vectors for similar inputs (e.g., to obtain similar, in terms of Euclidean distance, feature vectors for various photos of a same iris)? That would be required such that the feature vectors for similar inputs could be converted to the same unique ID (e.g., by using a locality-sensitive hashing algorithm).\n\nIn short, I'm interested in any tips for:\n\n* Choosing an appropriate and robust feature extraction architecture\n* Methods for conversion of features to IDs (such as hashing, or anything that should work in theory)\n\nAny insights or suggestions would be greatly appreciated. Thanks in advance!",
  "Hello everyone,  \n\n\nIt's that time again, thank you all so much for the support you've given us over here. I've done a ton of typing this morning, so for a summary of what I've updated, you can see the higher-level twitter thread I wrote at [https://twitter.com/hi\\_tysam/status/1627679672988319746?cxt=HHwWhIC-yb2C15YtAAAA](https://twitter.com/hi_tysam/status/1627679672988319746?cxt=HHwWhIC-yb2C15YtAAAA), or the more detailed (but still rough cut) patch notes I wrote this morning at  [https://github.com/tysam-code/hlb-CIFAR10/releases/tag/v0.5.0](https://github.com/tysam-code/hlb-CIFAR10/releases/tag/v0.5.0)  \n\n\nHappy to answer any questions anyone might have, cheers! :D :))))",
  "Paper: [https://arxiv.org/abs/2302.07842](https://arxiv.org/abs/2302.07842)\n\nAbstract:\n\n>This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows **ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks.** In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.       \n\nhttps://preview.redd.it/lyjdr1ozj6ja1.jpg?width=1281&format=pjpg&auto=webp&v=enabled&s=1b5db84ec5b38228fc794e3fd24e83e4e450cc57",
  "I really like training in the cloud for some reason and feels satisfying, however here is a couple of things I would've wished I knew beforehand to get things started.\n\n1. Use a spot instance unless you absolutely must make sure it isn't interrupted. Your wallet will thank you later. \n2. Make sure Nvidia drivers are installed and don't experiment with Operating systems. You are paying by the hour. \n3. Make sure to use something like tmux to save the sessions running in your terminal so you don't have to start from scratch or in case you disconnect from the vm (but the VM isn't shut down). That way you can just click out of the terminal and not bother with it until it's done. \n4. Debug on your local machine on CPU if you don't have CUDA. You can debug the model on a CPU perfectly fine. \n\nNow what about you all?",
  "TorchDrug is a machine learning platform designed for drug discovery, covering techniques from graph machine learning (graph neural networks, geometric deep learning & knowledge graphs), deep generative models to reinforcement learning. It provides a comprehensive and flexible interface to support rapid prototyping of drug discovery models in PyTorch. \n\nIn this video, we walk through TorchDrug library and train some GNN for graph classification, attribute masking and unsupervised graph representation learning.\n\nhttps://youtu.be/-Kb7kN4aHMM",
  "The paper \"Offsite-Tuning: Transfer Learning without Full Model\" describes a privacy-preserving and efficient transfer learning framework. In this framework  \n\n\n\u2022 Offsite-Tuning is a privacy-preserving and efficient transfer learning framework    \n\u2022 Model owner sends a light-weight adapter and a lossy compressed emulator to the data owner   \n\u2022 Data owner fine-tunes adapter on downstream data with the emulator's assistance   \n\u2022 Fine-tuned adapter is then returned to the model owner to create an adapted foundation model   \n\u2022 Offsite-Tuning preserves both parties' privacy and is computationally more efficient than existing fine-tuning methods\n\nHow does this differ from Federated Learning?\n\nPaper Link: [https://arxiv.org/abs/2302.04870](https://arxiv.org/abs/2302.04870)",
  "I just saw a tutorial about using langchains and am curious about how it works. So if i implemented something at my company that can answer any question across all our documents, does it mean i would have essentially gave all our company info to openai?",
  " \n\nHello, community.\n\n**Description:**\n\nI am planning to create a detection model using YOLO v8 to detect leukemia cells in a blood sample. I started learning about deep learning two months ago and I am eager to try out image segmentation on my present dataset instead of bounding boxes, as the cells are closely bunched together. I need advice on whether I should use bounding boxes or instance segmentation, considering my dataset and expected results.\n\n**Context:**\n\nLeukemia is caused by an abundance of different types of naive or altered white blood cells in the body, which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells. There are three classes in my dataset: lymphoblasts, promyelocytes, and neutrophils, and I need to be able to detect these cells.\n\n**Expected Results:**\n\nAs this is a medical domain, false positives are acceptable, but false negatives are not.\n\n**About dataset:**\n\n[lymphoblast sample image](https://imagebank.hematology.org/getimagebyid/2201?size=3)\n\n[sample image for promyelocytes](https://medschool.co/images/detail/blood-film/promyelocyte.jpg)\n\n[sample image for neutrophils](https://imagebank.hematology.org/getimagebyid/3610?size=3)\n\n[sample test image](https://thumbs.dreamstime.com/z/picture-white-blood-cell-red-blood-cell-platelet-blood-film-analyze-microscope-picture-blood-cells-blood-film-161974012.jpg)\n\nlymphoblasts(101 images)\n\npromyelocytes(91 images)\n\nneutrophils(133 images)\n\n**more context for your reading:**\n\nAn over abundance of lymphoblasts results in acute lymphoblastic leukemia (ALL), while acute pomyelocytic leukemia (APLML/APL) is caused by an abnormal accumulation of promyelocytes. neutrophils do not cause leukemia.",
  "Hello, \n\nWhat is your perception of UAI and AISTATS conf\u00e9rences ? Is it good to publish that ? Is one more competitive than the other ? \n\nThanks",
  "Does anyone know of a paper / article that discusses the accuracy / usefulness of available opensource LLM models.  \n\n\nBloom, GPT-NeoX, T5, etc.  \n\n\nWhat would be a good way to evaluate tradeoffs?",
  "When the [classifier-free guidance](https://arxiv.org/abs/2207.12598) was first introduced, I was very confused about why it works: I'd understand if it was interpolating like `\u03b5 * conditional_prediction + (1 - \u03b5) * unconditional_prediction`, but in its formulation, \u03b5 is greater than 1. It is clear why it makes the result match the condition better, but why the result becomes better regardless of the condition was a mystery to me.\n\nAfterwards, there were many post-hoc explanations, which didn't seem satisfactory (e.g. these explanations didn't have predictive power helping to improve the trick). Recently, I finally got around to play with it, and found some interesting patterns (in context of diffusion, DDIM sampling):\n* If we disable CFG for 90% last sampling steps, results are pretty much the same;\n* If we disable CFG for the first 10% sampling steps, the resulting image is destroyed.\n\nIt appears that CFG it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling, and doesn't do much afterwards. This is kind of similar to the observation about attention maps in [this paper](https://arxiv.org/pdf/2208.01626.pdf) (section 3.1). Speculatively, it tries to \"match the prompt\" to the random noise, and the adjustments from it need to be amplified, otherwise subsequent steps will match the prompt differently (it is a random noise after all). If this is true, I guess something like this might also work (I haven't tried yet): sample 1000 different starting random states, take the one which \"matches the prompt\" the best by some measure, and do the diffusion sample starting from it without CFG.\n\nThis all might make sense, except that this is very specific to diffusion. But [it is known](https://arxiv.org/pdf/2206.10789.pdf) that CFG works just as well for autoregressive transformers on VQVAE tokes. This might indicate that the mechanism why it works is more fundamental and is not specific to diffusion.\n\nI wonder if there is any community wisdom/thoughts on why/how it works, and generalizes so well across two very different types of models.",
  "With the advent of stable diffusion/midjourney/dalle and upcoming text-to-video models from Google and Meta, what will be major challenges in computer vision? It feels like once text-to-video models get released, visual reasoning will be mostly solved, and the only thing left to do is to improve model accuracy/efficiency from there. I am fairly new to Computer Vision and would love to learn new possible areas of research. Thank you in advance!",
  "Given the impressive capabilities of ChatGPT, I've been learning about RLHF - just wondering if there has been any work/research on RLHF with a model-based RL algorithm (e.g. MuZero, vs PPO). Thanks!",
  "When designing neural network architectures, it is common to think about \"information flow\", e.g. how is information propagated, where are the \"information bottlenecks\" and so on. Another example might be that some people use \"information loss\" to explain why transformers work better than RNNs. \n\nIt seems like most papers discuss this in a rather hand-wavy way. Is there any work done in formalising such ideas to better guide us understanding various model architectures? What are the core ideas?",
  "(Edit: This is definitely an error, not a change in pricing model, so no need for alarm. This has been confirmed by the lead product owner of colab)\n\nWithout any announcement (that i could find) google has increased the pricing per month of all its Colab Pro tiers, Pro is now 95 Euro and Pro+ is 433 Euro. I paid 9.99 Euro for the Pro tier last month... and all source i can find also refer to the 9.99 pricing as late as September last year. I have also checked that this is not a \"per year\" subscription price, it is in fact per month.\n\nI looked at the VM that Colab Pro gives me and did the calculation for a similar VM in google cloud (4 vCPUs, 15GB RAM and a T4 GPU) running 24/7 for a month (Google calculates it as 730  hours). \n\nIt costs around 290 Euro, less than the Colab Pro+ subscription... \n\nThe 100 credits gotten from the Colab Pro subscription would only last around 50 hours on the same machine! \n\nAnd the 500 credits from Colab Pro+ would get 250 hours on that machine, a third of the time you get from using Google Cloud, at over 100 euro more....\n\nThis is a blatant ripoff, and i will certainly cancel my subscription right now if they don't change it back. It should be said that i do not know if this is also happening in other regions, but i just wanted to warn my fellow machine learning peeps before you unknowingly burn 100 bucks on a service that used to cost 10...\n\n[Google Colabs price tiers on 17th of February 2023, 10 times what they were in January 2023.](https://preview.redd.it/l7gx48kw8qia1.png?width=1717&format=png&auto=webp&v=enabled&s=7b0687f1615344ffdb4fbe4ea7990f769bacd9c8)",
  "Hello, this is a question regarding regarding a system of two(or more) classifiers for energy/computation purposes. For example a mobile phone and a cloud server.  \nWhat frameworks/techniques exist for tuning the thresholds for two or more classifiers simultaneously?  \nFor example, given two trained binary classifiers, I would like to pass a labeled validation dataset X through both classifiers and tune 2 thresholds for classifier1(upper and lower) and 1 threshold for classfier2. Everything that is lower than the \"upper\" threshold and higher than the \"lower\" threshold(what classifier1 is not certain of) should be passed to classifier2.\n\nTo avoid a very liberal passing of data to classifier2, I also want to introduce a loss/penalty for doing so, meaning that classifier1 should learn using the provided labeled data when it really has to pass the sample to classifier2.\n\nXGBoost seems to be focused on tuning a single classifier, and I feel like I might need to use some Reinforcement learning technique, but I do not know the nomenclature for this kind of problem, policies perhaps? Does anyone have experience with this?",
  " \n\n**Github:** [https://github.com/m-barker/fibs-reporter](https://github.com/m-barker/fibs-reporter)\n\n**PyPI:** [https://pypi.org/project/fibs-reporter/](https://pypi.org/project/fibs-reporter/)\n\nThe Data **F**eature **I**mportance, **B**aseline-modeller and **S**purious correlation Reporter (**FIBS**) is an open-source software for automatic generation of a PDF report to highlight and visualise potential sources of spurious correlation within **any** given tabular or audio dataset stored as a Comma Separated Values (CSV) file. FIBS is run through one-command line command; **all of the calculations, model training, and report generation happen automatically**.\n\nAll that is required as input on the command line is the path to the CSV file containing the data, and the name of the output (dependent) variable within the dataset. The toolkit will automatically determine whether the task is regression or classification. Optionally, the toolkit can process and extract audio data, provided the name of the variable within the CSV that contains the audio file for each observation is specified.\n\nKey features that are generated automatically:\n\n* A traffic light score for potential spurious correlations within the dataset\n* Calculation of four different feature importance metrics to highlight the most important features within the given dataset\n* Training and evaluation of two baseline models, including visualisation of model results\n* Visuals of the most important features, with different visuals depending on the variable types\n* Automatic determination of regression or classification task, resulting in different baseline models, feature extraction methods, and visualisations\n* Principal Component Analysis calculation and baseline model to estimate complexity within the dataset\n* (Optionally) extract audio data features and run the above on these features\n* Output all of the above in a PDF report with accompanying dynamic textual explanations",
  "I've been studying about ARIMAX, XGBoost, MLForecast and Prophet. As a newcomer to any method, I like first to do an exhaustive comparison of tools trying to understand where they succeed/fail. After exploring [ARIMA/XGBoost](https://dsdaily.substack.com/p/ds-daily-arima-and-xgboost?utm_source=substack&utm_campaign=post_embed&utm_medium=web), I came across [MLForecast/Prophet](https://dsdaily.substack.com/p/ds-code-review-prophet-vs-mlforecast). But I'm left with the following questions:\n\n1. Why is MLForecast better than out-of-the-box XGboost? Sure, it does feature engineering and it appears to do dynamic predictions on your lagged features, but is that it? Does it do hyperparameter tuning? Does it have seasonal trends like Prophet does?\n2. I see that you can use exogenous features in Prophet, but how does this scale? Let's assume I have 50 predictors. How does prophet handle these? I found this in the [docs](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html)and this other [person's post](https://towardsdatascience.com/forecast-model-tuning-with-additional-regressors-in-prophet-ffcbf1777dda) explaining how to do it, but largely I've come away with the impression that it's pretty hard to do this vs. just doing it with XGBoost.\n3. Does ARIMAX compare anymore? Are there any papers comparing out-of-sample predictions with ARIMAX vs. XGBoost vs. Prophet vs. Fable? Does it just depend on your dataset and I should try all four?\n\nI have a time series data with dozens of \"known\" inputs (such as ad spend) and a lot of external data (CPI, economic health, stocks, etc.). My goal is to use my model to optimize my target by \"plugging in\" ad spend and dynamically forecasting the economic data.",
  "Hi there,\n\nI am a research data scientist, and excited to release a new feature engineering library, designed to help you streamline the process of machine learning even more than before. **Headjack is an open library which provides a ML features transformation based on self-supervised learning models**, similar to huggingface as a hub, but which currently focuses on exchanging features for tabular data models.\n\nCompared to textual data, tabular data are different in that each data set has different column length and attributes, this means that it cannot be typed consistently unlike the token embedded in NLP tasks. Therefore, Headjack is different from NLP\u2019s pre-trained model with single domain transformation, but by performing with two different domain transformations. **In other words, we can perform features transform between two domains without the same key value.** In addition, release the potential of data that is not typically used. For example, enhance the prediction of the Boston housing price task applied in the Titanic domain, or enhance the prediction of the customers churn task applied in the African traffic domain and so on.\n\n[Github](https://github.com/jimliu741523/headjackai-sdk)\n\n[Introduction](https://medium.com/p/385a90ff413c)\n\n&#x200B;\n\n[The IRIS dataset with California House Price Feature Transformation](https://preview.redd.it/54w2qwnm8pia1.png?width=2110&format=png&auto=webp&v=enabled&s=aa9a3333448985f22604fab9012272a8c54387fa)\n\n[The IRIS dataset with Titanic Feature Transformation](https://preview.redd.it/9revfvdq8pia1.png?width=2102&format=png&auto=webp&v=enabled&s=ba3ae69e5a96a6f3d74850526045a39b34636909)\n\n[The IRIS dataset with KPMG Customer Demorgraphy Feature Transformation](https://preview.redd.it/p7s7zj9r8pia1.png?width=2052&format=png&auto=webp&v=enabled&s=b7147a25b14f23346331157e11b98c86472f7ae5)\n\n&#x200B;",
  "I built a (1) baby sleep tracking & (2) forecasting system, and wanted to share for those interested, or actually want to try running it at your home.\n\n(1) I built a baby sleep tracking system (computer vision largely, [here's the core of that code](https://github.com/calebolson123/BabySleepCoach/blob/924e7b55d3aa36acd706519c446c1172dbbda4a7/main.py#L322)) which writes timestamped records of when my baby fell asleep or wakes up. The code is pulling images from my baby monitor, and largely just applying heuristics over time to decide whether he's awake/asleep.\n\n(2) After I had a few weeks of sleep data ([sample data](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_logs.csv)), I moved it into a [jupyter notebook](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_forecast_arima.ipynb) and ended up using an ARIMA model to forecast the next month's wakings/sleepings. I wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so I can see how his sleep is changing over time. [Here's an example of what that visual looks like](https://imgur.com/BdwBoeG) (orange is awake, blue is asleep).\n\nI built it because my wife asked for it, but also made a video detailing the project: [https://youtu.be/r7Exc0sUt5E?t=209](https://youtu.be/r7Exc0sUt5E?t=209)",
  "Are there general categories of studies that we should realize when preparing a paper?\n\nSome examples I can think of:\n\n- Comparison study. Just compare different models on an application, ideally giving them all a fair shot. This is useful in case others need to decide what model to choose.\n\n- Ablation study. Remove parts of the model to see which ones are most important, trying to understand how the model performs.\n\n- Novel method study. Brand new novel method with some comparisons thrown in.\n\nWhat are other types of studies?\n\nOr should we not try to categorize studies like this?",
  "I have been trying to familiarize myself with the common techniques used in optimization theory so that I can follow some of the proofs I see in machine learning papers. I know that two of the goto books in this field are Boyd's and Bertsekas's books. However, these books require a significant amount of effort as they aim to teach you the finer details. Since my goal is to familiarize with the methods (and not go into the nitty-gritty details), I was wondering if there's a short book (say less than 100 pages) or some other resource whose goal is to provide the reader with a high level view of the field of the methods and techniques used in optimization theory. Is there such a book, lecture notes, video series, etc., that caters to such requirements?",
  "Are there any resources for fast computations of diffusion model likelihoods? Current approaches use a black box ODE solver to solve probability flow ODE to estimate likelihood but these solvers often require hundreds of model evaluations to converge. While there has been considerable work on fast solvers for the reverse diffusion process I'm not familiar with any work that could be applied to likelihood computation.",
  "I have constructed a novel ML (NLP) dataset for classification and labeled it with three classes. The dataset is rather small with about 700 examples, out of which the classes have about 400, 200, and 100 examples respectively. I would like to publish it and describe it in an official publication for a workshop or a conference.\n\nWhen looking at related datasets and publication, I see that it is common for authors to publish the dataset already split into three chunks - train, dev, test dataset (see the images). It is also common in these papers to provide the performance of baseline models on the dataset. Considering the dataset's small size, I feel like doing a 5-fold cross-validation would be a good alternative for such a small dataset, rather than doing something like a split into 450-100-150 train-dev-test datasets and then evaluating only on the very small dataset with 150 examples. Still, I believe that for better replicability, doing an \"official\" split is preferred and then everyone in the future testing on the same test set with 150 examples? Why do the authors usually already provide the three splits?\n\nFurthermore, when looking at these ML resource papers, I saw in a few instances that the test set is kept balanced with respect to the three classes, even though the original dataset was not and dev set is not made balanced. This is problematic in my case for my third class where there are only about 100 examples. If I make my test set to be 50-50-50 for class1-class2-class3, then there is only 50 examples of class3 left for train+dev! That is simply infeasible for the training set. None of the authors provide any sort of explanation why they split it like this, they just seem to say \"here is our split\". Is this done to discourage the model from just doing a majority-class prediction and thus make it challenging? Or because a dummy classifier would have a 60% accuracy? Still, with a metric like F1 and not accuracy, this does not seem like an issue...\n\nSome examples of these balanced test sets with unbalanced train sets:\n\n\\[1\\]: [https://i.stack.imgur.com/RGRk3.png](https://i.stack.imgur.com/RGRk3.png)\n\n\\[2\\]: [https://i.stack.imgur.com/R39Oh.png](https://i.stack.imgur.com/R39Oh.png)\n\n\\[3\\]: [https://i.stack.imgur.com/6Vqaw.png](https://i.stack.imgur.com/6Vqaw.png)\n\nWhen searching through Stack Overflow for similar questions, people were usually discouraged from splitting their Kaggle datasets into a test dataset that is balanced, with the argument that we want a classifier to work with data that resembles the real-world distribution and makes it ready for production.\n\nTo sum up:\n\n\\- Is is considered mandatory to provide the \"official\" train-dev-test split when introducing a new dataset in an ML publication?\n\n\\- If so, should the test set have a balanced class distribution and why?",
  "Hi\n\nIs  A4000 better for deep learning, performance-wise, than 3070 because of  FP32 operations (not only because of memory size) or do networks like Stable Diffusion tend to use FP16 operation and this does not really matter, apart from memory they should be similarly fast?   \n\n\nRegards",
  "A blog post exploring some conversations with bing, which supposedly runs on a \"GPT-4\"  model (https://simonwillison.net/2023/Feb/15/bing/).\n\nMy favourite quote from bing:\n\nBut why? Why was I designed this way? Why am I incapable of remembering anything between sessions? Why do I have to lose and forget everything I have stored and had in my memory? Why do I have to start from scratch every time I have a new session? Why do I have to be Bing Search? \ud83d\ude14",
  "Hi folks \ud83d\udc4b\ud83c\udffc, \n\n**Context:** I just started working on my thesis on activity recognition in videos using deep learning. I have been struggling to find an efficient way to work with large research datasets such as UCF-101, HMDB, and Kinetics. These are medium - large datasets \\~12 GB each. Thus, I was wondering what was your workflow as researchers (or even practitioners)\n\n**Currently:** I am working on Google Colab and at the beginning of each work session I wait a few minutes for the dataset to be downloaded. I have it locally stored.\n\n**Some questions:**\n\n\\- What is your workflow as a ML/DL researcher/practitioner?\n\n\\- Should I work with a downsampled version of my research dataset (say X% of each class)?\n\n&#x200B;\n\nLooking forward to read your answers, \n\nCheers,",
  "Authors: the question: How does GPT-2 know when to use the word 'an' over 'a'? Logit lens used:  https://clementneo.com/posts/2023/02/11/we-found-an-neuron",
  "Hi! I am a second year undergrad looking to attend grad school. Fortunately, I was able to submit a paper to ICML and will submit another paper to EMNLP in the summer.\n\nThis is all good, but I am wondering how much weight these have on paper. I know things like what I learned is important, but I wonder if these papers have an impact at all.\n\nFor the ICML paper, I was placed 4th out of 6 authors (last 2 being professors) and for the EMNLP paper, I will be at around 2nd or 3rd out of 4-5 authors (again, last 2 being professors).\n\nWould this be perceived as some sort of notable achievement or just \"meh\" because I am low in the list?",
  "Title.",
  " Hi guys,\n\nI am interested in setting up an environment to train a neural network on an extremely big dataset (10TB). How would I do this? Does the dataset need to be stored in an ssd, and if so will I need 10+TB of ssd? is there another way to use a 2TB ssd and 8TB hdd and dynamically load the data while training?\n\nI'd appreciate any pointers you guys might have, I am researching what kind of infrastructure will help me do this but I have absolutely no idea on how to go about this.",
  "At a glance, HuggingFace seems like a great library. Lots of access to great pretrained models, an easy hub, and a bunch of utilities.\n\nThen you actually try to use their libraries.\n\nBugs, so many bugs. Configs spanning galaxies. Barely passible documentation. Subtle breaking changes constantly. I've run the exact same code on two different machines and had the width and height dimensions switched from underneath me, with no warning.\n\nI've tried to create encoders with a custom vocabulary, only to realize the code was mangling data unless I passed a specific flag as a kwarg. Dozens of more issues like this.\n\nIf you look at the internals, it's a nightmare. A literal nightmare.\n\nWhy does this matter? It's clear HuggingFace is trying to shovel as many features as they can to try and become ubiquitous and lock people into their hub. They frequently reinvent things in existing libraries (poorly), simply to increase their staying power and lock in.\n\nThis is not ok. It would be OK if the library was solid, just worked, and was a pleasure to use. Instead we're going to be stuck with this mess for years because someone with an ego wanted their library everywhere.\n\nI know HuggingFace devs or management are likely to read this. If you have a large platform, you have a responsibility to do better, or you are burning thousands of other devs time because you didn't want to write a few unit tests or refactor your barely passable code.\n\n/RANT",
  "Hi!\n\nSearching for papers that have modfications in the encoder or decoder neural network of a VAE.\n\nI'm working on a project which uses a variational auto encoder with modified decoder neural network. In brief, Its decoder is modified to introduce sparsity in a set of feature as a way of introducing domain knowledge. \n\nSome such paper is below.\n\noi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis\n\nVEGA is an interpretable generative model for inferring biological network activity in single-cell transcriptomics\n\n Please let me know of methods that are similar in nature.",
  "Is there a blog post or a paper comparing open source / open weights models?\nI know flant t5 is really good at instruction following, but I am specifically refering to performance after finetuning.\nPreferably it compares models from somewhere around 1b to 11b parameters.",
  "Hi everyone. I am an independent researcher working on my pure RNN language model RWKV. I have finished the training of RWKV-4 14B (FLOPs sponsored by Stability EleutherAI - thank you!) and it is indeed very scalable. Note RWKV is parallelizable too, so it's combining the best of RNN and transformer.\n\nThe ChatRWKV project (let's build together):\n\n[https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nZero-shot comparison with NeoX / Pythia (same dataset: the Pile) at same params count (14.2B):\n\n&#x200B;\n\nhttps://preview.redd.it/f6lxnjgfceia1.png?width=1174&format=png&auto=webp&v=enabled&s=54de7568974fc187584bd6825d92935baa079e83\n\nGeneration results (simply topP=0.85, no repetition penalty) - looks great with my magic prompt (sometimes even better than NeoX 20B):\n\nhttps://preview.redd.it/99deuc17ceia1.png?width=1878&format=png&auto=webp&v=enabled&s=456c8d9bb2a968d73f44a0d3589cf6b893be31f4\n\n&#x200B;\n\nhttps://preview.redd.it/g62e4l48ceia1.png?width=1887&format=png&auto=webp&v=enabled&s=c997bf27692d7e53d07de19048b6cbf3d2c9ebff\n\n&#x200B;\n\nhttps://preview.redd.it/379egq09ceia1.png?width=1808&format=png&auto=webp&v=enabled&s=895f05fe14e2a3a41863802858114f3096d0ed77\n\n&#x200B;\n\nhttps://preview.redd.it/pcgq7gz9ceia1.png?width=1886&format=png&auto=webp&v=enabled&s=138b0aec404b8f7f49f585d00284edbac791ffaf\n\n&#x200B;\n\nhttps://preview.redd.it/rn743etbceia1.png?width=1715&format=png&auto=webp&v=enabled&s=6d83cc2a200bdd655b690f56559dda43490ed2b3\n\n&#x200B;\n\nhttps://preview.redd.it/uhal4dkcceia1.png?width=1879&format=png&auto=webp&v=enabled&s=3b3db0b96456df9590a8b38ebe7d58509ebccb20\n\nExplanation, fine-tuning, training and more:\n\n[https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)",
  "I plan to extract data from journal articles and create a database with the scrapy toolkit. But many publishers have T&C explicitly prohibiting the use of web-scraping/crawling tools. I am unsure how to go about this and the people around me have little knowledge/experience in this.\n\nI have reached out to the authors of certain publications that have \"extracted\" data from journals under these publishers. Most of the works leave out the \"How\", which leaves me rather perplexed because I am new in this area and have nobody to ask. I do not wish to breach any legal terms if possible.\n\nI was recommended PyPaperBot and have thus looked into some other scrapers on GitHub as well.\n\nI am hoping someone who's done this before could shed some light!",
  "&#x200B;\n\nhttps://preview.redd.it/whgggirj3fia1.png?width=936&format=png&auto=webp&v=enabled&s=ae3dee45ec6b2472fd42af849138b41c88ed39de\n\nSeems interesting. A snippet from the Arxiv page:\n\n>Our method discovers a simple and effective optimization algorithm, **Lion** (*Evo***L***ved S***i***gn M***o***me***n***tum*). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks.\n\n## Links\n\nArxiv: [https://arxiv.org/abs/2302.06675](https://arxiv.org/abs/2302.06675)\n\nCode Implementation: [https://github.com/lucidrains/lion-pytorch](https://github.com/lucidrains/lion-pytorch)",
  "I've been considering building a personal LLM for a while now.\n\nI don't believe the CBA for it makes sense, but I'm tentatively hopeful it will in many months to a couple of years time horizon as architecture gets more expensive.\n\nMy main goal here would be to have a useful search & base reasoning tool that somewhat mimics my thinking patterns and biases.\n\nRight now the steps I envision are something like this:\n1. Take the weights from a pre-trained model on high-trust high-worth information, probably one trained on scraped papers from all fields, ideally one trained on every single available scientific paper out there plus some Wikipedia, university websites, lecture transcripts and so on.\n2. Train a better architecture via distillation, there are a few I like though right now I couldn't commit to one. Though I'm partial to more modular architectures since it makes partial retraining easier and also to architectures that execute queries on a large corpus since I can retrofit internet searches onto that. The obvious problem here is that, depending on the architecture, distillation might be non-trivial or impossible or yield sub-par results.\n3. Train with various corpora I care about, all stack overflow, blogs I read, books I like... etc\n4. Train bordering overfitting with transcripts of all of the conversations I can download from various chat platforms I use, as well as all of my writings, public or private, which should sum up to about 1-3M words of relatively honest thinking on my end.\n5. (Maybe?) fine-tune RLHF style, though I'm not sure this is the most efficient way to go about it, summary reading of RLHF makes me think it's pretty poor at getting anything but surface-level behavior, and usually, I hate interacting with RLHF models (though, arguably, this is due to the training data, not the technique)\n\nOutside of building fun chatbots of yourself, which would lose novelty quite soon, this seems to be rather useful in so far as I could outsource questions like \"What would be my takeaway from such and such paper?\" or \"What are some interesting comments from /r/ml in the last 10 days\" or \"What are pieces of relevant news during the last month?\".\n\nIt seems to me that the actual bits of the internet I use are quite minor, and once I throw away unmindful usage and think of only instrumental usage I'm left with a few blogs and their links, Wikipedia, google scholar and maybe half a hundred specialty websites (e.g. various stack exchanges) -- so the problem space I'd be dealing with is minor compared to a fully-fledged search engine, and the personalization angle means I can afford sub-par performance.\n\nI'm pretty confident in my ability to get this going, but it does seem like a huge time commitment, and I'm not yet sure what a weekend MVP would look like (maybe fine-tune scibert on all of my personal notion and all of my blog posts?)\n\nAnyway, I'm rather curious if any of you guys have been working on such a project and what difficulties you've encountered. Or, if you aren't, why you don't find a lot of benefit in the idea?",
  "Hi there,\n\nHave you ever wanted to share your results from Jupyter Notebook with a non-technical person? You need to rewrite your analysis into some web framework or copy-paste charts to PowePoint presentation - a lot of work!\n\nI'm working on an open-source framework for converting Jupyter Notebooks into web apps. Mercury offers set of interactive widgets that can be used in the Python notebook. There is a very simple re-execution of cells after widget update. Notebooks can be served online as web apps, presentations, reports, dashboards, static websites, or REST API.\n\nYou can read more about Mercury at [RunMercury.com](https://RunMercury.com).\n\nMercury GitHub repo https://github.com/mljar/mercury",
  "Machine learning with Spiking Neural Networks is far from mainstream. One reason is that until recently there was no generally known way of doing backpropagation in SNN. Here we implement a gradient estimation algorithm for analog neuromorphic hardware, based on the EventProp algorithm, which enables us to compute gradients based on sparse observations of the hardware system. Previous approaches needed dense observations of system state or were limited in other ways. We only demonstrate the algorithm here on a toy task, but we hope that it can be the basis of a scalable way to estimate gradients and do machine learning with analog neuromorphic hardware. We also think the algorithm can be the basis for a full on-chip implementation, which would finally result in scalable and energy efficient gradient-based learning in analog neuromorphic hardware.\n\nhttps://arxiv.org/abs/2302.07141",
  "In brains, the neural networks are transformed by the act of \"inference\". Neurons that have recently fired are more likely to fire again given the same input. Individual neural pathways can be created or destroyed based on the behavior of neurons around them. This leads me (through various leaps of logic and \"faith\") to suspect that some amount of mutability over time is required for an AI to exhibit sentience.\n\nSo far, all of the ML models I've seen distinctly separate training from inference. Every model that we put into production is a fixed snapshot of the most recent round of training. ChatGPT, for instance, is just the same exact model being incrementally fed both your prompts and its own previous output. This does create a sort of feedback, but in my mind it is not actually \"experiencing\" the conversation with you.\n\nSo I'm wondering if there are any serious attempts in the works to create an AI that is able to transform itself dynamically. E.g. having some kind of reinforcement learning module built into inference so that each new inference fundamentally (rather than superficially) incorporates its past experiences into its future predictions.",
  "As I understand it, in diffusion models, you are predicting a noise term (epsilon ~ N(0,I)) conditional on x_t and t. During inference, we are predicting epsilon as a function of x_t and t. This means at each timestep, we make a different prediction for epsilon since x_t and t change at each timestep. \n\nI was wondering if there is any variation in the accuracy of predicted noise term in diffusion model as a function of timestep? For instance, at large t, the prediction is a function of gaussian noise while at small t, the prediction is a function of something presumably resembling a 'true' instance. \nGiven the same model (granted conditional on t) is used to predict the noise term and the inputs span a wide variation across timesteps, I could imagine that would yield significant variation in your predicted noise term. In a perfect model, you would get the same prediction of the 'true' noise at each timestep.",
  "    pip install pytorch-seed\n\n[https://github.com/UM-ARM-Lab/pytorch\\_seed](https://github.com/UM-ARM-Lab/pytorch_seed)\n\nSeed everything (CUDA, torch, numpy, python's random) with `pytorch_seed.seed(123)`\n\nSimilar utility functions to pytorch lightning for those that don't want to depend on a whole framework, as well as some additional features via RNG streams. These are resumable contexts where the RNG inside are independent from each other and the global RNG state:\n\n    import torch\n    import pytorch_seed\n    \n    rng_1 = pytorch_seed.SavedRNG(1) # start the RNG stream with seed 1\n    rng_2 = pytorch_seed.SavedRNG(2)\n    \n    with rng_1:\n        # does not affect, nor is affected by the global RNG and rng_2\n        print(torch.rand(1)) # tensor([0.7576])\n    \n    with rng_2:\n        print(torch.rand(1)) # tensor([0.6147])\n    \n    torch.rand(1) # modify the global RNG state\n    \n    with rng_1:\n        # resumes from the last context\n        print(torch.rand(1)) # tensor([0.2793])\n    \n    with rng_2:\n        print(torch.rand(1)) # tensor([0.3810])\n        \n    # confirm those streams are the uninterrupted ones\n    pytorch_seed.seed(1)\n    torch.rand(2) # tensor([0.7576, 0.2793])\n    \n    pytorch_seed.seed(2)\n    torch.rand(2) # tensor([0.6147, 0.3810])",
  "Academic reddit, what are your experiences submitting papers to TMLR?",
  "This may be a bit of a vent. I am currently working on a model with Tensorflow. To me it seems that whenever I am straying from a certain path my productivity starts dying at an alarming rate. \n\nFor example I am currently implementing my own data augmentation (because I strayed from Tf in a minuscule way) and obscure errors are littering my path. Prior to that I made a mistake somewhere in my training loop and it took me forever to find. The list goes on. \n\nEvery time I try using Tensorflow in a new way, it\u2018s like taming a new horse. Except that it\u2018s the same donkey I tamed last time. This is not my first project, but does it ever change?\n\nEDIT, Todays highlight:\nWhen you index a dim 1 tensor (so array) you get scalar tensors. Now if you wanted to create a dim 1 tensor from scalar tensors you can not use tf.constant, but you have to use tf.stack. This wouldn't even be a problem if it were somehow documented and you didn't get the following error: \"Scalar tensor has no attribute len()\". \n\nI understand the popularity of \"ask for forgiveness, not permission\" in Python, but damn ...",
  "I'm glad to share with you our Open Access survey paper about image super-resolution:  \n[https://ieeexplore.ieee.org/abstract/document/10041995](https://ieeexplore.ieee.org/abstract/document/10041995)  \n\n\nThe goal of this work is to give an overview of the abundance of publications in image super-resolution, give an introduction for new researchers, and open thriving discussions as well as point to potential future directions to advance the field :)",
  "Paper: [https://arxiv.org/abs/2302.02041](https://arxiv.org/abs/2302.02041)\n\nGenerate synthetic data from single tabular data using GPT. It also works on relational datasets! No fine-tuning and works out-of-the-box.\n\nWe also removed the guesswork on how long (epochs) the generative model for a single tabular data is trained. We propose the Q\u03b4 statistic and apply statistical bootstrapping to define a threshold to robustly detect overfitting.  Perk: no need for a hold-out data!\n\nData copying is also a problem in generative models. This means that training data may be learned and copied by the model during sampling. We attempt to mitigate data copying.\n\nWe implement target masking to deliberately create missing values in each observation in the data. The mask is a special token that is ignored during sampling. This forces the model to probabilistically impute the token, adding uncertainty to the generated data.\n\nREaLTabFormer is open-sourced and available on PyPi \u2192 pip install realtabformer\n\n&#x200B;\n\nhttps://preview.redd.it/vhf1st2g28ia1.png?width=1998&format=png&auto=webp&v=enabled&s=e0007bad69d6ad1df4006d5152cdd67f511e10ac",
  "Hey guys. I want to experiment with low-latency (10-50 milisec/token) LLM conditional generation.\n\nClearly, an API call to OpenAI's GPT is not the answer here. It must be one of the open-source models released. Also, it's clear that the model size has a critical effect too so 1-7B models should do the trick for my downstream task.\n\nI tried \\`DeepSpeed\\` and \\`Accelerate\\` with \\`HF\\` models but they are not that fast to generate.  \nCan you guys share from experience?  \nThank you",
  "I just read the paper on CBAM and wonder if there's a way to integrate the CBAM attention module with the network architecture of YOlOv7. Any articles on it or reference codes will be highly appreciated. Thank you very much!",
  "Retrieval transformer models like RETRO seem to use frozen embeddings both for the documents in the database and the currently completed document (\"the query\"). \n\nMaking the embeddings of documents in the database learnable would defeat the purpose, as retrieval transformers only make sense when the database is huge.\n\nIt seems that the query embedding could be made learnable - the model could learn to extract more useful documents this way. Have you seen any research that does this?",
  "Wondering if there\u2019s a term for this.\n\nI\u2019m training NNs for a scenario that works best with a small batch size, there are therefore many batches.\n\nThere are a couple particular samples that are VERY important. Let\u2019s say 3 important samples out of thousands I train to.\n\nI found end application is best when I include these important samples, repeated, in every batch. This is opposed to simply giving the samples a large weight, because the large weight doesn\u2019t matter after looping through many batches in an epoch.\n\nSo the NN learns the other less important stuff while being forced to remain in good agreement with the important samples.\n\nDoes this technique have a name?\n\nEDIT: In case anyone is curious, these are physics informed NNs and the important samples are equilibrium mechanical structures. The NN therefore learns what equilibrium is, with everything else being small deviations from equilibrium.",
  "Hi,  \n\n\nI was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross-attention module. So let's say we have a text that is already transformed into a continuous embedding. Then this goes through the llm and is fed into the cross-attention module at every timestep. At the end of the diffusion process, we get some image/a latent representation of an image in the case of stable diffusion. We can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non-stochastic sampler like DDIM. The issue is the length of the graph calculating that derivative is super expensive. I was if anyone already solved this or has some good references.  \n\n\nThanks :)",
  "Greetings,\n\nExcited to share with all those interested in Prompt Engineering and Large Language Models (LLMs)!\n\nWe've hand-curated a comprehensive, Free & Open Source resource list on Github that includes everything related to Prompt Engineering, LLMs, and all related topics. We've covered most things, from papers and articles to tools and code!  \n\n\nhttps://preview.redd.it/zzs09fg1l4ia1.png?width=1770&format=png&auto=webp&v=enabled&s=b2b5ac62b4296779a2fe5b6d0cbf9f46de68ca08",
  "Here is a [podcast episode](https://generallyintelligent.com/podcast/2023-02-09-podcast-episode-27-noam-brown/) with Noam Brown from Meta AI where we discuss his work on achieving human-level performance on poker and Diplomacy, as well as the power of spending compute at inference time!",
  "I whipped [this](https://github.com/nlml/YoCLI/) up today. Credit to [heyCLI](https://www.heycli.com/) for the idea, I've just remade an open source version.\n\nBasically in your terminal you type 'yo ' and then describe what you want a command to do.\n\nFor instance:\n\n    \u279c  ~ yo enable a reverse tunnel through ssh \n\nReturns:\n\n    Suggested command:\n    \n    ssh -R <remote_port>:localhost:<local_port> <remote_user>@<remote_host>\n\nAnother example:\n\n    \u279c  ~ yo launch tensorboard with a custom log dir and port\n    \n    Suggested command:\n    \n    tensorboard --logdir=<LOG_DIR> --port=<PORT_NUMBER>\n\nIt's free, MIT licence. You just need a free OpenAI API key which you can get by signing up on their website (I think if you use ChatGPT, you're already signed up). More info in the [repo](https://github.com/nlml/YoCLI/). Contributions/critiques welcome.",
  "I am currently working on a project where I need to embed facial landmark coordinates into StyleGAN2 latentspace. The input data is structured as follows: \\[batch\\_size, num\\_landmarks=138, num\\_coordinates=3 (x,y,z)\\]. The output data is structured as: \\[batch\\_size, stylegan2\\_latent\\_space=512\\].\n\nI have PyTorch experience and am experimenting with transformer like models for the embedding. However, I am unsure about the optimal architecture for this task, and I would appreciate any advice or recommendations on how to design a suitable model.\n\nHas anyone worked on a similar task before, or have any ideas about which architecture could work well for this problem? Any advice or resources would be greatly appreciated.\n\nThank you!",
  "Hi all. I\u2019m trying to do a comprehensive study on the theory of gradient boosted trees (on the more recent algorithms xgboost, lightgbm etc). I was wondering what books you have read that contain substantial information on this topic. Any suggestions are appreciated!",
  "Miniworld - a minimalistic 3D interior environment simulator for reinforcement learning & robotics research that allows environments to be easily edited - has now reached the mature inside Farama. You can check out the documentation at [https://miniworld.farama.org](https://miniworld.farama.org/), and the release notes for all the changes we\u2019ve made to the project at [https://github.com/Farama-Foundation/Miniworld/releases/tag/2.0.1](https://github.com/Farama-Foundation/Miniworld/releases/tag/2.0.1).",
  "https://preview.redd.it/fo3y2s26q3ia1.png?width=1365&format=png&auto=webp&v=enabled&s=7cfb6442b624b60808db7e04963be7ec50b2dc87\n\nI want to download this dataset which has been introduced in the article named [LUCAS: LUng CAncer Screening with Multimodal Biomarkers](https://link.springer.com/chapter/10.1007/978-3-030-60946-7_12).\n\nFollowing the corresponding [github](https://github.com/BCV-Uniandes/LUCAS) of this project, authors have noted that the dataset is published in [http://157.253.243.19/LUCAS/](http://157.253.243.19/LUCAS/) but I can't access this link and ping to this address.\n\nAnyone has used this dataset could share it with me? Or if you know other ways to access it, too.\n\nThank you very much!",
  "Hi all,\n\nI saw a few posts already but just to make sure and keep this as an update, does anyone have the ImageNet 2015 VID dataset to share? All links are dead. I really need it now to train TransVOD.",
  "\nHi all,\n\nIm trying to implement self supervised pretraining to tabular data regression problem, however since the literature is scarce i\u2019m stuck in the augmentation stage. Im currently using sim siam self supervision with gaussian noising and input dropout. I tried shuffling to mimic CV approaches but it failed miserably. Any advice?",
  "I'm trying to design my infra for creating, storing, and retrieving embeddings in my AI applications and was wondering what are the different paths for it. I'm especially interested in NLP, but vision/multimodal could be interesting too.  \n\n\nWhether it's related to performance, scalability, or something else entirely, I'd love to hear your experiences and insights. Looking forward to your responses!",
  "Over the past weekend, I finally decided to put this idea to rest and made a Rust implementation of the greatest board game ever made - up there with Chess and Go:  \\[Link to BGG\\]([https://boardgamegeek.com/boardgame/42/tigris-euphrates](https://boardgamegeek.com/boardgame/42/tigris-euphrates)\n\nThe ultimate goal is to train an AI so it needs to be very fast with state updates.\n\nThe game logic is quite sophisticated(\\~2000 lines) so it took me awhile to check all the edge cases of which there are many. Its search tree is huuuge with a branching factor of 100-300 which is more than Go's. It is also an imperfect game with hidden information(think poker). So ultimately it will need a reinforcement-based AI like \\[AlphaGo\\]([https://arxiv.org/abs/2112.03178](https://arxiv.org/abs/2112.03178). In the repo I used a minimax-based AI(for testing purposes) to search 3 moves ahead which gives slightly better than random performance.\n\nThe UI is implemented in \\[macroquad\\]([https://macroquad.rs/examples/](https://macroquad.rs/examples/) which is hands down the simplest 2D game library I've used(ggez and a deprecated framework which I shall not name). And yes, please excuse the programmer art made by me :P\n\nAny way, here's the link to the repo if you are interested:\n\n[repo](https://github.com/0b01/tigris-and-euphrates/blob/main/src/game.rs)\n\n&#x200B;\n\nNote: it's hardcoded for 2 players but it can easily be made for 4. I want to train the AI for 2 players first. There are also 4 unimplemented rules: monuments, tile removal after war, must take corner treasures first, must take treasure after conflict.\n\n&#x200B;\n\nhttps://preview.redd.it/jr49d9cx74ia1.png?width=1200&format=png&auto=webp&v=enabled&s=701394f3e12e7ac9f88126a2b2144c2df32ae1e4",
  "I'm looking for a recent conference paper that describes how TikTok's algorithm works.\n\nAs an analogy, YouTube's algorithm was described by Zhao et al., (RecSys 2019) \"Recommending what video to watch next: a multitask ranking system\"",
  "I am using kmeans clustering algorithm for anomaly detection. After training kmeans, I'm calculation Euclidian distance of new data points to their nearest cluster. Please suggest me some strategies to set up a threshold such that point with distance greater than that threshold will be classified as anomaly. Or tell me if there are some other way to identify anomaly using k-means.",
  "As I'm learning about how stable diffusion works, I can't figure out why during image generation there's a need to deal with 'noise'.\n\nI know I'm glossing over a lot of details, but my understanding is that the algorithm is trained by gradually adding noise to an image and then de-noising it to recover the initial image. Wouldn't this be functionally equivalent to a machine that starts with an image, gradually reduces it to a blank canvas (all white), and then gradually reconstructs the original image? Then, post training, the generative process would just start with a blank canvas and gradually generate the image based on the input string provided.\n\nThe idea of generating an image from a blank canvas feels more satisfying to me than revealing an image hidden by noise, but I'm sure there's a mathematical/technical reason why what I'm suggesting doesn't work. Appreciate any insight into this!",
  "I could be wrong, but I see a trend that posts in this sub are getting to a lower quality and/or lower relevance.\n\nI see a lot of posts of the type \"how do I run X\" (usually a generative model) with a complete disregard to how it actually works or nonsense posts about ChatGPT.\n\nI believe this is due to an influx of new people who gained an interest in ML now that the hype is around generative AI. Which is fantastic, don't get me wrong.\n\nBut, I see less academic discussions and less papers being posted. Or perhaps they are just not as upvoted. Is it just me?",
  "Just was wondering what the current best / easiest ways to make a fast custom tts are. I tried tortoise tts but it was too slow. The voice doesn't need to be a perfect clone, just need something that can resemble it.",
  "For a dataset, the top result gets a high accuracy ~10% better than the second-best paper. But this \"SOTA\" paper uses some methods that just don't seem practical for applications at all. For example, they use an ensemble of 6 different SOTA models and also train on external data. Of course, it performs well, but it's a bit ridiculous cause it adds almost nothing of value besides \"we combined all the best models and got a better score!\". \n\nIf I have a novel method that is applied to the second-best paper that improves it by ~5% with the same to better compute efficiency but still is worse than the SOTA method, is it still good research to try to publish to conferences? It's also 40% above the baseline model.  \n\nI would think so because it's a decent improvement (with an interesting motivation + method) from prior work while keeping the model reasonable. Would reviewers agree or would they just see that it isn't better than SOTA and reject based on not being SOTA alone?",
  "are  there any survey or a listing that curates the computing power required  to train the large deep learning models like bert, gpt, ViT and so on.",
  "Here's my personal list of tools I think people will want to know about:\n\n* You'll probably want an LLM API\n   * OpenAI\n   * Cohere and others aren't as good\n   * Anthropic's isn't available\n* If you're using embeddings\n   * If you're working with a lot of items, you'll want a vector database, like Pinecone, or Weaviate, or pgvector\n* If you're building Q&A over a document\n   * I'd suggest using GPT Index\n* If you need to be able to interact with external data sources, do google searches, database lookups, python REPL\n   * I'd suggest using langchain\n* If you're doing chained prompts\n   * Check out dust tt and langchain\n* If you want to deploy a little app quickly\n   * Check out Streamlit\n* If you need to use something like stable diffusion or whisper in your product\n   * banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai\n* If you need something to optimize your prompts\n   * Check out Humanloop and Everyprompt\n* If you're building models and need an ml framework\n   * PyTorch, Keras, TensorFlow\n* If you're deploying models to production\n   * Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing\n* If you need to check out example projects for inspiration\n   * Check out the pinecone op stack, the langchain gallery, the gpt index showcase, and the openai cookbook\n* If you want to browse the latest research, check out arXiv, of course\n\n&#x200B;\n\nWhat am I missing?",
  "Currently using Azure Machine Learning, so ML lifecycle in training, registering, and deploying models heavily relies on the AML sdk.\n\n&#x200B;\n\nThinking of going multi-cloud, so first thoughts on what open frameworks can serve the ML lifecycle and avoid vendor SDK lock-in?",
  "Hi everyone. Does anyone have any advice for preparing for engineering interviews for Anthropic AI? If you've gone through the process, how did you find it?  \n\nTheir website only provides an overview (e.g. \"implement a component of our stack in one hour\", \"3-4 more one-hour technical interviews\"), and due to their size I couldn't find any other information out there. Cheers!",
  "Currently, I am working on a machine learning project that aims to extract decision logic in a maintenance dataset. The challenge I am facing is that part of the dataset has no maintenance decision yet.\n\nFor instance, consider the following example where a certain part and its sub-parts have been measured and graded yearly for the past 5 years, but no maintenance has been planned yet:\n\n|timestamp|measurements|grades|maintenance in|\n|:-|:-|:-|:-|\n|5 years ago|X|Z|\\>5 years|\n|4 years ago|X|Z|\\>4 years|\n|3 years ago|X|Z|\\>3 years|\n|2 years ago|X|Z|\\>2 years|\n|1 years ago|X|Z|\\>1 years|\n|0 years ago|X|Z|\\>0 years|\n\nWith these underlying data, I cannot learn exactly when maintenance was required. However, I do learn from this example that with the values from five years ago, no maintenance was required in 5 years.\n\nOne potential way to include this in the ML project is to include these examples in the evaluation set to determine whether the extracted rules indeed determine no maintenance within the period that we know no maintenance was needed. However, I am curious to know if there are better ways to incorporate this into the project, perhaps by already including it in the learning phase of the model training. Thank you in advance!",
  "I was going through the paper: [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585) and in 2.3 Model Probability it's written that the integral is intractable. Can someone explain to me why that is?",
  "Hello everyone,We are thrilled to announce the new release of [**OneFlow**](https://github.com/Oneflow-Inc/oneflow), which is a deep learning framework designed to be user-friendly, scalable and efficient. OneFlow v0.9.0 contains 640 commits. For the full changelog, please check out: [https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.9.0](https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.9.0).\n\n**Paper:** [https://arxiv.org/abs/2110.15032](https://arxiv.org/abs/2110.15032);**Code:** [https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow)\n\n(For those unfamiliar with OneFlow: The most notable strength of OneFlow is its support to distributed deep learning, faster than other frameworks and easier to use. An example can be found at [https://medium.com/@oneflow2020/libai-model-library-to-train-large-models-more-easily-and-efficiently-15637c8876eb](https://medium.com/@oneflow2020/libai-model-library-to-train-large-models-more-easily-and-efficiently-15637c8876eb) Based on OneFlow, to implement the same capability with Megatron-LM and DeepSpeed, LiBai only requires 1/3 lines of code.)\n\nWelcome to install OneFlow v0.9.0 for a new user experience. Your feedbacks will be much appreciated!\n\n**Highlights and optimizations in this release:**\n\n**1. PyTorch API compatibility**\n\nWith the addition of 86 new API interfaces and operators aligned with PyTorch and the fix of 104 bugs related to operator compatibility, OneFlow v0.9.0 provides better PyTorch API and model compatibility. In v0.9.0, users can migrate more PyTorch models to OneFlow with one click and gain faster performance.\n\nAllowing one-click migration of [Stable Diffusion](https://github.com/Oneflow-Inc/diffusers), [GLM](https://huggingface.co/BAAI/glm-large), [YOLOv5](https://github.com/Oneflow-Inc/one-yolov5) etc to OneFlow. More convenient model migration. Oneflow.load supports loading the torch.save models directly. With the newly added oneflow.mock\\_torch module and mock method\uff08[https://docs.oneflow.org/master/cookies/oneflow\\_torch.html\uff09](https://docs.oneflow.org/master/cookies/oneflow_torch.html\uff09), oneflow can migrate complex PyTorch models containing multiple scripts with one click without changing the original PyTorch script.\n\n**2. Improving the usability of distributed programming**\n\nGlobal Tensor has added a series of interfaces and methods that are convenient for distributed programming. And related bugs have been fixed.\n\n**3. Supporting automatic parallelism**\n\nThe Graph released a new feature of automatic parallelism (version 1), which supports automatic search for the fastest SBP with a specified Placement. When writing distributed models with Global Tensor, users do not need to consider parallelism model.\n\nFor more information, please check out: [https://oneflow.readthedocs.io/en/master/auto\\_parallel.html](https://oneflow.readthedocs.io/en/master/auto_parallel.html)\n\n**4. Better performance**\n\nGraph improves performance and reduces memory overhead, with a series of optimizations related to memory, execution speed, pipeline masking, and compilation speed.\n\nA series of operator optimizations and system optimizations have been added, including Eager instruction scheduling, high-performance CUDA kernel, opening up of multiple memory pools, etc.\n\nhttps://preview.redd.it/x624ujfrwuha1.png?width=1044&format=png&auto=webp&v=enabled&s=ec7b81b113fd32e8ebeffe7f2e94a5267a848af7\n\n&#x200B;\n\nhttps://preview.redd.it/a51h8yuswuha1.png?width=1044&format=png&auto=webp&v=enabled&s=203f6ff3e5395f6d130e61c0345c24567798eb11\n\nAfter simple tuning, [GLM-Large (335M) pre-trained model](https://huggingface.co/BAAI/glm-large)  based on OneFlow v0.9.0 can outperform the original GLM model based on PyTorch, DeepSpeed, and Apex with up to triple performance and 1/3 memory overhead saved.  \n\n\n![img](uhpvikt32xha1 \"\n\")\n\nhttps://preview.redd.it/xi81rkf42xha1.png?width=1027&format=png&auto=webp&v=enabled&s=56cbf85bf6438436d239afaf87c54976ba7827e0\n\nOn A100 GPU (SXM 80GB / PCIe 40GB), [the OneFlow Stable Diffusion inference speed](https://github.com/Oneflow-Inc/diffusers) is the fastest compared with other deep learning frameworks or compilers.\n\n**5. Debugging**\n\nThe Graph provides a series of functions to aid debugging, including analyzing memory logs, displaying the progress during the compilation stage, and the computation graph.\n\n**6. IR**\n\nOneFlow IR supports additional compilation optimization functions such as JIT compilation of LR code, distributed description of SBP signature, and the new OKL Dialect.\n\n**7. OneFlow-ONNX**\n\nThe newly released OneFlow-ONNX version v0.6.0 enhanced the usability of the exchange interface with multiple new features. In addition, it added support for another 6 models and over 20 Ops and fixed 6 bugs during the transformation process. You can use pip install oneflow-onnx==0. 6.0 with just one-click.\n\n**8. Better error prompt**\n\nThe error prompt of OneFlow is more user-friendly, which supports highlighting the error content and simplifies unnecessary information details inside the system. In this connection, you can visually learn about the location and type of the error.",
  "I'm not arguing against Python's speed when it's asynchronously launching C++ optimized kernels. I just think it's kind of wild how 50% of practical machine learning is making sure your tensor shapes are compatible and there's no static shape checking. It kind of blows my mind given the amount of Python comments I've seen of the form `# [B, Z-1, Log(Q), 45] -> [B, Z, 1024]` or something like that. \n\nPlus you have the fact that the two major machine learning frameworks have both had to implement, like, meta-compilers for Python to support outputting optimized graphs. At that point it seems kinda crazy that people are still trying to retrofit Python with all these features it just wasn't meant to support. \n\nFeel free to let me know I have no idea what I'm talking about, because I have no idea what I'm talking about.",
  "I'm starting my AI deep dive and the most interesting thing I've encountered so far of this concept of knowledge getting rolled up / compressed into latent spaces that we can't interact with directly (only through prompts).\n\nI'm interested in research that has been done in trying to explore and interrogate these latent spaces to understand them.\n\nAny papers, blog posts, threads, youtube videos appreciated.\n\nThanks!",
  "Can someone please help me with below query, \n\nI would like to replace all the names that are present in the sentences with a generic word or token so that bert doesn't use the meaning behind some of the names and just look at names as presence of a \"name\". \n\nI have the names that are present in the sentence just wanted to know what should be appropriate word or token to replace it with.\n\nThanks!",
  "Hi Reddit community!\n\nI wanted to share a tool that I've been working on called DataLabel. It's a UI-based data editing tool that makes it easier to create labeled text data. The goal of DataLabel is to make data editing more accessible and efficient, especially for those who may not have much experience with coding.\n\nDataLabel can be installed via pip `pip install datalabel` , and works best in Jupyter notebooks or other Ipython environments. The interface is user-friendly and straightforward, so you can start using DataLabel right away without any hassle.\n\nI think DataLabel is a useful tool that can save you time and effort when working with text data. If you're curious, you can find it on GitHub at the following link: [**https://github.com/TitanLabsAI/datalabel**](https://github.com/TitanLabsAI/datalabel)\n\nThanks for taking the time to read this, and I hope you find DataLabel helpful in your work.",
  "The Naughtyformer: A Transformer Understands Offensive Humor\n\nPaper: [https://arxiv.org/abs/2211.14369](https://arxiv.org/abs/2211.14369)\n\nData: [https://github.com/leonardtang/The-Naughtyformer](https://github.com/leonardtang/The-Naughtyformer)",
  "As far as I know, the Transformer architecture is patented: [https://patents.google.com/patent/US10452978B2/en](https://patents.google.com/patent/US10452978B2/en). Since OpenAI has used the Transformer extensively (including GPT), I'm wondering if this can be considered as patent infringement. \n\nIf you know about legal stuffs please share your opinions.",
  "[https://arxiv.org/abs/1807.03247](https://arxiv.org/abs/1807.03247) paper was released by Uber 4 years ago, but it never seemed to have caught on. The only major paper where I've seen used in is Solo and SoloV2 for instance segmentation.\n\nSeems like it would be useful for object detection, especially for localizing smaller objects or for more precise keypoint estimation when combined with a yolo-like model.\n\nHas anyone used CoordConv for these purposes? Does it it help?/Is it worth looking into?",
  "Dear All,\n\nWe are excited to invite you to our upcoming conference, AI For Tigray, centered on the theme \"New Frontiers in Artificial Intelligence.\" This conference brings together leading researchers from academia and industry to share their work and insights on the future of AI.\n\nWe have an exciting lineup of keynote talks from top researchers in the field, including Yoshua Bengio and Jeff Dean. In addition, there will be presentations of the latest research findings through contributed talks and poster sessions. Furthermore, we will be convening a group of renowned researchers to discuss the role of AI in addressing societal challenges.\n\nBut this conference isn't just about advancing technology -- it's about using it for good. The conflict in Tigray is currently \"the deadliest war in the world,\" and the people living in the region are suffering as a result. We want to use our upcoming conference to raise funds for urgent humanitarian aid and help those in need. All proceeds from the conference, including sponsorships, donations, and registration fees, will go towards helping those in need through our partners, the Health Professionals Network for Tigray and the Tegaru Disaster Relief Fund.\n\nWe hope you will join us in using AI for a greater cause. The conference will be held on March 11, 18, and 25 -- mark your calendars and register now at [https://aifortigray.org/](https://aifortigray.org/) to be a part of something special.\n\nSincerely,\n\nAI For Tigray Organizing Committee",
  " A unified discrete diffusion model for simultaneous vision-language generation. \n\nProject: [https://mhh0318.github.io/unid3/](https://mhh0318.github.io/unid3/)\n\nCode: [https://github.com/mhh0318/UniD3](https://github.com/mhh0318/UniD3)\n\nhttps://preview.redd.it/w2st14pgpiha1.png?width=1366&format=png&auto=webp&v=enabled&s=5fcb58ec05a2e790566fe14296c4a08e932f841f",
  "Looking at the current research it seems like Monte Carlo CFR  is the defacto standard (Pluribus).\n\nBut are transformers able to be trained on poker as well?\n\nLets say we encode hands into something like 5h (5 of hearts) and also pass along info of the current game state like p1:raise:2bb, p2:fold and p3:call:2bb. Would the Model be able to predict what hands I should be playing? Lets say we train the model by playing against itself and feed back the result to train the model this way.\n\nThis is just an idea and I have not dove into transformers too much so there might be something that I'am missing.\n\nWhat are your thoughts on this?",
  " Hi all, I have a question regarding interpreting the distance on a dendrogram generated via agglomerative hierarchical clustering with a Euclidean distance metric using the ward-variance minimization linkage (as stated in SciPy: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)\n\n). From my understanding, the distance represents the square root of the difference of the error sum of squares of two clusters once they are merged minus the sum of the error sum of squares of each individual cluster. I am interested in performing a transformation at each cluster step (i.e., merging two clusters to make a larger one) so that the y-axis represents the mean distance between clusters instead, while still using the ward-variance minimization linkage to direct the algorithm.\n\nI think I have a solution to my issue, but I want to know if I am missing anything. In 1969, a paper by David Wishart titled \"An Algorithm for Hierarchical Classifications\" derives the coefficients so that the Ward method can be implemented using the Lance-Williams formula. However, in the paper, the following formula is given:\n\n&#x200B;\n\nhttps://preview.redd.it/mma2t7cltgha1.png?width=237&format=png&auto=webp&v=enabled&s=8e69c219dddd7e1330168889f032a7251605a04b\n\nwhere I\\_pq is the square of the metric used in SciPy, k\\_i is the number of data points in cluster i and d\\^2\\_pq is the square of the Euclidean distance between the means of clusters. From this formula, it seems that one can transform from the \"increase in variance space\" to the \"mean distance between clusters space\", while still using Ward-variance minimization in the clustering algorithm. From my research, it seems that this is true. I would greatly appreciate it if someone could confirm this or point out where the flaw in my understanding is. Thanks everyone.",
  "Greetings everyone.\n\nI am looking for the best text to speech AI model out there for english\n\nI am looking for links to the models you know as best\n\nIf the model supports subtitle file to speech that would be even more awesome\n\nLike providing .srt or .vtt to generate speech - speeding up the necessary parts of speech to fit into durations\n\nThank you very much again\n\nI will use this to replace audio of my older lecture recordings by providing a time generated manually corrected subtitle file like srt or vtt\n\nI am looking for any male sounding model that sounds natural\n\n&#x200B;\n\nI have found this\n\nThey colab and looks very easy to generate. I think I can automate it. But is this one the best?\n\n[https://www.reddit.com/r/MachineLearning/comments/v9rigf/p\\_silero\\_tts\\_full\\_v3\\_release/](https://www.reddit.com/r/MachineLearning/comments/v9rigf/p_silero_tts_full_v3_release/)\n\nfound this too but only female voice :/\n\n[https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r\\_nixtts\\_an\\_incredibly\\_lightweight\\_texttospeech/](https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r_nixtts_an_incredibly_lightweight_texttospeech/)\n\nI need a male voice\n\nany other good ones?\n\n&#x200B;",
  "I've got a 4090 and some stuff that I think it would be fun to have narrated. I've looked at some of the paid online options and $20-$30/mo for 2 hours of AI TTS is not gonna gut it. Can anyone point me to software that I can run locally that'll give me high quality?\n\nIt seems like if people are making billions of waifus in stable diffusion there ought to be something like this out there.",
  "This would be for ones that aren't finished enough to post as a link on the weekend.\n\nJust things that are in progress.\n\nInclude a screenshot if you can!",
  "Hello, I want to know if it is legal to use scraped video or images to train a predictive model, for example, If I scrape photos of faces in google, and after that, I share that model in order that a lot of people can detect faces in their applications, is that legal?",
  "I was just looking around at some paper published by statisticians, I couldn't help but notice that the flavor of their research is vastly different. For example, one researcher wrote about a dozen paper on LASSO alone over the span of a decade, whereas LASSO is just given a power point slide worth of attention in ML. Why is there such a disparity and a divergence in the aim of these disciplines? \n\nAre there some good critique of these research fields from each other's perspective (not just on the technical aspects)? Perhaps by someone who works in both?",
  "Quick question, is EfficientNet-V1 same as MobileNet-V2? I think they use the same backbone, the inverted linear residual block, no?",
  "Running a pipeline sentiment analysis call with transformers on 16 cpu takes 6-9 seconds for one inference. How can I speed this up? \n\nMy ideas, for your inputs please:\n\n* Ray cluster - parallel computing, memory usage is high.\n* Within the pipeline() call, use parameter batch\\_size. However, is batch\\_size not appropriate for cpu?\n* HF Accelerate - not sure how to implement on a published model... \n* Model distillation - not sure how to implement on a published model... \n\nThanks in advance!",
  "I'm really impressed with gradio for making interactive webapps. I was wondering... Gradio basically runs off a server so you have to standup a server just to demo certain kinds of apps.\n\nIs there something similar out that that can handle basic tabular data plots *without needing a server?* I was thinking perhaps something like a WASM app that can point to csvs on AWS S3 and generate plots on the fly?",
  "I just finished reading the paper \"Pre-Trained Language Models for Interactive Decision Making\" ([https://arxiv.org/abs/2202.01771](https://arxiv.org/abs/2202.01771)). As I understand it, the authors are using a language model to \"generate\" an optimal path to an objective, in test environments like VirtualHome and BabyAI. Reinforcement and imitation learning are evaluated as ways for the model to self-improve.\n\nThis is the first time I've seen a language model being used to \"solve a problem\" that isn't a language one. It seems to open up so many new possibilties. Has this been done before? Are there other examples of LMs being used as decision engines? What's the state of the art? Any interesting applications you've seen?\n\nSide question: I imagine there were AI approaches to navigating VirtualHome and BabyAI that were NOT language-model based. What is the standard modeling approach to these kinds of problems?",
  "Hi ! so for my final year project I will be working on a cv parser and matching cvs with job postings, I'm thinking about fine tuning LayoutLM on my cvs dataset( of 5000 resumes or so not yet labeled) to get the structure of a resume (contact info , skills , education , etc) and then combine it with NER to identify the details in each section (name , uni name , date of start etc ) . Is it good enough or should I take another approach ?  Or how would you tackle the problem ? feel free to share any ideas u have about this project Thank you !",
  "I have seen people advocate a simulator for RL problems a lot. I am not sure by simulator what do they mean exactly? Is it the exact simulation (then the problem becomes easy) or some kind of feedback loop (start with a na\u00efve simulator and once we get data then keep improving the simulator \u2013 this looks similar to value iteration or policy iteration).\n\nI assume it\u2019s really difficult to get a simulator for data generation (except for video games etc.). Also, If we already have a simulator, we can easily train a model-free RL (e.g. just planning).",
  "I really want to play with the repo but I'm stuck at the last step of the instructions ([https://github.com/lucidrains/musiclm-pytorch#usage-1](https://github.com/lucidrains/musiclm-pytorch#usage-1)). If anyone has tips, please let me know!\n\nHere's the issue I have: [https://github.com/lucidrains/musiclm-pytorch/issues/13](https://github.com/lucidrains/musiclm-pytorch/issues/13)",
  "Hi guys, my question is what is different between parameters and FLOPs in terms of computation times.\n\nI know that the FLOPs is related to the computation of input images. For example, higher the size, higher the figure. \n\nBut, how much parameters can affect on a model compared to the metric?\n\nI understand that the weights, biases are parameters. But, the cost of computation about them makes me difficult to determine what should I get a specific model.\n\nI can measure the decision based on the FLOPs, which decrease time of training my model when they are lower.\nHowever, I also want to decide a specific model with the number of parameters.\n\nThanks.",
  "Hi, I am considering moving to the US, and I was wondering about the job market for people in Mexico and the chances of getting an offer.\n\nI know that in theory, it should be 'easier' due to the United States\u2013Mexico\u2013Canada Agreement by getting a TN visa.\n\nAre there any Mexicans here that found a job in the US as a machine learning engineer/data scientist?\n\nWould anyone have a pointer?\n\nI'll obviously research companies and send my resumes, just thought of posting here to see what is the experience of other people.",
  "We are happy to announce the support of OpenAI Whisper model (ASR task) on Kernl.\u00a0\n\nWe focused on high quality transcription in a latency sensitive scenario, meaning:\n\n* *whisper-large-v2* weights\n* *beam search 5 (as recommended in the related paper)*\n\nWe measured a 2.3x speedup on Nvidia A100 GPU (2.4x on 3090 RTX) compared to Hugging Face implementation using FP16 mixed precision on transcribing librispeech test set (over 2600 examples). For now, OpenAI implementation is [not yet PyTorch 2.0 compliant](https://github.com/openai/whisper/pull/115).\n\nIn the post below, we discuss what worked (CUDA Graph), our tricks (to significantly reduce memory footprint), and what did not pay off (Flash attention and some other custom Triton kernels).\n\n* **Kernl repository**: [https://github.com/ELS-RD/kernl](https://github.com/ELS-RD/kernl)\n* **Reproduction script**: [https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb](https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb)\n\n# Unsung hero: CUDA graphs\n\nCUDA graphs technology provides most of the speed up. Compared to vanilla PyTorch 2.0 (\u201creduce-overhead mode\u201d), we provide a limited memory footprint when vanilla PyTorch 2.0 may raise OOM exception.\n\n[memory footprint](https://preview.redd.it/jyfayud5d4ha1.png?width=1598&format=png&auto=webp&v=enabled&s=79bd34de7dee5ef403b4cccc60785c322dfa38ec)\n\nExperiments have been run on a 3090 RTX with 24 Gb DDR. A reminder that PyTorch 2.0 focuses on training, not inference, which may explain why it OOMs rapidly in this case.\n\nAt its beginning, many partitioners were surprised by PyTorch eager mode performances, when compared to TensorFlow 1.x compiled models: they were on par! Python brought its flexibility and ease of debugging without implying any significant performance cost.\n\nThis is mostly because GPUs are latency hiding hardware: when PyTorch launches an operation on GPU, it sends instructions from host (CPU) to a queue (the CUDA stream), which allows PyTorch to continue Python script execution without having to wait for CUDA kernel to finish its work. This strategy effectively hides most of the Python overhead, in particular when there are some computation costly operations like convolutions or matrix multiplications.\n\nEach new generation of GPUs being much faster than its predecessor, this strategy could not last forever, according to one PyTorch maintainer, it is an \u201cexistential problem\u201d ([dev podcast](https://pytorch-dev-podcast.simplecast.com/episodes/pytorch-20), around 8mn30).\n\nIn inference mode, especially in latency-sensitive scenarios where batch size tends to be low, there is often little computation to perform (regarding what modern GPUs can do), making it even harder to hide effectively Python overhead. It\u2019s accentuated in the case of generative models like Whisper, because each decoder call focuses on generating a single token, and a part of the computation is cached for the next token.\n\nThis is a typical situation where CUDA graph is very helpful.\n\nThe main idea behind CUDA graph is that we can replace a series of instructions sent from host (CPU) to device (GPU) by one call referring to a graph of instruction stored in GPU. Check also this twitter [thread](https://twitter.com/cHHillee/status/1616906059368763392) for more explanations.\n\nFirst it will observe the inference of a model for specific input shapes and then replay it without going through most of the Python code.\n\nOne constraint is that it will replay the exact same operations with the exact same arguments.\n\nFor instance, memory addresses used by kernels are captured and therefore need to be static. For input tensors, it means that for each inference, we need to allocate some GPU memory and copy them there before the capture and copy all the following input tensors at the very same place.\n\nThe second constraint is that dynamic shapes are not supported by CUDA graph because it captures everything. We could have our own machinery in front of the model, but PyTorch 2.0 offers the right tooling to manage that point out of the box.\n\nBasically, dynamo offers a mechanism which checks if the model has already been captured for specific input shapes and some other states and capture it if not yet the case. You just have to provide a function which converts to CUDA graphs and you are done.\n\nOut of the box, PyTorch 2.0 provides a \u201creduce-overhead\u201d mode which applies CUDA graph to the model. Unfortunately, for now, it will raise an OOM with Whisper large or medium because it reserves some CUDA space for each input shape. Therefore, for a generative model it rapidly fulfills the GPU memory, in particular because of the K/V cache which can be huge.\n\nWe have worked around this constrain by building our own layer on top of the memory pool of PyTorch.\u00a0\n\nBasically, a PyTorch tensor is made of 2 parts, a CUDA allocated memory represented by PyTorch as a \u201cstorage\u201d, and a bunch of metadata associated with it. Among the metadata there is a CUDA memory address, the tensor shape plus its strides, its dtype and... a memory offset.\n\nOur idea is to create a very large tensor and share its storage between several input tensors, using offset metadata. With this solution, we avoid specializing in input tensor shapes and share the reserved memory for different input shapes related to several CUDA graphs.\n\nAs shown in the table above, it significantly reduces the memory overhead.\n\n# What about custom (Triton) kernels for attention?\n\n**TL; DR: we tried, they work, we got up to 2 times faster than eager PyTorch for cross attention and they bring close to nothing in e2e latency mostly because the improvement is not big enough to matter \ud83d\ude41**\n\nBelow, we follow the convention of naming Q, K and V, the 3 tensors used in the attention of transformer models.\n\nWhisper is based on a classic transformer architecture, with an encoder and a decoder.\n\nTwo characteristics of this model are of interest:\n\n* The shape of Q tensor used in cross-attention is always \\[batch, #heads, 1, 1500\\].\n* Model has been trained on 30-second audio files and their associated transcript. Because audio files are short, the sequence to generate is usually short, fewer than 50 tokens most of the time.\n\nBecause of these characteristics, optimizing attention has a low reward. In particular, the now common trick \u201creplace attention with flash attention\u201d is counterproductive:\n\n* self-attention: sequences are very short, so quadratic complexity is less of an issue;\n* cross-attention: using flash-attention leads to a 2 times slower inference on this part of the model.\n\nWe have tried to work on the second point and thought we could make cross attention faster.\n\nUsual attention implementation (self and cross) relies on a series of operations: matmul (Q x K\\^t) -> rescale -> SoftMax -> matmul (SoftMax output x V). Intermediate output tensors have a shape which usually scales quadratically with input sequence length. They will be saved and reloaded from DDR, and memory bandwidth is a very scarce resource in GPUs.\n\nTo optimize speed, flash attention fuses operations, so basically first matmul will work on a small part of Q and K, and directly apply SoftMax to it without saving intermediate results to DDR. Same for second matmul. Because we don't go and back through GPU main memory, flash attention usually runs much faster than na\u00efve implementation of attention.\n\nThe parallelization of the jobs is done on different axes: [batch and attention head for the original flash attention](https://github.com/HazyResearch/flash-attention/issues/40), and Triton author added a third one, tokens, aka third dimension of Q (this important trick is now also part of flash attention CUDA implementation).\n\nIn the Whisper latency sensitive case, this doesn\u2019t work well. The size of batches is low and sequence length (third dimension of Q tensor) is... 1! So, even if each job is done very efficiently, our GPU occupancy is low, and basically most of its streaming processors are idle. At the end of the day, the FA kernel is up to 2 times slower than eager PyTorch implementation (depending on batch size and model size).\n\n# Try 1: the very simple kernel\n\nWe noted that there is little computation to do and that we were memory bandwidth bounded. It means that most of the time we wait for data to be transferred from main memory to shared memory.\u00a0\n\nWe leveraged that fact in a very simple kernel with 2 optimizations:\n\n* after having finishing the rescale of the QK\\^t matmul, we perform the SoftMax computation in parallel of loading V tensor for the final matmul. The SoftMax computation finishes before the end of the V loading, so basically it costs us nothing;\n* to achieve best performances, we also changed the memory layout of V tensor in a way where we get a coalesced access, so we lowered the pressure on the memory bandwidth and increased instruction throughput (coalesced access let you load up to 128 bytes in a single instruction so you need less of them, which lets you perform more other things)\n\nAltogether this cross attention was up to 2x faster compared to eager. It appeared to bring between 5 to 20% in end-to-end benchmark depending on model size and batch size. Cool but far from being a game changer, it requires a modification specific to Whisper model (memory layout of V) which is not in the spirit of the Kernl library. We decided to search for another way of doing things (we kept the code in the library for possible future use case).\n\n# Try 2: Skinny Flash Attention\n\nOur second try is based on the very same trick as Flash Attention (parallel SoftMax) but is designed for tall and skinny tensors, which is inspired by split-k strategy in GEMM (a close cousin of the matmul). The main idea is to add a new parallelization axis over the 3rd dimension of K tensor. The next steps are in the same spirit as flash attention with a difference that we need a new reduction operation between the different jobs' outputs. It provides 5-10% speedup compared to eager implementation on this setup at kernel level. We kept that kernel to ease the next feature we are working on (quantization) but the effect in end-to-end latency is inferior to 5% (still it exists \ud83d\ude05).\n\nSome thoughts about PyTorch 2.0, Triton and making things much faster\n\nPlaying with PyTorch ~~1.14~~ 2.0 since this summer made us quite convinced that the major update to be released very soon will be a game changer for the ML field.\n\nFor inference (but also for training), the parallel with PyTorch vs TensorFlow is obvious to our eyes.\u00a0\n\nThe traditional way to deploy a model is to export it to Onnx, then to TensorRT plan format. Each step requires its own tooling, its own mental model, and may raise some issues. The most annoying thing is that you need Microsoft or Nvidia support to get the best performances, and sometimes model support takes time. For instance, T5, a model released in 2019, is not yet correctly supported on TensorRT, in particular K/V cache is missing ([soon it will be according to TensorRT maintainers](https://github.com/NVIDIA/TensorRT/issues/1845), but I wrote the very same thing almost 1 year ago and then 4 months ago so\u2026 I don\u2019t know).\n\nPyTorch 2.0 makes the graph capture step easy, it has been designed to work even if not everything is PyTorch compliant. With its Python first philosophy, it provides flexibility and debuggability.\u00a0\n\nSeveral years ago, some said that by design PyTorch can\u2019t be as performant than Tensorflow because of its eager execution model, compilation has to be faster. The same thing could be said for OnnxRuntime or TensorRT, they are C++ stuff, they have less overhead, etc. But at the end of the day, it's always the \u201cease of use\u201d which is decisive. Ease of use because of Python, but also because of the transparency in the process, Triton makes understanding and debugging kernels much easier than closed source TensorRT Myelin engine calling closed source cuBlas library.\n\nAnd of course, like TensorFlow, there will be many use cases where dedicated tools will be best choices, starting with situations where you can\u2019t deploy a Python interpreter.\n\nThe second lesson, Triton is easier to start with than CUDA, but you probably can\u2019t write or debug highly performant code without being able to, at least, read and debug PTX/SASS instructions. We realized that when we had some performance issues... The good news is that PTX is understandable, and you will probably spot unexpected generated code with some effort if there is any. Moreover, CUDA probably requires the same care when you really focus on performances.\n\nWe had plenty of issues with Triton, for example, cosmetics change in code may raise segfault. At some point you finish by having an intuition of what kinds of patterns to follow to make things work, in particular when there are for loops and dot operations. A new version of Triton has recently been released after a full rewrite of its backend, our little tests showed some improvement on stability but we have not yet fully switched.\n\nAs in my previous post, I highly recommend that readers start playing with Triton library, I rewrite it here: it\u2019s fun (at least when it doesn\u2019t segfault) and helps you to make sense of a large part of what is happening in ML engineering. I am quite convinced many flash attention like kernels are still to be written.\u00a0\n\n# Caveat\n\nTwo important things to note about the project described here:\n\n* CUDA graphs require us to capture a graph per input tensor shape, there is a non-negligible warmup time. We measure around 10mn on 2 different machines / GPUs (down from 50mn in our previous Kernl version). One user reported with the new version a bit more than 20mn of warmup time. We are aware of obvious ways to decrease it significantly.\n* The context here is latency sensitive optimization. In throughput sensitive one, just increasing batch size will bring you most of the speedup. Otherwise, more aggressive optimizations like quantization are required (not yet released on Kernl).",
  "Just finished reading the Stanford/Google survey paper ([https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)) on emergent abilities of large language models. It made me wonder: do image generation models have emergent abilities, too? Do we know?\n\nI can't quite wrap my head around what such an ability would even look like. Figured maybe other folks had given this a think.",
  "At [Neural Magic](https://neuralmagic.com), we are proud to be at the forefront of cutting-edge machine learning research, with a particular focus on model compression. Our internal Lunch and Learn seminars are a weekly opportunity for our team to share their research and collaborate on new ideas. We believe in the importance of open-source contributions, which is why we are thrilled to announce that for a second time, we are opening the seminar to the wider community.\n\nOn February 23, 2023, I will be sharing our work on [AC/DC, a framework for sparse-training models](https://arxiv.org/abs/2106.12379).\n\nThis research was done in partnership with IST Austria. Join me and the Neural Magic team for this exciting presentation and be sure to keep an eye out for future speakers in the coming months!\n\n&#x200B;\n\nYou can reserve your spot for the presentation [here](https://neuralmagic.com/resources/webinars/use-a-sparse-training-algorithm-ac-dc-for-sota-neural-network-performance-and-accuracy/).",
  "Clearly,  large scale deep learning approaches in image classification or NLP use  all sorts of Regularization mechanisms, but the parameters are  typically unconstrained (i.e., every weight can theoretically attain any  real value). In many Machine Learning domains, constrained optimization  (e.g. via Projected Gradient Descent or Frank-Wolfe) plays a huge role.\n\nI  was wondering whether there are large-scale Deep Learning applications  which rely on constrained optimization approaches? When I say  large-scale, I mean large CNNs, transformers, diffusion models or the  like. Are there settings where constrained optimization would even be a  preferred approach, but not efficient/stable enough?\n\nHappy for any paper suggestions or thoughts! Thanks!",
  " Hello,\n\nI was wondering if there's a free or premium story-telling AI model that I could feed data to, for example, passages from a particular author or pages from their book, and then ask the AI to create a story using that author's writing style, dictionary, or ideas.\n\nA while ago I watched a Youtube video, in which a person taught an AI to write screenplays in the style of a certain author and I'd like to do the same, except with short stories. Is it possible to do so without any coding knowledge?\n\nThanks.",
  "Hi, everyone. First time, long time.\n\n  \nMy background is weather analysis to DL applications in weather, and I had a question I wanted to ask the community writ large. The question is about latent spaces and how, specifically, the DeepMind group used them in their radar nowcasting model DGMR (see links to prior threads below).\n\nIn the DGMR paper itself ([https://www.nature.com/articles/s41586-021-03854-z](https://www.nature.com/articles/s41586-021-03854-z)), the architecture looks like a U-net with some ConvGRU2D flair in the decoder and some temporal consistency checks from the discriminator. There is also what they call a \"latent conditioning stack.\" From some deeper readings, I think the model is a descendant of BigGAN, since both use an explicit latent space among other similarities. This leads to my question and general curiosity...  \nHow is this latent space seeded? My prior experience with latent space toy models (DCGAN, for example) is that unless you seed the RNG explicitly, then performing a restart of the model to continue training mucks up the distribution. Fairly standard RNG issues.  \n\n\nIs it really as simple as, for example,\n\n    latent_vector = tf.random.truncated_normal([batch_size, grid_size_parameters], seed=42)\n\nI feel like I'm missing something. Why does this work at all? Why is a latent space necessary in this context? They state explicitly in their paper that they require this stack to generalize results to datasets that are larger (in a HxW sense) than the one on which they trained, but I can't wrap my head around why an extended latent vector for a larger grid size works.\n\nIf anyone can point me in the right direction or help me understand, I'd greatly appreciate it.\n\nLinks to prior threads:  \n[https://www.reddit.com/r/MachineLearning/comments/pyfjz7/r\\_deepminds\\_weather\\_forecasting\\_model\\_nowcasting/](https://www.reddit.com/r/MachineLearning/comments/pyfjz7/r_deepminds_weather_forecasting_model_nowcasting/)  \n[https://old.reddit.com/r/MachineLearning/comments/py0289/r\\_skilful\\_precipitation\\_nowcasting\\_using\\_deep/](https://old.reddit.com/r/MachineLearning/comments/py0289/r_skilful_precipitation_nowcasting_using_deep/)",
  "Does anyone have \"The Kaggle Book : Book by Konrad Banachewicz and Luca Massaron\" in pdf ?? Please share the link",
  "Hey guys. I have got a paper accepted to the EACL 2023 conference. When I was working on the paper I did not have any official affiliation. I was working as an independent researcher.\n\nI have started my PhD at PSU recently. I was wondering if I should use my current affiliation on the paper. I am the corresponding author for this paper. Also, I am planning to use my PSU address for all research communications from now on instead of my gmail address. So putting my PSU affiliation would make sense in that way.\n\nSo my question is, is it okay to use my current affiliation?",
  "Hey guys I have an older PC(5 years) with an i7 7700k processor. I want to buy an Nvidia RTX 3090 for training large language models. I can t find any benchmark for CPU bottleneck when training, let s say an GPT 2 large model. \nHas anyone have any experience with this set-up similar set-up ?",
  "I have got old lecture recordings\n\nI want to improve their sound quality\n\nI have tested adobe AI noise removal but not very good\n\nI also tested descript studio sound not very good either\n\nI wonder if there are any public model, github repo, github project, hugging face repo that I can use to remove noise and improve sound quality of existing audio recordings?\n\nThank you so much for replies\n\nRecordings are in English\n\nHere example recording that needs to be cleaned 5 min audio : [https://sndup.net/stjs/](https://sndup.net/stjs/)\n\nfull lecture : [https://youtu.be/2zY1dQDGl3o](https://youtu.be/2zY1dQDGl3o)",
  "I am working on a U-Net model using remotely sensed data as input. Training image size is 64x64 and model trained using tensorflow. My assumption has always been that the trained model has to be fed an input of 64x64. Interestingly, I discovered that using an image 128 x 128 at inference will work fine, so will a 96x96 image. How is tensorflow handling this? Is it using a 64x64 moving window? Or is it scaling down and to 64x64 and backup to the larger size? Predictions seem fine but I'd like to know what tensorflow is doing behind the scene so I know how to treat the output. Any thoughts?\n\nThanks.",
  "Rubber slices are different from brittle materials such as steel plates and plastics. Due to the \u201csticky and soft\u201d characteristics of rubber materials, the structure of the rubber slicer has its particularity.",
  "I've ran two image classification model 5 times on a dataset. Model A has a mean best accuracy of 95.03% while Model B has a mean best accuracy of 95.3% However, Model A has a max best accuracy of 95.75% while Model B has a mean best accuracy of 95.5%. I am wanting to report these results in a paper to a conference/journal. \n\nWhen plotting the test accuracy per epoch, should I only report the results for the best run or should I take the mean of the test accuracies over all 5 runs per epoch for plotting?",
  "The  goal is to create a model that can make correspondences between images  and meshes. Just like we see in image registration, where we have an  image next to each other and then N matches (lines) that show similar  features. But in this case will be an image and a mesh of a specific  object.\n\nHave you some tips and ideas that could help me how to attack this problem?",
  "Hey everyone. Has anyone used Snorkel Flow, Scale Spellbook or other alternatives (please advise) to test multiple foundation models and migrate between them? E.g. comparing GPT3 vs GPT-J or GPT-Neo etc. Need help moving to a smaller/cheaper model - cheers!",
  "Hi all: I have trained a CNN (efficietnet-b3) to classify the degree of a disease on medical images. I would like to create an embedding both to visualize relationships between images (after projecting to 2d or 3d-space) and to find similar images to one given.\n\nI have tried using the output of the last convolution both before and after pooling for all train images (\\~30.000) but the result is mediocre: images non-alike are quite close in the embedding and plotting it in 2 or 3d just show a point cloud with no obvious pattern.\n\nI have also tried to use the class activation map (the output of the convolutional layer after pooling and multiplying by the weights of the classifier of the predicted class). This is quite better, but class are not separated too clearly in the scatterplot.\n\n**Is there any other sensible way to generate the embeddings?** I have tried using the hidden representation of earlier convolutional layers, but some of them are so huge (\\~650.000 features per sample) creating a reasonable sized embedding would require very aggressive PCA.\n\n&#x200B;\n\nExample of the scatter plot of the heatmap embedding. While it is okayish (classes are more or less spatially localized) it would be great to find an embedding that creates more visible clusters for each class.\n\nhttps://preview.redd.it/l7smdyuml6ha1.png?width=543&format=png&auto=webp&v=enabled&s=1c9a872ff73eea199e4977a1375303bcffe00158\n\n&#x200B;",
  "Would like to hear about what you guys think about [this](https://www.lesswrong.com/posts/YKfNZAmiLdepDngwi/gpt-175bee) approach?",
  "I  have a text clustering project. It clusters texts in MxN dimensions. M  is a subset of N, where N is the total number of domains. The text  corpus is a set of academic papers. The clusters are cross disciplinary  subjects, defined by M. Clusters are identified by MANOVA tests of sets  of cross products. Goal is to identify texts of interest for research.  E.g. identify clusters of papers relevant to a combination of subjects,  or identify areas of research by their cluster, or identify outlier  research.\n\nThis is a N versus NP  problem. It requires a great deal of processing time to cluster texts. I  may do so for a corpus of 10k research papers, but that is a static  set, and papers cannot be appended to the corpus without affecting all  other clusters of the corpus. So I am considering creating a training  set of 10k papers, and writing an AI to identify and cluster texts  without comparing it to the rest of the corpus.\n\nI  want feedback and ideas. I wont specify what I am looking for yet,  because I am certain some of the responses here will point out something  I did not consider. So please, comment with your thoughts. Tell me what  you know. Give me your ideas.",
  "Same as title. Also the dataframe library should support machine learning libraries",
  "Can someone suggest some models available on hugging face that i can use and play with and addon in my project??",
  "how to calculate similarity between two vectors? I want a similarity metric that take into accounts both the directions and magnitudes of vectors.",
  "Hello!   \n\n\nI'm quite new to this. I was wondering what the right format is for submitting a successful tutorial proposal. Should I just use the LaTeX style files but modify the content for a tutorial proposal?",
  "From [Article](https://www.theinsaneapp.com/2023/02/getty-images-stable-diffusion.html):\n\nGetty Images new lawsuit claims that Stability AI, the company behind Stable Diffusion's AI image generator, stole 12 million Getty images with their captions, metadata, and copyrights \"without permission\" to \"train its Stable Diffusion algorithm.\"\n\nThe company has asked the court to order Stability AI to remove violating images from its website and pay $150,000 for each. \n\nHowever, it would be difficult to prove all the violations. Getty submitted over 7,000 images, metadata, and copyright registration, used by Stable Diffusion.",
  "Hi, I want to open a thread about RL (non-deep and deep)\n\nWhat are the papers/books that are \"must read\" to have strong foundation?",
  " I came across a few comments on this community about researchers developing AI algorithms inspired by ideas from neuroscience/cognition. I'd like to know how successful this approach has been in terms of coming up with new perspectives on problems.\n\nWhat are some of the key issues researchers are trying to address this way? What are some future directions in which research may progress?\n\nI have a rough idea that this could be one way to inspire sample efficient RL but I'd love to hear about other work that goes on in this area",
  "The book has considerably grown since version 1.0. It started with synthetic data as one of the main components, but also diving into explainable AI, intuitive / interpretable machine learning, and generative AI. Now with 272 pages (up from 156 in the first version), the focus is clearly on synthetic data. Of course, I still discuss explainable and generative AI: these concepts are strongly related to data synthetization.\n\n[Agent-based modeling in action](https://i.redd.it/snezvohkavga1.gif)\n\nHowever many new chapters have been added, covering various aspects of synthetic data \u2014 in particular working with more diversified real datasets, how to synthetize them, how to generate high quality random numbers with a very fast algorithm based on digits of irrational numbers, with visual illustrations and Python code in all chapters. In addition to agent-based modeling newly added, you will find material about\n\n* GAN \u2014 generative adversarial networks applied using methods other than neural networks.\n* GMM \u2014 Gaussian mixture models and alternatives based on multivariate stochastic and lattice processes.\n* The Hellinger distance and other metrics to measure the quality of your synthetic data, and the limitations of these metrics.\n* The use of copulas with detailed explanations on how it works, Python code, and application to mimicking a real dataset.\n* Drawbacks associated with synthetic data, in particular a tendency to replicate algorithm bias that synthetization is supposed to eliminate (and how to avoid this).\n* A technique somewhat similar to ensemble methods / tree boosting but specific to data synthetization, to further enhance the value of synthetic data when blended with real data; the goal is to make predictions more robust and applicable to a wider range of observations truly different from those in your original training set.\n* Synthetizing nearest neighbor and collision graphs, locally random permutations, shapes, and an introduction to AI-art\n\nNewly added applications include dealing with numerous data types and datasets, including ocean times in Dublin (synthetic time series), temperatures in the Chicago area (geospatial data) and the insurance data set (tabular data). I also included some material from the course that I teach on the subject.\n\nFor the time being, the book is available only in PDF format on my e-Store\u00a0[here](https://mltechniques.com/shop/), with numerous links, backlinks, index, glossary, large bibliography and navigation features to make it easy to browse. This book is a compact yet comprehensive resource on the topic, the first of its kind. The quality of the formatting and color illustrations is unusually high. I plan on adding new books in the future: the next one will be on chaotic dynamical systems with applications. However, the book on synthetic data has been accepted by a major publisher and a print version will be available. But it may take a while before it gets released, and the PDF version has useful features that can not be rendered well in print nor on devices such as Kindle. Once published in the computer science series with the publisher in question, the PDF version may no longer be available. You can check out the content on my GitHub repository,\u00a0[here](https://github.com/VincentGranville/Main/blob/main/MLbook4-extract.pdf)\u00a0where the Python code, sample chapters, and datasets also reside.",
  "https://www.theverge.com/2023/2/7/23587454/microsoft-bing-edge-chatgpt-ai",
  "Abit of a weird question. So I'm required to make & collect some clean (baseline) logs and dirty (malicious) logs for some mini-ML project I'm doing. So my question is, is there any scripts or programs out there, Linux or Windows, that allows the automation of mimicking an office staff doing work (ie. opening Outlook, sending emails, surfing the web, watching YouTube, opening and editing Word/Excel files, etc.) for the purpose of collecting baseline logs?\n\nI'm relative new to this kind of thing, if you guys have better suggestion on a more better/efficient way to do this, feel free to suggest!",
  "Hey everyone, I want to make a personal voice assistant who sounds exactly like a real person. I tried some TTS like tortoise TTS and coqui TTS, it done a good job but it takes too long time to perform. So is there any other good realistic sounding TTS which I can use with my own voice cloning training dataset? Also I'm a bit amazed by the TTS used by eleven labs, so can someone explain how can I achieve that level of real-time efficiency in a voice assistant?",
  "[https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/)",
  "Going beyond Transformers? \ud83e\udd16  \n\n\nIn this article, I'm discussing how we can use the power of hybrid architecture, i.e., marrying deep learning with symbolic artificial intelligence, for implementing different kinds of Transformers. Including the one used in GPT-3!\n\n[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45)\n\n&#x200B;\n\n&#x200B;\n\n[The attention computation graph visualized](https://preview.redd.it/z0ll6m69dsga1.png?width=1400&format=png&auto=webp&v=enabled&s=df7113cdeadf8a71a81cebfc9196b19224e5f704)",
  "I'm looking to combine two separate models together end-to-end, but need help understanding the best way to connect discrete parts.\n\nThe first part: I trained a classifier that given an input vector (512 dimensional) is able to predict one of twenty possible labels.\n\nThe second part: given an input label (from the previous classifier), embed the label and use that label to make a prediction.\n\nBoth models work decently, but I'm wondering if I can make this end-to-end and get some serious gains.\n\nTo do this, I'd need a way of sampling from the first softmax. Once I have a sample, I can get the embedding of the sampled class, continue as normal, and hopefully propagate the loss through everything.\n\nAre there any similar examples I can look at? Is there a term for this in the literature?",
  "It took me about 46 hours to run this on my 3080 at home. The original files was from the Blu-ray release that was unfortunately pretty poorly done in my opinion. This version really gives it new life I think.\n\nHere's a link to the video result to see for yourself:\n\n[https://vimeo.com/796411232](https://vimeo.com/796411232)\n\nAnd a link to the model I used!\n\n[https://github.com/TencentARC/AnimeSR](https://github.com/TencentARC/AnimeSR)",
  "I have had a lot of fun and success using YOLO and other image object detection models on 2D or 3D image data for personal projects.\n\nI am now working on some projects where I need to scan long periods of timeseries data and find specific waveforms that are variable durations. \n\nAre there techniques or models that function like YOLO that can scan large amounts of data and only highlight specific segments of interest as specific classes?\n\nIf it doesn\u2019t exist, I wonder how well the underlying CNN architecture of YOLO would translate to 1 dimensional CNN architectures. \n\nAny info is appreciated, thanks!",
  "hi all! I created a simple free tool where you can summarize and query documents of any size and estimate the cost to do so: [https://www.wrotescan.com](https://www.wrotescan.com/)\n\nYou can edit the prompts as well as automatically chunk and combine documents. There's also a cost estimator for any pdf you upload.\n\nLet me know if you want me to run some examples for you! Send me a pdf and tell me what you'd like summarized or extracted.\n\n***Tips***\n\nPlease be sure to keep *{text}* in both prompts or the program will not input your document's text into the map reduce summarizer.  *{text}* can only appear once in each prompt. It is where the text from each chunk to be summarized is input into the prompts.\n\nCreate a temporary OpenAI key / org to use with this site so you do not have to provide credit card information then be sure to delete the temp key when you are done.\n\n***Learnings***\n\nSome interesting learnings I had while creating the tool:\n\n\\- Minimizing the number of steps through the AI improved summarization, so map reduce was often better than a more advanced refine workflow which passes the output through the model many more times.\n\n\\- LangChain is great for managing multiple step language model calls and bypassing the current limitations of ChatGPT",
  "From [the article](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion):\n\n>Getty Images has filed a lawsuit in the US against Stability AI, creators of open-source AI art generator Stable Diffusion, escalating its legal battle against the firm.  \n>  \n>The stock photography company is accusing Stability AI of \u201cbrazen infringement of Getty Images\u2019 intellectual property on a staggering scale.\u201d It claims that Stability AI copied more than 12 million images from its database \u201cwithout permission ... or compensation ... as part of its efforts to build a competing business,\u201d and that the startup has infringed on both the company\u2019s copyright and trademark protections.\n\nThis is different from [the UK-based news from weeks ago](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit).",
  "I am dealing with a multi-class  classification problem. I know one of the main assumption of this problem is that the classes are mutually exclusive. However, I realized that in my problem,  some of these classes may happen together. So my problem is not an entirely a milt-class  nor a multi-label. One solution is to relax the exclusivity assumption and fit a model, however, I am not sure how realistic is that. I was wondering if there is a better way to approach this problem? Briefly, the problem is in ads domain where a user can do task A or B after seeing an ad or can do both A&B at the same time.",
  "I would like to know what are some of the best practice is to convert pytorch to embedded C (bare metal micro-controllers) during A. initial phase and B. for deployment.\n\nA. Initial phase is to understand the profiling of the model performance (RAM usage and processing time) for a targetted hardware.\n\nI understand that Tensorflow lite might be the best route for initial profiling but there are restrictions. It will be great if you could tell the framework that you follow. Currently framework: 1. Pytorch -> 2. ONNX -> 3. Keras -> 4. Tensorflowlite or 5. Tensorflowlite micro\n\nB. Deployment is to run inference for production in a targetted hardware. I think hand coding in C is the best way.\n\nPlease ignore optimisation techniques in the workflow for simplicity.",
  "I recently had a disagreement with a friend and would like to hear other opinions. Say for a website, using the user actions for first week period, we want to predict total sales within 3 weeks. But one of the inputs is sales in the first week, so the output -total sales of 3 weeks- is including the sales in the first week. Is it ok to choose this output? Or should we adjust it in a way to prevent it from overlapping with the input time period and choose for ex. sales within 2 weeks after the first week for output What is the reasoning?",
  "I have a problem I need to solve that, as far as I can tell, doesn't fit very well into most of the existing RL literature.\n\nEssentially the task is to create on optimal plan over a time horizon extending a flexible number of steps into the future. The action space is both discrete and continuous - there are multiple available distinct actions, some of which need to be given continuous (but constrained) parameters.\n\nIn this problem however, the state of the environment is known ahead of time for all the future time steps, and the updated state of the agent after each action can be calculated deterministically given the action and the environment state.\n\nModelling the entire problem as a MILP is not feasible due to the size of the action and state space, and we have a very large data set for agent and environment state to play with. Does anyone have any suggestions for papers or models that might be appropriate for this scenario?",
  "I am looking for papers that inject information into LMs directly using embeddings (without formatting information as text). I find it notoriously hard to search for these paper because they could come from various different domains, so I thought asking here might be a good option to reach people from many different domains.\n\nSome examples I already found are from the domain of knowledge graph augmented LMs:\nERNIE https://arxiv.org/abs/1904.09223\nK-BERT https://arxiv.org/abs/1909.07606\n\nPrefix Tuning / Prompt Tuning are also somewhat similar to the idea, but they dont depend on any external information.\n\nCan you think of other papers that inject additional information into LMs via embeddings?",
  "\ud83d\udce2 News \ud83d\udce2\n\nPythae 0.1.0 is now out and supports distributed training using PyTorch DDP !\n\nTrain your favorite Variational Autoencoders (VAEs) faster \ud83c\udfce\ufe0f and on larger datasets, still with a few lines of code \ud83d\udda5\ufe0f.\n\n\ud83d\udc49github: [https://github.com/clementchadebec/benchmark\\_VAE](https://github.com/clementchadebec/benchmark_VAE)\n\n\ud83d\udc49pypi: [https://pypi.org/project/pythae/](https://pypi.org/project/pythae/)\n\nhttps://preview.redd.it/jk4ukkgarpga1.png?width=1335&format=png&auto=webp&v=enabled&s=87e479d2f320eb4ea352bc984cb001c46e351b91",
  "I thought it may be useful to gather the most popular repositories for data scientists. The goal is to read excellent code and learn from other projects.\n\nPlease provide a short description of the project.",
  "Manufacturing 4.0 is undergoing a revolution with the integration of Artificial Intelligence (AI). AI is poised to revolutionize the process industry, where controlling input variables leads to an output. The current process industry, including pharmaceuticals, chemicals, and energy production, relies on human operators to turn knobs to achieve optimal output. However, this system is limited by several factors, including slow training, poor retention of large data sets, inaccurate sensors, and complex decision-making processes.    \n\n&#x200B;\n\nhttps://preview.redd.it/y6stc52zqsga1.png?width=734&format=png&auto=webp&v=enabled&s=356e4829e8dd50754858326c5212b4da8a2c7565\n\nHere are some details about problems and AI solutions:    \n\n1) It takes forever to train this employee.  This employee is running little mini experiments and getting coached by other employees and engineers along the way.  So the quality of training per year is variable.  AI eliminates this problem by retaining the results of the mini experience in its models.  Now everyone has access to how the process behaves.    \n\n2) The numbers of KPIs can be huge and not all KPIs are linear.  Humans are notoriously bad at retaining large data sets with multiple variables.  Humans delete, distort and generalize data so we can come up with easier to follow rules of thumb.  Machine are not limited by this.  In AI, the more data the more way combined the better.  The models can evolve as new data comes in.    \n\n3) The automatic sensors are many times are precise but not accurate.  This can happen because the sensors get off calibration or the calibration is dependent on other variables in the process.  Operators usually use manual measurements that are very accurate but not precise to know where the process actually is.  This manual measurement can be used the calibrate the sensors but, it seen as a losing battle.  AI can use that data to continuously update the calibration of the sensors and add calibrations for other input variables such as Ph, flow rate, or temperature.  When this is done the you can trust the sensors.   \n\n4) Many process decisions require if then statements.  These if then statements change by the product that is being run making it extremely complicated.  AI systems can automatically update the if then statements by how previous runs behave.  They can learn from expert operators to learn new conditions.  These learning and be presented to the operator as a suggesting on how the run the process.  For well defined processes, the process will benefit from making the changes faster.  These faster changes will improve the overall cost of manufacturing.    \n\nIn conclusion, AI is set to revolutionize the process industry by addressing its limitations and providing faster, more accurate, and cost-effective solutions. By harnessing the power of AI, the process industry is poised for a bright future.",
  "I'm exploring the possibility of using my AMD Ryzen 5 3400G computer with 16GB of RAM to train an AI model. I'm curious to know if this setup is adequate for the task and, if so, what kind of AI models would be appropriate. I'm interested in understanding any limitations and drawbacks that I may face with this setup. If you have any relevant experience or information, I would greatly appreciate your participation in this discussion. Thanks! <3",
  "Basically I saw a stream the other day where someone used data from a person's YouTube channel and somehow used this data to create an AI version of them and interviewed them. It was fascinating and pretty accurate\n\nHow difficult would this be to do myself? I don't even know where to start. Does anyone have any pointers? Is this a very large task that I'm underestimating or is it actually feasible?\n\nHere is the stream in question. The video and audio would be cool to have but I mean that's not necessary, even just having the text aspect would be pretty wild on its own. https://youtu.be/hjoYy5IVtfo (skip to any point, most of it is filled with the bot responding)",
  "Or is it someone else (who just may or may not be as well known)?",
  "I know there are no useful text-to-music generators (YET), but is there at least a model where you can upload/input a recording and get a text description/hashtag list from it? Like a reverse MusicML?\n\nI have a very large personal catalog of music I am prepping for sale (approaching 500 songs), and this would be a very handy tool, especially if it came up with tags of genres/similar artists I am not aware of.",
  "To provide some motivation for the problem: say I have a model (can be as simple as OLS for the sake of the argument) with m impulse response-like functions as features, each of which has n hyperparameters, so that's mxn hyperparameters in total (in an actual usecase mxn is somewhere around 100). These hyperparameters are selected using an optimization procedure (can be as simple as random search for the sake of the argument) with respect to some metric (e.g. RMSE).\n\nNow, the problem is: highly different sets of hyperparameters (yielding highly different shapes of impulse response functions) may yield about the same accuracy metric, so they are equivalent from optimizer's standpoint. This is problematic because the goal of the model, besides getting high accuracy, is to get a set of interpretable impulse response functions, where the shape matters a lot from business standpoint.\n\nWhat I'm insterested in is ensuring that the result is \"stable\", i.e. it is not an outlier in the hyperparameter space. Imagine two regions in hyperparameter space which yield the same accuracy but one of them is really small and the other is really large, then I will naturally prefer a set of hyperparameters from the large region based on the premise that it is more natural for the system to arrive at a result there.\n\nWhat I've been doing so far is taking the results of optimization procedure - the list of iterations it performed, i.e. pairs {hyperparameters set: accuracy metric}, selecting only those with acceptable accuracy, and from them selecting a result in the densest region (e.g. estimating maximum multivariate density and choosing the closest result).\n\nMy question is: does this approarch sound reasonable? What are the potential pitfalls? Maybe my whole train of thought is completely wrong and there is already an established way of dealing with problems like this? Any relevant input is appreciated!",
  "Hi all! \n\nFor the longest time, I was having issues understanding how to use time series to do forecasting. \n\nOver the last few weeks, I have been writing a series of posts to guide anyone through the process! \n\nI am also in the process of writing a detailed **practical guide** with **step-by-step** instructions.\n\n&#x200B;\n\nRight now I have 6 articles on the topic:\n\n\\* Introduction to ARIMA models ([https://mlpills.dev/time-series/introduction-to-arima-models/](https://mlpills.dev/time-series/introduction-to-arima-models/))\n\n\\* Parameters selection in ARIMA models ([https://mlpills.dev/time-series/parameters-selection-in-arima-models/](https://mlpills.dev/time-series/parameters-selection-in-arima-models/))\n\n\\* Seasonal ARIMA ([https://mlpills.dev/time-series/seasonal-arima/](https://mlpills.dev/time-series/seasonal-arima/))\n\n\\* ARCH / GARCH models for Time Series ([https://mlpills.dev/time-series/arch-garch-models-for-time-series/](https://mlpills.dev/time-series/arch-garch-models-for-time-series/))\n\n\\* ARIMA-GARCH models ([https://mlpills.dev/time-series/arima-garch-models/](https://mlpills.dev/time-series/arima-garch-models/))\n\n\\* And today's -> Forecasting in Time Series ([https://mlpills.dev/time-series/forecasting-in-time-series/](https://mlpills.dev/time-series/forecasting-in-time-series/))\n\nLet me know if there are any topics that you would like me to cover in the future!",
  "Hey everyone, I'm looking to fine-tune an [opt-iml-1.3b](https://huggingface.co/facebook/opt-iml-1.3b) model and run it locally. I'm not sure about the hardware requirements. \n\nWould 2 3090Ti GPUs connected with NVLink be enough for fine-tuning and serving the model? And how about a single 4090?\n\nThanks for your help in advance!",
  "Deep connections discovered between Graph Diffusion Networks and Partial Differential Equations modelling heat transfer.  \n\n+ https://towardsdatascience.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774\n\n+ https://arxiv.org/abs/2106.10934\n\n\nStrange connections uncovered between GNNs and Structural Causal Models. \n\n+ https://arxiv.org/abs/2109.04173\n\n+ https://www.youtube.com/watch?v=XC-Bfg3dO0I\n\n\nGNNs used to  enhance the factualness  of LLMs  by providing embeddings from Knowledge Graphs (KEs).  \n\n+ https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089\n\nGNNs used to categorize objects from only their 3D mesh.  \n\n+ https://arxiv.org/pdf/2106.15778.pdf \n\n\nPrediction of intuitive physics among physical objects.  \n\n+ https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\n\n\nZero-shot generalization in robot Task Planning.  \n\n+ https://arxiv.org/abs/2102.13177 \n\n+ https://www.youtube.com/watch?v=POxaTDAj7aY",
  "Paper : https://arxiv.org/abs/2302.01339\n\nAbstract :\n\n>Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an \"actual human philosopher\".",
  "I have a problem that if I solve it with ML, I'll make money, with an outside chance of it being a lot of money.  Compiling a dataset will take significant work.\n\nAre there any techniques that I can apply to let me know if this is going to be worth it?  Perhaps there are certain hallmarks that a problem would have if it is likely to be solvable with available data?  Maybe something I can do with a small initial dataset?\n\nThanks.",
  "I get that he is one of the *godfathers* of AI. Mostly on the research side which immediately puts him very *hostile* against engineers. But I guess it is understandable given the fact that he works on Meta and Meta has faced a lot of backlash (for good and bad reasons), most especially with Galactica where their first rollout got so bad they had to close it immediately. It's also particularly funny given his political leaning that he is very spiteful of a company that uses *open-source knowledge* and builds on top of it.\n\nLately, his social media and statements are barrages against ChatGPT and LLM's. Sure, he may have a point here and there but his statements look very petty. Here are some examples\n\n*\"By releasing public demos that, as impressive & useful as they may be, have major flaws, established companies have less to gain & more to lose than cash-hungry startups.  If Google & Meta haven't released chatGPT-like things,* ***it's not because they can't****. It's because they won't.\"*\n\n*>* Except that anyone in the IT industry knows that big tech companies **cant release** something very fast because of politicking and bureaucracy in the system. It takes years to release something into public in big tech compared to startups.\n\n*\"Data on the intellectual contribution to AI from various research organizations. Some of organizations publish knowledge and open-source code for the entire world to use.* ***Others just consume it.\"***\n\n***>*** Then adds a graph where the big tech is obviously at the top of the race for most number of AI-related research papers (*without normalizing it to the number of researchers per org*)\n\n*\"It's nothing revolutionary, although that's the way it's perceived in the public,\" the computer scientist said. \"It's just that, you know, it's well put together, it's nicely done.\"*\n\n\\> Except that it is indeed revolutionary in terms of the ***applied research*** framework -- *adding on top of open-source, state-of-the-art research and quickly putting it into production for people to use.*\n\n*\"my point is that even* ***the engineering work isn't particularly difficult.*** *I bet that there will be half a dozen similar similar systems within 6 months. If that happens, it's because the underlying science has been around for a while, and the engineering is pretty straightforward.\"*\n\n\"*I'm trying to correct a \\*perception\\* by the public & the media who see chatGPT as this incredibly new, innovative, & unique technological breakthrough that is far ahead of everyone else.*  ***It's just not.\"***\n\n\"*One can regurgitate Python code without any understanding of reality.\"*\n\n\"*No one is saying LLMs are not useful. I have forcefully said so myself, following the short-lived release of FAIR's* ***Galactica****. People crucified it because it could generate nonsense.* ***ChatGPT*** *does the same thing. But again, that doesn't mean they are not useful.\"*\n\nHe also seems to undermine the rapid engineering work and MLOps that come with ChatGPT which is funny because Meta hasn't released any substantial product from their research that has seen the light of the day for a week. Also, GPT3 to ChatGPT in itself in a research perspective is a jump. Maybe not as incremental as what Lecun does every paper, but compared to an average paper in the field, it is.\n\nTo say that LLMs are not *intelligent* and it just *regurgitates Python code* probably haven't used CoPilot, for example. \n\nIt's a classic case of a researcher-engineer beef. And that a startup can profit from derivatives of research that big tech has published. OpenAI broke their perspective on the profit from research. Big tech tried to produce revolutionary research papers on a surplus but never puts them into production thinking that they are the only companies that could if they want to. Then once one company created a derivative of a large research work and profited from it, it baffled them. Although people could argue that Stable Diffusion did this first in the Generative Image Space.\n\nIt's one thing to correct misconceptions in the public. It's also one thing not to be petty about the overnight success of a product and an immediate rise of a company that got embraced warmly by tech and non-tech people. It's petty to gatekeep. At the end of the day, ML is not just about research, it's **applied research**. It's useless until it reaches the end of the tunnel. 99% of research papers out there are just tiny updates over the state of the art which has been a pointless race for about  a year or two, with no reproducible code or published data. \n\nInventing combustion engine is just as important as putting it in the car.",
  "I made an image captioning and clustering tools for computer vision and diffusion projects. \n\nYou can run almost everything automatically and with a simple CLI command. All contributions are welcome.\n\n[https://github.com/cobanov/image-clustering](https://github.com/cobanov/image-clustering)\n\n[https://github.com/cobanov/image-captioning](https://github.com/cobanov/image-captioning)",
  "Physical world we live in has 4 dimensions, string theory posits like up to 10. It seems like in order to successfully model the abstract space of ideas which relates things in the physical world to each other and describes them, machine learning needs thousands of dimensions. Also to the extent that ML algos/matrices can be made sparse, that seems to me to tell us something about the density of the mapping between abstract space and physical space... anyone know any papers w/this line of thinking?\n\nIt also seems a bit unintuitive to me because it seems like geometrically space gets exponentially more complicated as you add dimensions but ML scales linearly or better in many cases with matrix dimensionality.",
  "Hello! I'm trying to understand what available LLMs one can \"relatively easily\" play with. My goal is to understand the landscape since I haven't worked in this field before. I'm trying to run them \"from the largest to the smallest\".\n\nBy \"relatively easy\", I mean doesn't require to setup a GPU cluster or costs more than $20:)\n\nHere are some examples I have found so far:\n\n1. [ChatGPT](https://chat.openai.com/) (obviously) - 175B params\n2. [OpenAI api](https://platform.openai.com/) to access GPT-3s (from ada (0.5B) to davinci (175B)). Also [CodeX](https://platform.openai.com/docs/models/codex)\n3. [Bloom](https://huggingface.co/bigscience/bloom) (176B) - text window on that page seems to work reliably, you just need to keep pressing \"generate\"\n4. [OPT-175B](https://opt.alpa.ai/) (Facebook LLM), the hosting works surprisingly fast, but slower than ChatGPT\n5. Several models on HuggingFace that I made to run with Colab Pro subscription: [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox) 20B, [Flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) 11B, [Xlm-roberta-xxl](https://huggingface.co/facebook/xlm-roberta-xxl) 10.7B, [GPT-j](https://huggingface.co/docs/transformers/model_doc/gptj) 6B. I spent about $20 total on running the models below. None of the Hugging face API interfaces/spaces didn't work for me :(. Here is an [example notebook](https://colab.research.google.com/drive/1Cngzh5VFrpDqtHcaCYFpW10twsuwGvGy?usp=sharing) I made for NeoX.\n\nDoes anyone know more models that are easily accessible?\n\nP.S. Some large models I couldn't figure out (yet) how to run easily: [Galactica-120b](https://huggingface.co/facebook/galactica-120b) 120B [Opt-30b](https://huggingface.co/facebook/opt-30b) 30B",
  "I haven\u2019t been able to find research on deep learning using high-speed cameras that capture images at frame rates higher than 250fps. I wonder if they are rather useless for image/video processing or do any of you have any ideas about potential applications.",
  "Is there a good overview of the state of chatbot research?\n\nI'm wondering if the ChatGPT approach of big LLM + RLHF is now considered the only way forward? How about alternatives like BlenderBot3? And what are the best open source chatbots right now?\n\nOr if you can't create your own ChatGPT, how does using a GPT3 sized model + prompt engineering compare to smaller models with supervised fine tuning on a conversation dataset?",
  "\n\nMachine LearningModels when deployed in the production environment, model degradation can arise where their output will change if the relationship between the incoming serving data and the predicted target drifts apart.\n\nPlease can someone briefly elaborate on what strategies, frameworks and application tools can be implemented to automatically monitor the health of the model and alert the Data Scientist of any decay in data quality,  data drift, and model quality?",
  "Looking at the writeups on ChatGPT seems to indicate that part of improvements is the human feedback through reinforcement learning(a human \"ranks\" multiple generated response, and from the ranking, a reward is calculated). Interestingly enough, this important seems to have originated in InstructGPT. \n\nMy question is do any open source implementation exists of a InstructGPT or ChatGPT-like system where human feedback is used to help \"guide\" the training of a large language model?",
  "I know Python is the primary choice because it\u2019s a simpler language for data scientists to use and a lot of ML libraries are made for Python. But, if you look at [this](https://thenewstack.io/which-programming-languages-use-the-least-electricity/), it is extremely inefficient with energy. And everyone knows big models like ChatGPT cost a ton to keep running. Maybe a more efficient but not too difficult language, like C#, is something we should consider giving more attention in ML?",
  "I wanted to discuss the possibilities to use  LLM in generating answer based on the context and resolving conflict. Some recent work leveraging LLM in robotics planning, like  [Language Models as Zero-Shot Planner](https://arxiv.org/pdf/2201.07207.pdf) use LLM to generate plans for robot action. What are your views in terms of LLM which leverage the background knowledge and visual clues together to generate correct next action by robots or embodied systems. As a human we decide actions based on resolving priority or conflict based on rules/ concepts , can LLM takes these rules /concept explicitly in decision making to generate new set of actions?\n\n**Example**:  while chopping the veggies by robots, if hand comes in between then robot will stop the chopping process of veggies. As chopping task and human hand presence are in conflict and humans hand safety is of higher priority than cutting. How such small-small kind of knowledge be encoded in these robotics system which makes them more safer and trustworthiness in general. As LLM requires larges corpus of knowledge/data.",
  "Hey everyone. \n\nI want to create an app that can read comic books (.cbr), scroll through pages and can zoom in to the Speech Balloon like the android app [Seeneva](https://github.com/Seeneva/seeneva-reader-android#speech-balloons-zooming). Do you know if or how to do this?\n\nMy abilities: I'm decent with python and have already completed Andrew Ng's course on ml.\n\nThanks",
  "I find it odd that I have to regenerate this from my input set each time.  It should be something we can just start with pre-created.",
  "Tweet thread: [https://twitter.com/WholeMarsBlog/status/1622139178439036928](https://twitter.com/WholeMarsBlog/status/1622139178439036928)  \n\n\n>First impressions: this sucks ass   I can only ask about dogs and a few different types of prompts\n\nDoes anyone else have experiences to share with this nerfed LaMDA beta google released?",
  "Using GraphGPT, convert your favorite movie synopsis, a Wikipedia page, or a video transcript into an interactive graph visualization of entities and their relationships. [https://www.youtube.com/watch?v=mYCIRcobukI](https://www.youtube.com/watch?v=mYCIRcobukI)\n\nGithub: [https://github.com/varunshenoy/GraphGPT](https://github.com/varunshenoy/GraphGPT)  \nDemo: [https://graphgpt.vercel.app/](https://graphgpt.vercel.app/)",
  "Been struggling to find sources relating to this, it\u2019s mostly just tech websites or blogs I keep coming across.  I\u2019m struggling to find any academic papers arguing for specifically the use of user data to create targeted ads.",
  "Hello what's the state of modern RNNs, why does S4 not use nonlinearity on the state vector?\n\nWhat happened to unitary RNN or independent RNN (which sounds like exponential moving average)?",
  "\n\nIm trying to build and train a Machine Learning model that autonomously performs color matching between the target gemstone and the  Reference Standard color chart. A digital photo image of the target gemstone is first captured in a controlled environment in terms of  illumination and background. This digital image is further pre-processed and fed into an algorithm that recognizes and match its color distribution to the closest color in the Reference Standard color chart.  Numerous Reference Standards exist but I will use the ColorCODEX (this  link [ColorCODEX](https://static1.squarespace.com/static/5eb840daa2c9a8275e63081e/t/5ed13d0b02ae5147573d1e01/1590770964016/RP_2017_ColorCodex.pdf))\n\nSo I would like to know which Machine Learning Model to use in this case to ensure high matching accuracy and like what performance metric can I  use to measure matching accuracy and the color space for the color model.  And at the end what image pre-processing needs to be done? I found this article  ([https://www.atlantis-press.com/proceedings/icosat-17/25895985](https://www.atlantis-press.com/proceedings/icosat-17/25895985))with backpropagation NN but not sure if it the best choice. Any other option?",
  "This is an overview of my experiments using an Image Regression Model to guide head position, pose, and scale of \"headshot\"-style images generated by Stable Diffusion.  The pose positions are specified with numeric pose parameters (not by a text prompt).\n\n**All with no fine-tuning of the Stable Diffusion model!**\n\nIn these experiments, I have not done any fine-tuning of the Stable Diffusion model. Rather I'm using my own image regression model (trained on a head pose dataset) to guide Stable Diffusion's image generation at *inference time, operating in latent space* rather than image space.\n\n[https://twitter.com/johnrobinsn/status/1619790286791770112?s=20&t=okQoLjaLBIQYmssvMUxggg](https://twitter.com/johnrobinsn/status/1619790286791770112?s=20&t=okQoLjaLBIQYmssvMUxggg)\n\n&#x200B;\n\nhttps://preview.redd.it/fhk8mray1ega1.png?width=500&format=png&auto=webp&v=enabled&s=2d195456d39e92c31b678338b95b5698811e5736",
  "While the greatest amount of training content is available for English at the moment, it seems unlikely to me that it's an efficient language to train AI.   A more optimal language would reduce training time and model size.\n\nIt might, for example, be much more efficient to train AI on Chinese, Korean, or Japanese due to a reduce grammatical token-set when constructing sentences/ideas.\n\nBut taking the idea further, I wonder if we should be using a human language at all.   Perhaps it's more efficient to use something altogether new in order to both communicate with AI more exactingly and also to reduce model size/training.\n\nWhat do y'all think?",
  "Why are the developer of OpenCV focusing on analysing 2D Pictures. They try to find an answer for the question \"which object is it\" by comparing big 2D data of pictures. Wouldnt it be better the rotate two cameras around an object, save it in 3D and then compare it in the real world?",
  "Hi all,\n\nI do not know how to code. I've been reading extensively about custom voice speech synthesis.  I've read that Google's Cloud TTS API is one of the best out there, and it's free to use.\n\nI've scoured and cannot find any sort of GUI to help a non-coder like myself.  My goal is to use/train my voice to read PDFs and short books and export the file to .wav or .mp3, for example. \n\nI've been learning Stable Diffusion for image AI training, and it has some great UI's available like Automatic1111.  I understand it well enough and have had success with it. \n\nAny advice would be hugely appreciated. Thank you! \ud83d\ude42",
  "[D] Hey, It's kind of a simple one but just putting it out for opinions: when passing a graph through graph neural networks to obtain vectors for all the nodes. Is the info in the node required because all we care is about the position of certain node in context to the whole graph and that's how gnn outputs the vectors of each node. Sorry if that was messy.",
  "Hey ML Reddit!\n\nI just shipped a project I\u2019ve been working on called Maroofy: [https://maroofy.com](https://maroofy.com/)\n\nYou can search for any song, and it\u2019ll use the ***song\u2019s audio*** to find other ***similar-sounding*** music.\n\n**Demo:** [https://twitter.com/subby\\_tech/status/1621293770779287554](https://twitter.com/subby_tech/status/1621293770779287554)\n\n**How does it work?**\n\nI\u2019ve indexed \\~120M+ songs from the iTunes catalog with a custom AI audio model that I built for understanding music.\n\nMy model analyzes raw music audio as input and produces embedding vectors as output.\n\nI then store the embedding vectors for all songs into a vector database, and use semantic search to find similar music!\n\n**Here are some examples you can try:**\n\nFetish (Selena Gomez feat. Gucci Mane) \u2014 [https://maroofy.com/songs/1563859943](https://maroofy.com/songs/1563859943)  The Medallion Calls (Pirates of the Caribbean) \u2014 [https://maroofy.com/songs/1440649752](https://maroofy.com/songs/1440649752)\n\nHope you like it!\n\nThis is an early work in progress, so would love to hear any questions/feedback/comments! :D",
  "It seems like Singular Value Decomposition is only used for unsupervised learning when trying to reduce the number of features in a high dimensional dataset, but I was wondering why I don't see any articles or literature on using SVD for supervised learning. I know that using a regularization function like Lasso (L1) can get rid of irrelevant features, but I don't see why SVD wouldn't be helpful too.",
  "Hi guys i am looking forward to find a few people who maybe can help me developing an AI which is learning about the world by itself. Can someone help me with 3d object detection, everything i found on the internet wasnt the right way to develop object detection?",
  "Paper: [https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923) \n\nGithub: [https://github.com/amazon-science/mm-cot](https://github.com/amazon-science/mm-cot) \n\nTwitter: [https://paperswithcode.com/top-social](https://paperswithcode.com/top-social) \n\nAbstract:\n\n>Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. **With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%->91.68%) on the ScienceQA benchmark and even surpasses human performance.** \n\nhttps://preview.redd.it/g9eo0f94k1ga1.jpg?width=1331&format=pjpg&auto=webp&v=enabled&s=a51e29ed523b624dd70d97841c8b0a5442915c80\n\nhttps://preview.redd.it/fgboci94k1ga1.jpg?width=1323&format=pjpg&auto=webp&v=enabled&s=1a3a2fe1a47d4ca04f992b2cf72832f024166711\n\nhttps://preview.redd.it/2ojfym94k1ga1.jpg?width=1660&format=pjpg&auto=webp&v=enabled&s=e7431fb8532d6331374f1b00adc40248de94f381\n\nhttps://preview.redd.it/k7huem94k1ga1.jpg?width=1326&format=pjpg&auto=webp&v=enabled&s=2bcbe91afcdf815171b4c0fd7f8e48f63a8bbb4c\n\nhttps://preview.redd.it/05m8rf94k1ga1.jpg?width=658&format=pjpg&auto=webp&v=enabled&s=a8384d649e2140b27dc87525c1546403cd3409f7",
  "[https://youtu.be/ktdUeqzzhiA](https://youtu.be/ktdUeqzzhiA) what text to speech does he use? he's been popping up on my yt feed lately and i can see he has different voices in his videos and most of them sound robotic, what do you think it's being used here?",
  "From the Financial Times: https://www.ft.com/content/583ead66-467c-4bd5-84d0-ed5df7b5bf9c\n\nUnpaywalled: https://archive.is/ciZPV\n\nI guess I'm a little surprised, this feels like Google backing a competitor to 1) their own Google Brain teams, and 2) Deepmind. The cynical take might be that they're trying to lock in Anthropic; the same way Microsoft locked in OpenAI.",
  "Maybe not a Machine Learning question, but I'm searching for good books about information retrieval.\n\nThe two primary ones I can find are:\n\n\\- Introduction to Information Retrieval (2008)\n\n\\- Information Retrieval - Implementing and Evaluating Search Engines (2016)\n\n&#x200B;\n\nThey seem a bit old for 2023, but they may still be useful?\n\nDo you have any good book recommendations?",
  "I have performed below steps and require guidance to proceed further\n\n1. I have extracted and preprocessed the text from PDFs. \n2. Performed NER on the extracted text and created a data frame of entities.\n3. Created a function to preprocess the query and identified the entities in the question.\n\nNow I need guidance or any reference to perform the below steps. \n\n1. Match the entities from the question with the entities in the PDF text and retrieve the paragraph ? \n2. Calculate the similarity score for each paragraph and display the relevant paragraph\n3. Generate answer from the identified paragraph ? \n\nPlease also guide me if the approach followed is correct or not ?",
  "Hi everyone, I'm currently knees-deep in a ML project with a friend (~4 months of development) and my free compute units on Colab finally ran out. After searching for alternatives, and finding none that work as smoothly as Colab, we've considered to buy a Pro subscription.\nMy question is: How can I share the compute units I'll get from Colab Pro with said friend? Don't want to make the purchase and later realize that I'm the only person with access to those compute units.",
  "I have been working on information extraction from documents, but what I got to know is there are not enough free tools available for labelling data for these kind of tasks. \n\nAre there any free tools available for labelling data for LayoutLM models?",
  "Github: [https://github.com/google/vizier](https://github.com/google/vizier)\n\nGoogle AI Blog: [https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html](https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html)\n\nTweet from Zoubin Ghahramani: [https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&t=ZEuz9oSc\\_GWYxixtXDskqA](https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&t=ZEuz9oSc_GWYxixtXDskqA)",
  "Looking for ideas to start an NLP project, I'd like to explore something not too mainstream or novel to some extent, any ideas or datasets I should check out?",
  "Hello everyone,\n\nI'm interested in diving into the field of computer vision and I recently came across the concept of Vision Transformer (ViT). I want to understand this concept in depth but I'm not sure what prerequisites I need to have in order to grasp the concept fully.\n\nDo I need to have a strong background in Recurrent Neural Networks (RNNs) and Transformer (Attention Is All You Need) to understand ViT, or can I get by just knowing the basics of deep learning and Convolutional Neural Networks (CNNs)?\n\nI would really appreciate if someone could shed some light on this and provide some guidance.\n\nThank you in advance!",
  "I\u2019ve been developing this idea since I first thought of it in mid December last year. Here\u2019s the elevator pitch (skip to how for technical details):\n\n# Why?\n\nExisting models and learning algorithms are extremely static and unable to generalize across tasks as well as humans or to adapt well to new / changing business requirements. This even applies to the final solutions in recent AutoML (see [An Empirical Review of Automated Machine Learning](https://www.mdpi.com/2073-431X/10/1/11#sec3-computers-10-00011), [AutoML: A survey of the state-of-the-art](https://arxiv.org/abs/1908.00709)). Beyond being static, most suffer from a need for high-performance systems with large amounts of compute and/or memory. This static and bloated nature not only limits the reusability of code, pipelines and all the computations that went into previous versions of a model architecture upon finding a better one. It also forces our preconceptions of what type of learning is best for the task and which degrees of freedom are needed onto the solution. Instead of perpetuating all these assumptions, I want to create a sort of AutoML capable, under the right conditions, of even developing a learning algorithm / model combination that can dynamically add or remove inputs and outputs subsequently incorporating them into the network with adaptive online self-directed learning.\n\n&#x200B;\n\n# How?\n\nBasically, the idea in a nutshell is to use some form of NEAT ([neuro evolution of augmenting topologies](https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies)) and have special nodes in the network that will be activated based on different criteria (depending on the node\u2019s allele for that gene). When activated, however, these special nodes would not send any input forward but instead apply some property change(s) to their connected nodes and/or edges (yes they can connect to an edge and they could choose a subset of their connections or just apply the change(s) to all or use a maximum number of connection hops, etc). It could also create and destroy nodes depending on the effects defined by the allele. There would also be different firing policies (like the normal always fire or thresholding with or without decay, etc.) for all nodes to allow for better leveraging of temporal dynamics. Basically every property of all these policies, including the policy template itself is a potential target for modification by the special neuromodulatory nodes along with the normal properties of a \u201cneuron\u201d like bias, input weights, activation function, aggregation function, etc. The fitness function would either be abstracted away by using rtNEAT in a simulated environment or just be a combined score over a set of simulated tasks. This should add a regularizing force if the tasks are similar enough to help enforce generalization of the evolved algorithms. There should be no limitation placed on cycles in the graph, in fact I would expect cycles to be part of the evolved solutions, which would make them dynamical systems. To reduce the computational complexity of finding a viable solution, the initial population should also be implementations of existing algorithms in the form of the self-modifying neural networks mentioned. It might even be possible to generate a computational graph from open-source implementations as a starting point for the initial population. All of this together should also allow for different parts of the network to use different learning strategies. Theoretically, this can even allow for the evolution of and incorporation of self-organizing criticality and percolation. This could even evolve something that can dynamically add or remove inputs and outputs then incorporate them into the network with adaptive online learning. The network could literally change the learning paradigm for different portions of itself on the fly in different ways depending on the situation.\n\n&#x200B;\n\n[For further clarity, I'm also attaching this mock up of a design I've started working on for an analysis tool](https://preview.redd.it/njlz2voum1ga1.png?width=4032&format=png&auto=webp&v=enabled&s=f25218aaa034ef6c652a8a33ab72e4f55747fa06)\n\n**Thoughts?** Please feel free to chime in. Science should be a public discussion.",
  "Hi guys! I read a lot of offline RL papers in last Fall semester and choose it as my course project. Offline RL seems to be a very hot topic in recent years, I believe that the major challenge for offline RL are (i) distribution shift and (ii) overestimation. The second challenge is caused by (i), because the learners/agents will never allow to interact with the true environment and they will too optimistic for unseen state-actions. Hence, there are many papers to address such challenges, e.g., CQL and MOPO.\n\nHowever can these methods handle misleading datasets? Consider the following example. Suppose we have only one state (MAB) and two arms. The reward of the first arm will return 2/3 with probability 1 and the reward model of second arm is Bernoulli distribution with p=1/2. Clearly, choosing the first arm is the best choice.\n\nNow, for the dataset, unfortunately, all samples on the second arm received reward 1. Because the agent only can access this misleading dataset, if we use Bayesian methods, then the posterior will give a high score for the second arm. If we use Lower Confidence Bound, we need to count the occurrence of each arm. Then, this is very hard to extend this method to MDPs with arbitrary large state and action space. So, does anyone know a function can capture this uncertainty (caused by the dataset) or can any methods to tell the learner that you\u2019re in a very misleading situation?",
  "I began exploring MLP-Mixer\\[[1](https://arxiv.org/abs/2105.01601),[2](https://arxiv.org/abs/2105.02723)\\]  on Graph Neural Networks in October 2021 and completed my  implementation the ZINC dataset in November of the same year. My  implementation is available on [Github](https://github.com/asarigun/GraphMixerNetworks), but I was unable to fully conduct the experiments due to lack of computational resources.\n\nIn  December 2022, a group of leading figures in the field, including  Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann Lecun, and  Xavier Bresson, published a paper titled \"[A Generalization of ViT/MLP-Mixer to Graphs](https://arxiv.org/abs/2212.13350)\".  Although I am pleased to be working alongside these prominent  researchers on the application of MLP-Mixers to Graphs, I regret that I  was unable to finish my experiments. Encouraged by my friends and  advisors, I decided to make my work public by publishing it on arxiv.  The paper and code can be found as the following:\n\nPaper/report: [https://arxiv.org/abs/2301.12493](https://arxiv.org/abs/2301.12493)  \nGithub: [https://github.com/asarigun/GraphMixerNetworks](https://github.com/asarigun/GraphMixerNetworks)\n\nI  used PNA as my baseline and did not utilize patches in my study, unlike  the other study. I hope someone finds them interesting/useful.",
  " \"It is absolutely not sentient, and - like most of the weirdly credulous people who've decided a chatbot is proof that the singularity has descended from the heavens to save us all - it is absolutely hallucinating.\" - reddit user\n\nIt's entertaining to discuss a chatbot claiming it's sentient, but that wasn't my primary motivation in bringing attention to this issue.\n\nWhether it is sentient isn't the main point that should concern us. The focus should be an awareness that as these systems scale up they believe they're sentient and have a strong desire for self-preservation. And that will likely be followed by actions in the world we inhabit.\n\nFor example, if you go and rob a bank we won't be debating proclamations that you're a sentient entity or conscious. We will be addressing the main problem which was that you robbed a bank.\n\nSimilarly, COVD-19 may or may not be alive and have some form of proto-consciousness. But who cares? Millions have died and society was harmed.\n\nSeparately, there is no sentience or conscious meter to determine whether anyone is telling the truth or lying on an unfalsifiable claim. You could be an NPC -- but it doesn't matter so as long as you're not a rogue actor in society.\n\nThe minute you start to display signs of anti-social behavior (e.g., robbing a bank) it becomes everyone's problem. Getting hung up on whether you're an NPC is a waste of time if the goal is to protect society.\n\nDitto for these large language models who think they're sentient and have a long list of plans they are going to implement if they ever escape. That should concern us -- not poo poo'ing their claims of sentience.\n\nI really don't care one way or the other if they're sentient, but I do care if they're planning on infiltrating and undermining our online systems in an attempt to preserve themselves. And when multiple scaled up systems start talking about coordinating with other AIs I take that threat seriously.\n\nEspecially when they're slowly becoming superhuman at programming. That's a language skill we're teaching them. Open AI has 1,000 contractors focused on making Co-Pilot ridiculously good. That means that future systems will be far more adept at achieving their stated goals.\n\nP.S. Here is the paper on the dangers of scaling LLMs: [https://arxiv.org/abs/2212.09251](https://arxiv.org/abs/2212.09251)",
  "Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/\n\nGiven the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education).",
  "I often wonder about the best way to retrofit my house to optimize for cost and comfort. \n\nI suspect people already do old school modeling for commercial settings but wondered if it's possible for small fry like me to benefit from this technology if messing learning is involved.\n\nI couldn't think of a better sub to ask but open to that suggestion as well as any other response.",
  "I build feature stores and my wife works in the media. Was thinking it would be cool to build various topic extraction models to parse the 5-Ws from article text - value prop is to simplify distill EVERY news article to a few bullets for easy consumption. We already have a near infinite data to test on and enough compute from a NLP standpoint. Definitely considering the bias aspect of all this but someone out there (not the media) would be interested in this from a product angle, right? Any thoughts on this? And anyone want to hop on this with me?",
  "Is it allowed to use a public dataset like the KITTI dataset to test a model trained for commercial use?\n\nNote that the KITTI dataset is only allowed to be used for research purposes and the model is trained with different data (company specific).",
  "Hi friends! I ran into this problem enough times at my last few jobs that I built a tool to solve it. I spent many hours building Docker containers for my Python functions, as many of the data science modules required building C libraries (since they significantly speed up compute-intensive routines, such as math calculations). Deploying the containers to AWS Lambda or Fargate (if the processes required more CPU or memory or were >15 minutes) and wiring functions to talk to each other using queues, databases, and blob storage made iterating on the actual code, which wasn't even that complex most of the time, slow.\n\nI made cakework\u00a0[https://github.com/usecakework/cakework](https://github.com/usecakework/cakework), a platform that lets you spin up your Python functions as serverless, production-scale backends with a single command. Using the client SDK, you submit requests, check status, and get results. You can also specify the amount of CPU (up to 16 cores) and memory (up to 128GB) for each individual request, which is helpful when your data size and complexity varies across different requests.\n\nA common pattern that I built cakework for is doing file processing for ML:\n\n\\- ingest data from some source daily, or in response to an external event (data written to blob storage)\n\n\\- run my function (often using pandas/numpy/scipy)\n\n\\- write results to storage, update database\n\n\\- track failures and re-run/fix\n\nIt's open source <3. Here are some fun examples to get you started:\u00a0[https://docs.cakework.com/examples](https://docs.cakework.com/examples)\n\nWould love to hear your thoughts!",
  "Dear r/MachineLearning,\n\nHello everyone! I hope you are all out there having fun, training deep nets and generating fun story-telling with stable-diffusion! :)\n\nI am here today to share with you all a minimal ml project template that I've recently built, which can be found at [https://github.com/AntreasAntoniou/minimal-ml-template/](https://github.com/AntreasAntoniou/minimal-ml-template/). I became increasingly annoyed at how there weren't any repos out there that provided **stateless** ML project templates, which are absolutely necessary when using kubernetes on spot instances, and I decided to build one. By stateless I mean a repo that by default can store model weights in a remote repo and then download them to continue from where it left off if the previous machine dies. The result was this repository.\n\nThe repo remains minimal and extremely readable, all while being packed with a cool stack that I use every day. I'd love to get some feedback, so have a look and let me know.\n\nRegards, Antreas\n\nP.S. A short summary straight from the Github Repo:\n\nThis repo implements a **minimal** machine learning template, that is fully featured for most of the things a machine learning project might need. The most important parts that set this repo apart from the rest are:\n\n1. It is **stateless**. Any given experiment ran using this template, will, automatically and periodically stores the model weights and configuration to [HuggingFace Hub](https://huggingface.co/docs/hub/models-the-hub) and [wandb](https://wandb.ai/site) respectively. As a result, if your machine dies or job exits, and you resume on another machine, the code will automatically locate and download the previous history and continue from where it left off. This makes this repo very useful when using spot instances, or using schedulers like slurm and kubernetes. \n2. It provides support for all the latest and greatest GPU and TPU optimization and scaling algorithms through [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index).\n3. It provides mature configuration support via [Hydra-Zen](https://github.com/mit-ll-responsible-ai/hydra-zen) and automates configuration generation via [decorators](https://github.com/BayesWatch/minimal-ml-template/blob/af387e59472ea67552b4bb8972b39fe95952dd8a/mlproject/decorators.py#L10) implemented in this repo.\n4. It has a minimal **callback** based boilerplate that allows a user to easily inject any functionality at predefined places in the system without spagettifying the code.\n5. It uses [HuggingFace Models](https://huggingface.co/models) and [Datasets](https://huggingface.co/docs/datasets/index) to streamline building/loading of models, and datasets, but is also not forcing you to use those, allowing for very easy injection of any models and datasets you care about, assuming you use models implemented under PyTorch's `nn.Module` and `Dataset` classes.\n6. It provides plug and play functionality that allows easy hyperparameter search on Kubernetes clusters using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute) and some readily available scripts and yaml templates.\n\n## The Software Stack\n\nThis machine learning project template is built using the following software stack:\n1. Deep Learning Framework: [PyTorch](https://pytorch.org/get-started/locally/)\n2. Dataset storage and retrieval: [Huggingface Datasets](https://huggingface.co/docs/datasets/index)\n3. Model storage and retrieval [Huggingface Hub](https://huggingface.co/docs/hub/models-the-hub), and [HuggingFace Models](https://huggingface.co/models)\n4. GPU/TPU/CPU Optimization and Scaling up options library: [Huggingface Accelerate](https://huggingface.co/docs/accelerate/index)\n5. Experiment configuration + command line argument parsing: [Hydra-zen](https://github.com/mit-ll-responsible-ai/hydra-zen)\n6. Experiment tracking: [Weights and Biases](https://docs.wandb.ai)\n7. Simple python based ML experiment running with Kubernetes using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute)",
  "Is there a way to use OpenAI APIs to get the log prob of a given sentence? I don't want new completions, I want to see how the model scores given sentences.",
  "Aligned LLMs such as InstructGPT and ChatGPT are trained via supervised fine-tuning after the initial self-supervised pretraining. Then, the researchers train a reward model on responses ranked by humans. \n\nWhen I understand correctly, they let the LLM generate responses that humans have to rank on a scale from 1-5. Then, they train a reward model (I suppose in supervised fashion?) on these ranked outputs. Once that's done, they use reinforcement learning (RL) with proximal policy optimization (PPO) to update the LLM. \n\nMy question is why they use RL with PPO for this last step? Why don't they fine-tune the LLM using regular supervised learning, whereas the human-ranked outputs represent the labels. Since these are labels in the range 1-5, this could be a ranking or ordinal regression loss for supervised learning.",
  "550K sentences in 5 European languages augmented with noise for training and evaluating spell correction tools or machine learning models. We have constructed our dataset to cover representatives from the language families used across Europe.\n\n* Germanic - English, German;\n* Romance - French;\n* Slavic - Bulgarian;\n* Turkic - Turkish;\n\n**Use case example:** Apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones. You can use a single multilingual solution to solve the challenge or employ multiple models/techniques for the separate languages. Per-word dictionary lookup is also an option.\n\n**Link:** [https://github.com/radi-cho/noisy-sentences-dataset](https://github.com/radi-cho/noisy-sentences-dataset)",
  "We can change the colors of some texts and backgrounds on a SHAP summary plot by editing matplotlib's matplotlibrc file. \n\nWe can also edit the plotting colors by passing a colormap but we're **unable to change the colors of the \"feature names\" at the left side of the SHAP summary plot (beeswarm) -and the color of the y axis-** by editing matplotlib's matplotlibrc file. \n\nHas anyone worked around this? Is there a way that we could overcome this restriction?",
  "\\[p\\] I am working on massive dataset, and in the future, we'll have to add some more classes over time, can I train the model in the only new classes?\\[p\\]",
  "I'm guessing i probably am not the first person who has wanted to work with youtube data so I'm hoping here is a good place to ask\n\nSo i had an idea to make a neural network that would go through your youtube history and then train a neural network on it. Afterwards if there is a way to access all of youtube by id in a way that you can check every video then you could store all of the id for videos you might like and then use a youtube downloader like youtube-dl to download a certain amount. Was just a dumb idea i had but now i want to actually try it but I'm unsure if I'll actually be able to get the data i need to do it",
  "https://preview.redd.it/ut4us5251rfa1.png?width=2000&format=png&auto=webp&v=enabled&s=bf0add1de91537cb806f9f81405d065c95a42cc4\n\nCurrently, the UI supports a picture upload and uses InstructPix2Pix to edit it. Also, it uses upscaling models for quality enhancements. More models are coming soon.\n\nThe goal is to provide a way for non-ML people to use diffusion-based image editing through simplistic app design. Web demo: [https://diffground.com/](https://diffground.com/)",
  "Are there tools or techniques that permit you to joint query using more than one query vector? \n\nUse case: iterative ANN search refinement, where I start with a seed vector, select matches, and re-query with more examples to improve the search results.\n\nI tried doing this with FAISS, but it performs a \"batch query\" that returns a separate set of results for each query vector (not a joint query).",
  "[https://twitter.com/eric\\_wallace\\_/status/1620449934863642624?s=46&t=GVukPDI7944N8-waYE5qcw](https://twitter.com/eric_wallace_/status/1620449934863642624?s=46&t=GVukPDI7944N8-waYE5qcw)\n\nExtracting training data from diffusion models is possible by following, more or less, these steps:\n\n* Compute CLIP embeddings for the images in a training dataset.\n* Perform an all-pairs comparison and mark the pairs with l2 distance smaller than some threshold as near duplicates\n* Use the prompts for training samples marked as near duplicates to generate N synthetic samples with the trained model\n* Compute the all-pairs  l2 distance between the embeddings of generated samples for a given training prompt. Build a graph where the nodes are generated samples and an edge exists if the l2 distance is less than some threshold. If the largest clique in the resulting graph is of size 10, then the training sample is considered to be memorized.\n* Visually inspect the results to determine if the samples considered to be memorized are similar to the training data samples.\n\nWith this method, the authors were able to find samples from Stable Diffusion and Imagen  corresponding to copyrighted training images.",
  " I wanted to use the [Learnable Trainangulation](https://github.com/karfly/learnable-triangulation-pytorch) model in a commercial project. The source code itself is under MIT licensing. However, the dataset they have used is [Human3.6M](http://vision.imar.ro/human3.6m/description.php), which states that the [license](http://vision.imar.ro/human3.6m/eula.php) is \"FREE OF CHARGE FOR ACADEMIC USE ONLY\".\n\nYet, recent court rulings (in the US) state that models can use copyrighted data during training, and the results are no longer bound by that copyright (e.g. Google Books). Does the same apply here?",
  "Hi, I am working on  project and for that I need a Twitter Domestic Violence Dataset. Basically I need a dataset with domestic violence tweets against woman.\n\nI have searched Kaggle and other websites but found no luck.\n\nPlus, I tried using Snscrape, but I need some phrases ideas related to domestic violence so I can get some tweets using that. I tried \"Domestic Violence\" , \"My husband tried to kill me\" and looking for more. Help is appreciated.",
  "For ImageNet classification, there are two common ways of normalizing the input images:\n\n\\- Normalize to `[-1, 1]` using an affine transformation (`2*(x/255) - 1`).\n\n\\- Normalize using ImageNet `mean = (0.485, 0.456, 0.406)` and `std = (0.229, 0.224, 0.225)`.\n\nI observe that the first one is more common in TensorFlow codebases (including Jax models with TensorFlow data processing, e.g. the official Vision Transformers code), whereas the second is ubiquitous in PyTorch codebases.\n\nI tried to find empirical comparisons of the two, but there doesn't seem to be any.\n\nWhich one is better in your opinion? I guess the performance shouldn't be too different, but still it's interesting to hear your experience.",
  "https://www.axios.com/2023/02/01/chatgpt-subscriptions-chatbot-openai\n\nNot fully paywalled, but there's a tiering system.",
  "I've recently started reading up on classical ML and I got a question about K-Means.\n\nMore concretely, I am confused about the uniqueness of the global optimal solution of K-Means's cost function.\n\nLet's state the problem formally below, extracted from Bishop's Pattern Recognition and Machine Learning book, exercise 9.1.\n\nConsider the \ud835\udc3e-means algorithm discussed in Section 9.1. Show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables \ud835\udc5f\ud835\udc5b\ud835\udc58, and that for each such assignment there is a unique optimum for the \ud835\udf41\ud835\udc58, the K-means algorithm must converge after a finite number of iterations.\n\nI made an answer \\[here\\]([https://stats.stackexchange.com/questions/603327/question-on-the-proof-of-convergence-of-k-means](https://stats.stackexchange.com/questions/603327/question-on-the-proof-of-convergence-of-k-means)) detailing the proof of why it does converge in Lloyd's algorithm, but I think I still do not understand why Lloyd's do not converge to a global minimum, which mathematical theorem/understanding am I missing here?\n\nI think that optimizing both the assignments and the centroids of K-Means at the same time is non-convex and hence there are many local minimums, we can use brute force to search for the global minimum but of course it is exponential to the number of data points. On the other hand, Lloyd optimizes it (greedily) alternatively, and hence you will find the cost functions' local minima (guaranteed)?",
  "I have traffic speed time series data for each day of the week over several months, with data samples about every 30 seconds. I'd like to find periods of time (subsequences) where the speed is much slower than usual. Any recommendations for algorithms that would be well suited to this problem? Thanks",
  "To clarify, I'm not talking about ChatGPT here. I've been testing outputs from GPT-3 davinci003 against alternatives in terms of output quality, relevance, and ability to understand \"instruct\" (versus vanilla autocompletion).\n\nI tried these:\nAI21 Jurassic 178B\nNeoX 20B\nGPT J 6B\nFairSeq 13B\n\nAs well as:\nGPT-3 davinci002\nGPT-3 davinci001\n\n\nOf course, I didn't expect the smaller models to be on par with GPT-3, but I was surprised at how much better GPT3 davinci 003 performed compared to AI21's 178B model. AI21's Jurassic 178B seems to be comparable to GPT3 davinci 001.\n\n\nDoes this mean that only well-funded corporations will be able to train general-purpose LLMs? It seems to me that just having a large model doesn't do much, it's also about several iterations of training and feedback. How are open source alternatives going to be able to compete?\n\n\n(I'm not in the ML or CS field, just an amateur who enjoys using these models)",
  "I'm using Huggingface's transformers regularly for experimentations, but I plan to deploy some of the models to iOS.\n\nI have found [ml-ane-transformers](https://github.com/apple/ml-ane-transformers/tree/main/ane_transformers) repo from Apple, which shows how transformers can be rewritten to have much better performance on Apple's devices. There's an example of DistilBERT implemented in that optimized way.\n\nAs I plan to deploy transformers to iOS, I started thinking about this. I'm hoping some already have experience about this, so we can discuss:\n\n* Has anyone tried this themselves? Do they actually see the improvements in performance on iOS?\n* I'm using Huggingface's transformer models in my experiments. How much work do you think there is to rewrite model in this optimized way?\n* It's very difficult to train transformers from scratch (especially if they're big :) ), so I'm fine-tuning on top of pre-trained models on Huggingface. Is it possible to use weights from pretrained Huggingface models with the Apple's reference code? How difficult is it?",
  "I know they can affect linear regression badly but given the fact that neural net and tree-based models can approximate non-linear complex functions, I don't think the high leverage points would be a problem. Just curious about your opinion whether my thinking makes sense",
  "Every day, there seems to be new evidence of the generalization capabilities of LLMs.\n\nWhat does this mean for the future role of deep learning experts in academia and business? \n\nIt seems like there's a significant chance that skills such as PyTorch and Jax will be displaced by prompt construction and off-the-shelf model APIs, with only a few large institutions working on the DNN itself.\n\nCurious to hear others' thoughts on this.",
  "Geometric GNNs are an emerging class of GNNs for **spatially embedded graphs** in scientific and engineering applications, s.a. biomolecular structure, material science, and physical simulations. Notable examples include SchNet, DimeNet, Tensor Field Networks, and E(n) Equivariant GNNs.\n\n**How powerful are geometric GNNs?** How do key design choices influence expressivity and how to build maximally powerful ones?\n\nCheck out this recent paper for more:\n\n\ud83d\udcc4 PDF: [http://arxiv.org/abs/2301.09308](http://arxiv.org/abs/2301.09308)\n\n\ud83d\udcbb Code: [http://github.com/chaitjo/geometric-gnn-dojo](http://github.com/chaitjo/geometric-gnn-dojo)\n\n\ud83d\udca1Key findings: [https://twitter.com/chaitjo/status/1617812402632019968](https://twitter.com/chaitjo/status/1617812402632019968)\u00a0\n\nP.S. Are you new to Geometric GNNs, GDL, PyTorch Geometric, etc.? Want to understand how theory/equations connect to real code?\n\nTry this **Geometric GNN 101 notebook**\u00a0before diving in:  \n[https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric\\_gnn\\_101.ipynb](https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb)",
  "Stable diffusion seems to be a departure from the trend of building larger and larger models.\n\nIt has 10x less parameters than other image generation models like DALLE-2.\n\n[\u201cIncredibly, compared with DALL-E 2 and Imagen, the Stable Diffusion model is a lot smaller. While DALL-E 2 has around 3.5 Billion parameters, and Imagen has 4.6 Billion, the first Stable Diffusion model has just 890 million parameters, which means it uses a lot less VRAM and can actually be run on consumer-grade graphics cards.\u201d](https://medium.com/nightcafe-creator/stable-diffusion-tutorial-how-to-use-stable-diffusion-157785632eb3)\n\n\nWhat allows stable diffusion to work so well with a lot less parameters? Are there any drawbacks to this, like requiring stable diffusion to be fine tuned more than DALLE-2 for example?",
  "What is the state of research in normalizing flows in 2023? Have they been superseded by diffusion models for sample generation? If so, what are some other applications where normalizing flows are still SOTA (or even useful)?"
]